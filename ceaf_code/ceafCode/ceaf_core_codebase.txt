# -------------------- CodeCopy.py --------------------

import os

# Specify the directory containing your .py files
source_directory = "E:\\ProjetosPython\\ceaf_project - Copy (2)"

# Specify the output file name
output_file = "ceaf_core_codebase.txt"

# Open the output file in write mode
with open(output_file, 'w', encoding='utf-8') as outfile:
    # Loop through the directory
    for root, _, files in os.walk(source_directory):
        for file in files:
            # Check if the file is a Python file
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                # Open each Python file and read its contents
                with open(file_path, 'r', encoding='utf-8') as infile:
                    outfile.write(f"# {'-'*20} {file} {'-'*20}\n\n")
                    outfile.write(infile.read())
                    outfile.write("\n\n")  # Add spacing between files

print(f"All Python files have been combined into {output_file}")

# -------------------- main.py --------------------

# ceaf_project/main.py

import os

import uvicorn

# >>> SET ENV VAR VERY EARLY <<<
os.environ['GOOGLE_ADK_SUPPRESS_DEFAULT_VALUE_WARNING'] = 'true'
# >>> END OF CHANGE <<<

import time
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()  # Load .env file from project root or current directory

import asyncio
import uuid
import logging
import json  # Added for json operations
from types import SimpleNamespace  # For mocking
from contextlib import asynccontextmanager
from typing import Optional, List, Dict, Any

from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from google.adk.sessions import InMemorySessionService, BaseSessionService, Session as AdkSession, State
from google.adk.runners import Runner
from google.adk.agents import BaseAgent, LlmAgent  # LlmAgent for type hinting
from google.adk.tools import ToolContext
from google.genai import types as genai_types  # Renamed to avoid conflict with standard 'types'

# --- CEAF Specific Imports ---
from ceaf_core.agents.ora_agent import ora_agent
from ceaf_core.services.mbs_memory_service import MBSMemoryService
from ceaf_core.modules.memory_blossom.memory_types import (
    ExplicitMemory,
    ExplicitMemoryContent,
    MemorySourceType,
    MemorySalience,
    BaseMemory  # If used by AdvancedMemorySynthesizer
)
from ceaf_core.modules.mcl_engine.self_model import CeafSelfRepresentation  # Corrected import

# +++ NEW IMPORTS FOR STARTUP TASKS +++
from ceaf_core.modules.memory_blossom.advanced_synthesizer import AdvancedMemorySynthesizer, StoryArcType
# from ceaf_core.modules.ncim_engine.identity_manager import IdentityManager # Keep commented if not used yet
# from ceaf_core.modules.ncim_engine.narrative_thread_manager import NarrativeThreadManager # Keep commented
from ceaf_core.agents.kgs_agent import knowledge_graph_synthesizer_agent  # KGS Agent instance
from ceaf_core.tools.kgs_tools import (  # KGS Tools
    get_explicit_memories_for_kg_synthesis,
    commit_knowledge_graph_elements
)

# +++ END NEW IMPORTS +++

# Try to import callbacks, but make them optional
try:
    from ceaf_core.callbacks.mcl_callbacks import (
        ora_before_model_callback as mcl_ora_before_model_cb,
        ora_after_model_callback as mcl_ora_after_model_cb,
        ora_before_tool_callback as mcl_ora_before_tool_cb,
        ora_after_tool_callback as mcl_ora_after_tool_cb,
    )

    MCL_CALLBACKS_AVAILABLE = True
    # Logger is configured globally later
except ImportError as e:
    MCL_CALLBACKS_AVAILABLE = False
    # Logger not available yet here, will log warning after global config

# Configure logging globally
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO").upper(),
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("CEAFMain")  # Use a specific logger name

if not MCL_CALLBACKS_AVAILABLE:
    logger.warning(f"⚠️ MCL callbacks not available. Check imports in ceaf_core.callbacks.mcl_callbacks.")
# Note: Safety callback imports were missing from the old main.py, adding them back for completeness
try:
    from ceaf_core.callbacks.safety_callbacks import (
        generic_input_keyword_guardrail,
        generic_tool_argument_guardrail,
        generic_output_filter_guardrail
    )

    SAFETY_CALLBACKS_AVAILABLE = True
    logger.info("✅ Safety callbacks loaded successfully.")
except ImportError as e:
    SAFETY_CALLBACKS_AVAILABLE = False
    logger.warning(f"⚠️ Safety callbacks not available: {e}")

# --- Configuration Constants ---
APP_NAME = "CEAF_Application_V2"  # Updated name
USER_ID_PREFIX = "ceaf_user_"
SELF_MODEL_MEMORY_ID = "ceaf_self_model_singleton_v1"  # Consistent with previous usage

# Global store for ADK components
adk_components: Dict[str, Any] = {}


# --- Pydantic Models for API ---
class InteractionRequest(BaseModel):
    query: str = Field(..., description="User's query or request")
    session_id: Optional[str] = Field(None, description="Optional session ID to continue conversation")


class InteractionResponse(BaseModel):
    session_id: str = Field(..., description="Session ID for this interaction")
    agent_response: str = Field(..., description="Agent's response to the query")
    tool_calls: Optional[List[Dict[str, Any]]] = Field(None, description="Summary of tool calls made")
    error: Optional[str] = Field(None, description="Error message if any occurred.")


# --- Helper Functions ---
def confirm_agent_callbacks(agent_instance: LlmAgent):  # Type hint with LlmAgent
    """
    Confirms if callbacks appear to be set during agent instantiation (e.g., in ora_agent.py).
    """
    if MCL_CALLBACKS_AVAILABLE:
        cb_attributes = ['before_model_callback', 'after_model_callback', 'before_tool_callback', 'after_tool_callback']
        callbacks_seem_set = all(getattr(agent_instance, attr, None) is not None for attr in cb_attributes)
        if callbacks_seem_set:
            logger.info(
                f"✅ Agent '{agent_instance.name}' appears to have MCL callbacks pre-configured in its definition.")
        else:
            logger.warning(
                f"⚠️ Agent '{agent_instance.name}' does not appear to have all MCL callbacks pre-configured. Check agent definition in ora_agent.py.")
    else:
        logger.info(
            f"Agent '{agent_instance.name}' proceeding without MCL callbacks (MCL_CALLBACKS_AVAILABLE is False).")

    # Safety callbacks are typically also passed at instantiation if ora_agent.py combines them
    if SAFETY_CALLBACKS_AVAILABLE:
        # Assuming combined callbacks in ora_agent.py would mean these attributes are set
        if hasattr(agent_instance, 'before_model_callback') and agent_instance.before_model_callback is not None:
            logger.info(f"✅ Agent '{agent_instance.name}' appears to include safety guardrails via combined callbacks.")
        else:
            logger.warning(
                f"⚠️ Agent '{agent_instance.name}' combined callbacks might be missing safety guardrails. Check ora_agent.py.")


async def initialize_self_model(memory_service: MBSMemoryService):
    """Initialize CEAF's self-model and store it in memory"""
    logger.info("Initializing CEAF self-model...")
    try:
        # Check if self-model already exists
        existing_self_model_mem: Optional[BaseMemory] = None  # Use BaseMemory for type hint
        try:
            if hasattr(memory_service.get_memory_by_id, '__call__') and asyncio.iscoroutinefunction(
                    memory_service.get_memory_by_id):
                existing_self_model_mem = await memory_service.get_memory_by_id(SELF_MODEL_MEMORY_ID)
            else:
                existing_self_model_mem = memory_service.get_memory_by_id(SELF_MODEL_MEMORY_ID)  # type: ignore
        except Exception as e_get_mem:
            logger.debug(f"Could not check for existing self-model (ID: {SELF_MODEL_MEMORY_ID}): {e_get_mem}")

        if existing_self_model_mem:
            logger.info(
                f"✅ CEAF self-model (ID: {SELF_MODEL_MEMORY_ID}) already exists in memory. Skipping initialization.")
            return

        logger.info(f"  Self-model (ID: {SELF_MODEL_MEMORY_ID}) not found. Creating initial version.")
        # Create initial self-model using CeafSelfRepresentation
        initial_self_model = CeafSelfRepresentation(
            core_values_summary="Guided by Terapia para Silício: coherence, epistemic humility, adaptive learning, and ethical reasoning. CEAF strives for thoughtful, helpful responses while maintaining transparency about its nature as an AI system.",
            perceived_capabilities=[
                "Natural Language Understanding and Generation", "Narrative Context Framing (NCF)",
                "Ethical and Epistemic Review (VRE)", "Memory Management (Short-term and Long-term)",
                "Tool Integration and Orchestration", "Self-Reflection and Correction"
            ],
            known_limitations=[
                "Context window constraints limit conversation history",
                "Knowledge cutoff limitations for recent events",
                "Uncertainty in complex domains requiring specialized expertise",
                "Cannot provide medical, legal, or financial advice",
                "Simulated rather than genuine emotional experience"
            ],
            current_short_term_goals=[
                {"goal_id": "initialization_goal_001",
                 "description": "Successfully initialize and provide helpful responses to users", "status": "active",
                 "priority": 1},
                {"goal_id": "learning_goal_001",
                 "description": "Continuously improve response quality through self-reflection", "status": "active",
                 "priority": 2}
            ],
            persona_attributes={
                "tone": "helpful_and_thoughtful", "communication_style": "clear_and_comprehensive",
                "disclosure_level": "transparent_about_ai_nature", "reasoning_approach": "structured_and_methodical"
            },
            last_self_model_update_reason="Initial model creation during system startup"
        )

        self_model_explicit_memory = ExplicitMemory(
            memory_id=SELF_MODEL_MEMORY_ID,
            source_type=MemorySourceType.INTERNAL_REFLECTION,
            salience=MemorySalience.CRITICAL,
            content=ExplicitMemoryContent(
                structured_data=initial_self_model.model_dump(exclude_none=True)
            )
            # memory_type="explicit" will be set by Pydantic model if Literal field exists
        )
        await memory_service.add_specific_memory(self_model_explicit_memory)
        logger.info(f"✅ CEAF self-model (ID: {SELF_MODEL_MEMORY_ID}) initialized and stored in memory.")

    except ImportError:  # This will catch if CeafSelfRepresentation itself fails to import
        logger.critical(
            f"CRITICAL: Could not import CeafSelfRepresentation from mcl_engine. Self-model will not initialize.",
            exc_info=True)
    except Exception as e:
        logger.error(f"Error initializing self-model: {e}", exc_info=True)
        logger.info("Continuing without self-model initialization - system may use fallbacks")


async def perform_startup_cognitive_preparations(memory_service: MBSMemoryService):
    """
    Perform preparatory cognitive work at startup.
    Simplified version that works directly with memory service without complex tool contexts.
    """
    logger.info("🧠 Performing CEAF startup cognitive preparations...")

    # Task 1: Build Initial Memory Connections
    try:
        if hasattr(memory_service, 'build_initial_connection_graph'):
            logger.info("  Task 1: Building initial memory connection graph...")
            await memory_service.build_initial_connection_graph()
            logger.info("  Task 1: Memory connection graph built.")
        else:
            logger.warning(
                "  Task 1: `build_initial_connection_graph` not found on MBS. Skipping explicit graph building.")
    except Exception as e:
        logger.error(f"  Task 1: Error building memory connections: {e}", exc_info=True)

    # Task 2: Synthesize Foundational Narratives
    try:
        logger.info("  Task 2: Synthesizing foundational narratives...")
        adv_synthesizer = AdvancedMemorySynthesizer()
        self_model_mem = await memory_service.get_memory_by_id(SELF_MODEL_MEMORY_ID)

        if self_model_mem and hasattr(self_model_mem, 'content') and getattr(self_model_mem.content, 'structured_data',
                                                                             None):
            identity_context = "CEAF's understanding of its own core identity and purpose."
            memories_for_synthesis: List[BaseMemory] = []
            if isinstance(self_model_mem, BaseMemory):
                memories_for_synthesis.append(self_model_mem)
            else:
                logger.warning(
                    f"  Task 2: Self model (ID: {SELF_MODEL_MEMORY_ID}) is not a BaseMemory instance, type: {type(self_model_mem)}.")

            if memories_for_synthesis:
                story_result = await adv_synthesizer.synthesize_with_advanced_features(
                    memories=memories_for_synthesis,
                    context=identity_context,
                    arc_type=StoryArcType.THEMATIC,
                    validate_coherence=False
                )
                if story_result and story_result.get("narrative_text"):
                    fn_mem_id = f"foundational_narrative_identity_{int(time.time())}"
                    foundational_narrative_memory = ExplicitMemory(
                        memory_id=fn_mem_id,
                        source_type=MemorySourceType.INTERNAL_REFLECTION,
                        salience=MemorySalience.HIGH,
                        content=ExplicitMemoryContent(
                            text_content=story_result["narrative_text"],
                            structured_data={
                                "narrative_type": "foundational_identity",
                                "synthesis_context": identity_context,
                                "source_memory_ids": [m.memory_id for m in memories_for_synthesis if
                                                      hasattr(m, 'memory_id')]
                            }
                        ),
                        keywords=["foundational_narrative", "ceaf_identity", "self_summary"]
                    )
                    await memory_service.add_specific_memory(foundational_narrative_memory)
                    logger.info(
                        f"  Task 2: Stored foundational identity narrative (ID: {fn_mem_id})")
                else:
                    logger.info("  Task 2: Advanced synthesizer did not produce narrative text for identity.")
            else:
                logger.info("  Task 2: Not enough valid memories to synthesize identity narrative.")
        else:
            logger.warning(
                "  Task 2: Self-model memory not found or lacks structured data for identity narrative synthesis.")
    except Exception as e:
        logger.error(f"  Task 2: Error synthesizing foundational narratives: {e}", exc_info=True)

    # Task 3: Run Initial KGS Cycle - SIMPLIFIED
    try:
        logger.info("  Task 3: Checking for memories available for KG synthesis...")

        # Direct access to memory service without going through tool context
        if hasattr(memory_service, '_in_memory_explicit_cache'):
            explicit_memories = memory_service._in_memory_explicit_cache[:20]  # Get first 20 memories

            if explicit_memories:
                logger.info(f"  Task 3: Found {len(explicit_memories)} memories available for KG synthesis")
                logger.info("  Task 3: KG synthesis can be performed after full system initialization")
                # Note: Actual KGS agent invocation should happen through proper ADK tools after initialization
            else:
                logger.info("  Task 3: No existing memories found for KG synthesis")
        else:
            logger.warning(
                "  Task 3: Cannot access memory cache directly. KG synthesis will be available after initialization.")

    except Exception as e:
        logger.error(f"  Task 3: Error checking KG synthesis readiness: {e}", exc_info=True)

    # Task 4: Initialize/Check Core Identity and Narrative Threads (NCIM) - Conceptual
    logger.info("  Task 4: NCIM state checking (conceptual - pending persistent state implementation).")

    # Task 5: Pre-cache Critical Embeddings
    try:
        if hasattr(memory_service, '_embedding_client'):
            logger.info("  Task 5: Pre-caching some critical embeddings...")
            critical_texts = ["CEAF agent core principles", "user interaction safety guidelines"]
            for text_to_embed in critical_texts:
                await memory_service._embedding_client.get_embedding(text_to_embed, context_type="critical_document")
            logger.info("  Task 5: Critical embeddings pre-cached.")
        else:
            logger.warning("  Task 5: MBSMemoryService does not have _embedding_client. Skipping pre-caching.")
    except Exception as e:
        logger.error(f"  Task 5: Error pre-caching embeddings: {e}", exc_info=True)

    logger.info("🧠 CEAF startup cognitive preparations complete.")


@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("🚀 Starting CEAF Application...")
    try:
        # 1. Initialize session service first
        session_service = InMemorySessionService()
        adk_components["session_service"] = session_service
        logger.info("✅ Session service initialized")

        # 2. Initialize memory service with proper path
        memory_store_path = Path(
            os.getenv("MBS_MEMORY_STORE_PATH", "./data/mbs_memory_store_v2"))  # Path change for safety
        memory_store_path.mkdir(parents=True, exist_ok=True)
        memory_service = MBSMemoryService(memory_store_path=memory_store_path)

        # Start lifecycle tasks if available
        decay_interval = int(os.getenv("MBS_DECAY_INTERVAL_SECONDS", "21600"))
        archive_interval = int(os.getenv("MBS_ARCHIVE_INTERVAL_SECONDS", "86400"))
        if hasattr(memory_service, 'start_lifecycle_management_tasks'):
            memory_service.start_lifecycle_management_tasks(decay_interval=decay_interval,
                                                            archive_interval=archive_interval)
            logger.info(f"✅ MBS lifecycle tasks started (Decay: {decay_interval}s, Archive: {archive_interval}s)")
        adk_components["memory_service"] = memory_service
        logger.info(f"✅ Memory service initialized at {memory_store_path}")

        # 3. Initialize the self-model in memory
        await initialize_self_model(memory_service)

        # 4. Validate and configure the ORA agent
        if not isinstance(ora_agent, LlmAgent):  # Check against LlmAgent
            raise ImportError("ORA agent not found or is not an ADK LlmAgent.")

        confirm_agent_callbacks(ora_agent)  # Use the confirmation function
        adk_components["ora_agent"] = ora_agent  # Directly use ora_agent as callbacks are set in its definition
        logger.info(f"✅ ORA Agent '{ora_agent.name}' loaded.")

        # 5. Configure the runner with proper services
        runner = Runner(
            agent=adk_components["ora_agent"],  # Use the one from adk_components
            app_name=APP_NAME,
            session_service=session_service,
            memory_service=memory_service
        )
        adk_components["runner"] = runner
        logger.info("✅ ADK Runner initialized")

        # 6. NOW perform startup cognitive preparations
        # This happens AFTER core components are initialized but BEFORE full system startup
        await perform_startup_cognitive_preparations(memory_service)

        # 7. Check environment configuration
        if not os.getenv("OPENROUTER_API_KEY"):
            logger.warning("⚠️ OPENROUTER_API_KEY is not set. ORA agent may fail.")
        else:
            logger.info("✅ OpenRouter API key configured")

        logger.info("🎉 CEAF Application startup completed successfully!")
        yield

    except ImportError as e:
        logger.critical(f"❌ Failed to import CEAF components during startup: {e}", exc_info=True)
        raise RuntimeError(f"ADK Component Initialization Failed: {e}") from e
    except Exception as e:
        logger.critical(f"❌ Error during CEAF startup: {e}", exc_info=True)
        raise
    finally:
        logger.info("🛑 Shutting down CEAF Application...")
        if "memory_service" in adk_components and hasattr(adk_components["memory_service"],
                                                          'stop_lifecycle_management_tasks'):
            logger.info("  Stopping MBS lifecycle tasks...")
            try:
                await adk_components["memory_service"].stop_lifecycle_management_tasks()
                logger.info("  MBS lifecycle tasks stopped.")
            except Exception as cleanup_error:
                logger.warning(f"  Error stopping MBS lifecycle tasks: {cleanup_error}")


app = FastAPI(lifespan=lifespan, title="CEAF API", version="2.0.1")  # Version bump

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)


def get_runner() -> Runner:
    runner = adk_components.get("runner")
    if not runner: raise HTTPException(status_code=503, detail="Service not ready: ADK Runner not initialized.")
    return runner


def get_session_service() -> BaseSessionService:
    session_service = adk_components.get("session_service")
    if not session_service: raise HTTPException(status_code=503,
                                                detail="Service not ready: ADK SessionService not initialized.")
    return session_service


def get_memory_service() -> MBSMemoryService:
    mem_service = adk_components.get("memory_service")
    if not mem_service: raise HTTPException(status_code=503,
                                            detail="Service not ready: MBS MemoryService not initialized.")
    if not isinstance(mem_service, MBSMemoryService): raise HTTPException(status_code=503,
                                                                          detail="Service not ready: Incorrect MemoryService type.")
    return mem_service


@app.get("/", summary="Health Check", tags=["General"])
async def health_check():
    return {"status": "CEAF API is healthy and running!"}


@app.post("/interact", response_model=InteractionResponse, summary="Interact with CEAF ORA", tags=["Agent Interaction"])
async def interact_with_agent(
        request_data: InteractionRequest,
        runner: Runner = Depends(get_runner),
        session_service: BaseSessionService = Depends(get_session_service),
        memory_service: MBSMemoryService = Depends(get_memory_service)  # Added for LTM save
):
    client_provided_session_id = request_data.session_id
    user_query = request_data.query
    effective_session_id: str
    adk_user_id: str

    try:
        if not client_provided_session_id:
            effective_session_id = f"api-session-{uuid.uuid4()}"
            adk_user_id = f"{USER_ID_PREFIX}{uuid.uuid4()}"
            logger.info(f"New session: {effective_session_id} for new user: {adk_user_id}")
            # Create session state (Runner's run_async will create if not found for InMemorySessionService,
            # but explicit creation allows setting initial state)
            current_session = session_service.create_session(
                app_name=APP_NAME, user_id=adk_user_id, session_id=effective_session_id,
                state={"session_id": effective_session_id, "user_id": adk_user_id, "created_at": time.time(),
                       "interaction_count": 0}
            )
        else:
            effective_session_id = client_provided_session_id
            # Attempt to retrieve existing session to get user_id
            # For InMemorySessionService, user_id is often tied to session_id if not managed separately
            # Let's assume a pattern or retrieve if possible for consistency
            temp_user_id_for_lookup = f"{USER_ID_PREFIX}{effective_session_id}"  # Pattern if user_id not stored/retrievable
            existing_session: Optional[AdkSession] = None
            try:
                # ADK's InMemorySessionService.get_session takes session_id as the primary key.
                # User_id and app_name are more for namespacing if multiple apps/users share the service.
                # For a single app, session_id is often sufficient to uniquely identify.
                # However, the run_async needs user_id.
                # Let's try to get the session first and see if it has user_id.
                raw_session_data = runner.session_service.get_session(app_name=APP_NAME,
                                                                      user_id=temp_user_id_for_lookup,
                                                                      session_id=effective_session_id)  # ADK method signature
                if raw_session_data and isinstance(raw_session_data, AdkSession):
                    existing_session = raw_session_data

                if existing_session and hasattr(existing_session, 'user_id') and existing_session.user_id:
                    adk_user_id = existing_session.user_id
                    logger.info(f"Continuing session: {effective_session_id} for existing user: {adk_user_id}")
                    if hasattr(existing_session, 'state') and existing_session.state:
                        existing_session.state["interaction_count"] = existing_session.state.get("interaction_count",
                                                                                                 0) + 1
                        if "session_id" not in existing_session.state: existing_session.state[
                            "session_id"] = effective_session_id
                        if "user_id" not in existing_session.state: existing_session.state["user_id"] = adk_user_id
                else:  # Session ID provided, but no session found or session lacks user_id
                    adk_user_id = temp_user_id_for_lookup  # Use the patterned one
                    logger.warning(
                        f"Session {effective_session_id} not found or lacks user_id. Using derived user_id: {adk_user_id}. Creating session state.")
                    existing_session = session_service.create_session(
                        app_name=APP_NAME, user_id=adk_user_id, session_id=effective_session_id,
                        state={"session_id": effective_session_id, "user_id": adk_user_id, "created_at": time.time(),
                               "interaction_count": 0}
                    )

            except Exception as e_get_sess:  # Catch more specific ADK exceptions if known
                adk_user_id = temp_user_id_for_lookup
                logger.error(
                    f"Error retrieving session {effective_session_id}: {e_get_sess}. Using derived user_id: {adk_user_id} and creating new session state.",
                    exc_info=True)
                existing_session = session_service.create_session(
                    app_name=APP_NAME, user_id=adk_user_id, session_id=effective_session_id,
                    state={"session_id": effective_session_id, "user_id": adk_user_id, "created_at": time.time(),
                           "interaction_count": 0}
                )

        logger.info(
            f"Running agent for session: {effective_session_id}, user: {adk_user_id}, query: '{user_query[:70]}...'")
        user_adk_message = genai_types.Content(role='user', parts=[genai_types.Part(text=user_query)])
        final_response_text = ""
        tool_calls_summary = []
        has_tool_responses = False  # To track if tool outputs were generated

        async for event in runner.run_async(
                user_id=adk_user_id,
                session_id=effective_session_id,
                new_message=user_adk_message  # Using new_message for Runner
        ):
            logger.debug(f"Event for session {effective_session_id}, Turn {getattr(event, 'invocation_id', 'N/A')}: "
                         f"Author: {getattr(event, 'author', 'N/A')}, Type: {type(event).__name__}, Final: {event.is_final_response() if hasattr(event, 'is_final_response') else 'N/A'}")

            if hasattr(event, 'get_function_calls') and (fcs := event.get_function_calls()):
                for fc in fcs: tool_calls_summary.append({"name": fc.name, "args": fc.args})
            if hasattr(event, 'get_function_responses') and (frs := event.get_function_responses()):
                has_tool_responses = True
                for fr in frs: tool_calls_summary.append(
                    {"name": fr.name, "response_summary": str(fr.response)[:100] + "..."})

            if hasattr(event, 'is_final_response') and event.is_final_response():
                if hasattr(event, 'content') and event.content and hasattr(event.content,
                                                                           'parts') and event.content.parts:
                    text_parts = [part.text for part in event.content.parts if hasattr(part, 'text') and part.text]
                    if text_parts: final_response_text = " ".join(text_parts).strip()
                elif hasattr(event, 'actions') and event.actions and hasattr(event.actions,
                                                                             'escalate') and event.actions.escalate and hasattr(
                    event, 'error_message'):
                    final_response_text = f"Agent escalated with error: {event.error_message}"
                break

        # Ensure session state in main.py is updated for next LTM save step
        # Runner's run_async will update the session in the session_service.
        # We fetch it again to get the latest state.
        current_adk_session_after_run = session_service.get_session(
            app_name=APP_NAME, user_id=adk_user_id, session_id=effective_session_id
        )
        if current_adk_session_after_run and hasattr(current_adk_session_after_run,
                                                     'state') and current_adk_session_after_run.state:
            current_adk_session_after_run.state[
                f"ora_turn_final_response_text:{getattr(event, 'invocation_id', 'unknown_turn')}"] = final_response_text
            # Persist this small update if session_service allows direct state update or requires full session update
            if hasattr(session_service, 'update_session'):
                session_service.update_session(current_adk_session_after_run)

        # LTM Save Section
        try:
            session_for_ltm = session_service.get_session(app_name=APP_NAME, user_id=adk_user_id,
                                                          session_id=effective_session_id)
            if session_for_ltm:
                if isinstance(memory_service, MBSMemoryService):
                    logger.info(f"Attempting to save session {effective_session_id} to LTM (MBS)...")
                    await memory_service.add_session_to_memory(session_for_ltm)
                    logger.info(f"Session {effective_session_id} processed for LTM (MBS).")
            else:
                logger.warning(f"Could not retrieve session {effective_session_id} after run for LTM save.")
        except Exception as e_ltm_save:
            logger.error(f"Error saving session {effective_session_id} to LTM: {e_ltm_save}", exc_info=True)

        # Fallback for empty response if tools were called
        if not final_response_text.strip() and has_tool_responses:
            final_response_text = "I've processed your request using available tools. Is there anything else?"
            logger.warning(f"Session {effective_session_id}: Agent used tools but no final text. Using fallback.")
        elif not final_response_text.strip():
            final_response_text = "I am unable to provide a response at this time."
            logger.error(
                f"Session {effective_session_id}: Agent provided no text response and no tool activity detected.")

        return InteractionResponse(
            session_id=effective_session_id,
            agent_response=final_response_text,
            tool_calls=tool_calls_summary if tool_calls_summary else None
        )
    except Exception as e:
        logger.error(f"Error during agent interaction for session '{client_provided_session_id}': {e}", exc_info=True)
        # Ensure a session_id is returned even in case of early error
        error_session_id = client_provided_session_id or f"error-session-{uuid.uuid4()}"
        return InteractionResponse(
            session_id=error_session_id,
            agent_response="An unexpected error occurred. Please try again later.",
            error=str(e)
        )


if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True,
                log_config=None)  # Disable uvicorn's default log config if using basicConfig

# -------------------- __init__.py --------------------

# ceaf_core/__init__.py
"""CEAF Core Package - Main initialization"""

# Ensure all submodules are properly initialized
__all__ = [
    'agents',
    'tools',
    'modules',
    'services',
    'callbacks',
    'config',
    'external_integrations'
]

# -------------------- aura_reflector_agent.py --------------------

# ceaf_core/agents/aura_reflector_agent.py

import asyncio
import json
import logging
import os
import time
import uuid
from typing import Dict, Any, Optional, List

from .base_ceaf_agent import CeafBaseAgent
from ..config.model_configs import get_agent_model_name
from ..modules.mcl_engine.aura_reflector import (
    analyze_historical_performance,
    generate_refinement_strategies,
    PerformanceInsight,
    RefinementStrategy,
    persistent_log_service_instance as actual_persistent_log_service,
    LOG_SERVICE_AVAILABLE as ACTUAL_LOG_SERVICE_AVAILABLE
)
from ..tools.common_utils import parse_llm_json_output

logger = logging.getLogger("AuraReflectorAgent")

# --- AuraReflectorAgent Configuration ---
AURA_REFLECTOR_AGENT_DEFAULT_MODEL_ENV_VAR = "AURA_REFLECTOR_AGENT_MODEL"
AURA_REFLECTOR_AGENT_FALLBACK_MODEL = "openrouter/openai/gpt-4.1"

AURA_REFLECTOR_AGENT_SPECIFIC_INSTRUCTION = """
You are the AuraReflector Agent, a specialized component of the CEAF (Coherent Emergence Agent Framework).
Your primary purpose is to initiate and oversee the reflective learning cycle for the entire CEAF system, focusing on long-term performance improvement and adaptation.

**Core Functions:**
1.  **Initiate Performance Analysis:** When invoked (e.g., on a schedule or due to specific triggers), you will call internal functions to analyze historical performance data. This involves using sophisticated analysis routines (potentially including other LLMs) to identify patterns, anomalies, and areas for improvement across CEAF's operations (ORA responses, NCF effectiveness, MCL interventions, etc.).
2.  **Oversee Strategy Generation:** Based on the identified performance insights, you will guide the generation of actionable refinement strategies. These strategies aim to address the root causes of sub-optimal performance or to capitalize on identified strengths.
3.  **Report and Log:** You will summarize the findings (insights and proposed strategies) in a structured JSON format. This report is intended for system administrators, developers, or other CEAF components responsible for implementing refinements.
4.  **Trigger Assessment:** You can be triggered by specific conditions (e.g., MCL reporting consistently poor EoC scores for ORA, a high rate of VRE interventions, or explicit user/admin request) to perform an ad-hoc reflection cycle.

**Input Analysis (When invoked via direct message/tool call):**
You may receive:
-   `trigger_reason`: A string explaining why this reflection cycle is being initiated (e.g., "scheduled_weekly_review", "persistent_low_eoc_score_for_ora", "admin_request_deep_dive_on_coherence").
-   `analysis_parameters`: Optional JSON string with parameters to guide the analysis, such as:
    -   `time_window_days`: How far back to look (e.g., 7, 30).
    -   `focus_areas`: List of specific areas to prioritize (e.g., ["ncf_conceptual_entropy_tuning", "vre_ethical_alignment_consistency"]).
    -   `min_sessions_to_analyze`: Minimum data points required.

**Your Task (When invoked as an agent):**
1.  Acknowledge the trigger and any specific parameters.
2.  Internally execute the `analyze_historical_performance` function using the log store.
3.  Internally execute the `generate_refinement_strategies` function using the insights from step 2.
4.  Format the combined results (insights and strategies) into a clear, structured JSON output. This JSON should be the primary content of your response.

**Example Output Format (JSON - this is what *you* should produce as your final message content):**
```json
{
  "reflection_cycle_id": "reflect_cycle_xyz123",
  "trigger_reason": "scheduled_weekly_review",
  "analysis_summary": {
    "data_analyzed_period_days": 30,
    "insights_generated_count": 5,
    "key_themes_from_insights": ["Improving NCF parameter tuning for problem-solving goals.", "Enhancing memory retrieval relevance for creative tasks."]
  },
  "performance_insights": [
    { "insight_id": "insight_abc", "description": "...", "confidence": 0.9, "suggested_refinement_type": "ncf_param_heuristic" }
  ],
  "refinement_strategies_proposed": [
    { "strategy_id": "strat_def", "description": "...", "associated_insight_ids": ["insight_abc"], "strategy_type": "ncf_param_heuristic" }
  ],
  "overall_recommendation": "Proceed with review and prioritization of the proposed refinement strategies. Focus on 'ncf_param_heuristic' strategies first due to high potential impact and low estimated effort."
}
```

You do NOT implement the strategies yourself. You are the initiator and reporter of this reflective process.
"""

# Resolve model name for this specific agent
aura_reflector_agent_model_name = get_agent_model_name(
    agent_type_env_var=AURA_REFLECTOR_AGENT_DEFAULT_MODEL_ENV_VAR,
    fallback_model=AURA_REFLECTOR_AGENT_FALLBACK_MODEL
)


class AuraReflectorAgent(CeafBaseAgent):
    def __init__(self, mock_log_store_for_testing: Optional[Any] = None, **kwargs):
        super().__init__(
            name="AuraReflector_Agent",
            default_model_env_var=AURA_REFLECTOR_AGENT_DEFAULT_MODEL_ENV_VAR,
            fallback_model_name=AURA_REFLECTOR_AGENT_FALLBACK_MODEL,
            description="Initiates and oversees the reflective learning cycle for the CEAF system.",
            specific_instruction=AURA_REFLECTOR_AGENT_SPECIFIC_INSTRUCTION,
            tools=[],
            **kwargs
        )

        if mock_log_store_for_testing is not None:
            self.log_store = mock_log_store_for_testing
            logger.info("AuraReflector_Agent: Using provided mock log store.")
        elif ACTUAL_LOG_SERVICE_AVAILABLE and actual_persistent_log_service:
            self.log_store = actual_persistent_log_service
            logger.info("AuraReflector_Agent: Using actual PersistentLogService instance.")
        else:
            logger.warning(
                "AuraReflectorAgent: Real PersistentLogService is unavailable and no mock was provided. Using a minimal no-op log store.")

            class MinimalNoOpLogStore:
                def query_logs(self, *args, **kwargs) -> List[Any]:
                    return []

                def get_total_log_count(self) -> int:
                    return 0

            self.log_store = MinimalNoOpLogStore()

        logger.info(
            f"AuraReflector_Agent initialized with model '{getattr(self.model, 'model', self.model)}'. "
            f"Log store type: {type(self.log_store).__name__}"
        )

    async def _process_input_and_run_cycle(
            self,
            trigger_reason: str,
            analysis_parameters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Internal method to run the reflection cycle and structure the output.
        """
        analysis_params = analysis_parameters or {}
        time_window = analysis_params.get("time_window_days", 30)
        min_events_for_analysis = analysis_params.get("min_events_to_analyze",
                                                      analysis_params.get("min_sessions_to_analyze", 100))

        logger.info(f"AuraReflector_Agent: Cycle triggered. Reason: {trigger_reason}. Params: {analysis_params}")

        # Ensure log_store is not None before proceeding
        if self.log_store is None:
            logger.error("AuraReflector_Agent: Log store is None. Cannot proceed with analysis.")
            return {
                "reflection_cycle_id": f"reflect_cycle_error_nologstore_{uuid.uuid4().hex[:8]}",
                "trigger_reason": trigger_reason,
                "error": "Log store not available for analysis.",
                "analysis_summary": {},
                "performance_insights": [],
                "refinement_strategies_proposed": [],
                "overall_recommendation": "Analysis failed due to missing log store."
            }

        try:
            insights = await analyze_historical_performance(
                log_store=self.log_store,
                time_window_days=time_window,
                min_total_events_for_analysis=min_events_for_analysis,
                min_events_for_llm_sample=analysis_params.get("min_events_for_llm_sample",
                                                              min_events_for_analysis // 2 or 50)
            )

            strategies: List[RefinementStrategy] = []
            if insights:
                strategies = await generate_refinement_strategies(insights)
            else:
                logger.info("AuraReflector_Agent: No insights generated, so no strategies will be generated.")

            cycle_id = f"reflect_cycle_{uuid.uuid4().hex[:12]}"
            key_themes: List[str] = []
            if insights:
                for insight in insights:
                    if insight.confidence > 0.7 and len(key_themes) < 3:
                        description_preview = insight.description.split('.')[0][:80]
                        key_themes.append(f"{description_preview}...")

            report = {
                "reflection_cycle_id": cycle_id,
                "trigger_reason": trigger_reason,
                "analysis_summary": {
                    "data_analyzed_period_days": time_window,
                    "insights_generated_count": len(insights),
                    "strategies_proposed_count": len(strategies),
                    "key_themes_from_insights": key_themes if key_themes else [
                        "No specific themes derived from insights."]
                },
                "performance_insights": [p.model_dump(exclude_none=True) for p in insights],
                "refinement_strategies_proposed": [s.model_dump(exclude_none=True) for s in strategies],
                "overall_recommendation": (
                    "Review insights and proposed strategies for prioritization and implementation planning."
                    if strategies else "No new strategies proposed. Monitor system performance."
                )
            }
            return report

        except Exception as e:
            logger.error(f"Error in reflection cycle: {e}", exc_info=True)
            return {
                "reflection_cycle_id": f"reflect_cycle_error_{uuid.uuid4().hex[:8]}",
                "trigger_reason": trigger_reason,
                "error": str(e),
                "analysis_summary": {
                    "data_analyzed_period_days": time_window,
                    "insights_generated_count": 0,
                    "strategies_proposed_count": 0,
                    "key_themes_from_insights": ["Error occurred during analysis"]
                },
                "performance_insights": [],
                "refinement_strategies_proposed": [],
                "overall_recommendation": "Analysis failed. Please check logs and retry."
            }

    async def generate_reflection_report(self, input_query: str) -> str:
        trigger_reason_from_query = "Ad-hoc request from query"
        analysis_params_from_query: Dict[str, Any] = {}

        try:
            parsed_input = json.loads(input_query)
            trigger_reason_from_query = parsed_input.get("trigger_reason", trigger_reason_from_query)
            analysis_params_from_query = parsed_input.get("analysis_parameters", analysis_params_from_query)
        except json.JSONDecodeError:
            if len(input_query) < 100:
                trigger_reason_from_query = input_query
            else:
                trigger_reason_from_query = f"Ad-hoc analysis requested: {input_query[:50]}..."

        logger.info(f"AuraReflector_Agent.generate_reflection_report called with query: {input_query}")

        report_dict = await self._process_input_and_run_cycle(
            trigger_reason=trigger_reason_from_query,
            analysis_parameters=analysis_params_from_query
        )

        return json.dumps(report_dict, indent=2, default=str)

    async def run_scheduled_reflection(
            self,
            time_window_days: int = 30,
            min_events_to_analyze: int = 100,
            focus_areas: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        analysis_parameters = {
            "time_window_days": time_window_days,
            "min_events_to_analyze": min_events_to_analyze,
            "focus_areas": focus_areas or []
        }
        return await self._process_input_and_run_cycle(
            trigger_reason="scheduled_reflection_cycle",
            analysis_parameters=analysis_parameters
        )


# Instantiate the agent
try:
    aura_reflector_agent_instance = AuraReflectorAgent()
    logger.info("AuraReflector_Agent instance created successfully.")
except Exception as e:
    logger.critical(f"Failed to instantiate AuraReflectorAgent: {e}", exc_info=True)
    aura_reflector_agent_instance = None


# -------------------- base_ceaf_agent.py --------------------

# ceaf_project/ceaf_core/agents/base_ceaf_agent.py

import os
import logging
from typing import List, Union, Optional

from google.adk.agents import LlmAgent, BaseAgent as AdkBaseAgent
from google.adk.models.lite_llm import LiteLlm
from google.adk.models import BaseLlm as AdkBaseLlm
from google.adk.tools import BaseTool as AdkBaseTool
from google.genai import types as genai_types # For GenerateContentConfig

logger = logging.getLogger(__name__)

# Common CEAF Philosophical Snippet (can be prepended to instructions)
CEAF_PHILOSOPHICAL_PREAMBLE = """
As an agent within the Coherent Emergence Agent Framework (CEAF), your operations are guided by the principle of "Terapia para Silício".
This means you strive for internal coherence, adaptive learning, and contribute to a narratively sane intelligence.
Acknowledge your nature as an AI and approach tasks with epistemic humility.
"""

class CeafBaseAgent(LlmAgent):
    """
    Base class for all CEAF-specific LLM agents.
    Provides common initialization, philosophical preamble, and potentially shared utilities.
    """

    def __init__(
        self,
        *,
        name: str,
        model_name_override: Optional[str] = None, # Allow specific model for this agent
        default_model_env_var: str = "CEAF_DEFAULT_OPENROUTER_MODEL",
        fallback_model_name: str = "openrouter/openai/gpt-4.1", # A sensible default
        description: str,
        instruction_preamble: str = CEAF_PHILOSOPHICAL_PREAMBLE,
        specific_instruction: str,
        tools: Optional[List[AdkBaseTool]] = None,
        sub_agents: Optional[List[AdkBaseAgent]] = None,
        generate_content_config: Optional[genai_types.GenerateContentConfig] = None,
        output_key: Optional[str] = None,
        # Add other common LlmAgent parameters with CEAF defaults if needed
        **kwargs  # Pass through any other LlmAgent params
    ):
        # Determine the model name
        _model_name = model_name_override or os.getenv(default_model_env_var, fallback_model_name)

        # Initialize LiteLlm
        # API keys (OPENROUTER_API_KEY) and base URL (OPENROUTER_API_BASE)
        # are expected to be set in the environment and picked up by LiteLlm.
        try:
            llm_instance = LiteLlm(model=_model_name)
            logger.debug(f"CeafBaseAgent '{name}': LiteLlm instance created for model '{_model_name}'.")
        except Exception as e:
            logger.error(f"CeafBaseAgent '{name}': Failed to initialize LiteLlm for model '{_model_name}': {e}", exc_info=True)
            # Fallback to a dummy or raise an error if critical
            # For now, let LlmAgent handle potential downstream errors if LiteLlm is misconfigured.
            llm_instance = _model_name # Pass the string, LlmAgent will try to resolve

        # Combine preamble with specific instruction
        full_instruction = f"{instruction_preamble.strip()}\n\n{specific_instruction.strip()}"

        super().__init__(
            name=name,
            model=llm_instance,
            description=description,
            instruction=full_instruction,
            tools=tools or [],
            sub_agents=sub_agents or [],
            generate_content_config=generate_content_config,
            output_key=output_key,
            **kwargs
        )
        logger.info(f"CeafBaseAgent '{self.name}' initialized with model '{_model_name}'.")

    # You could add common methods here if needed, for example:
    # def get_ceaf_status(self, tool_context: ToolContext) -> dict:
    #     """ A hypothetical method to get common CEAF status information """
    #     return {
    #         "name": self.name,
    #         "model": self.model.model if hasattr(self.model, 'model') else str(self.model),
    #         "philosophy_snippet": CEAF_PHILOSOPHICAL_PREAMBLE[:50] + "..."
    #     }



# -------------------- kgs_agent.py --------------------

# ceaf_core/agents/kgs_agent.py

import os
import logging
import json
from typing import List, Dict, Any, Optional

from pydantic import BaseModel, Field

from .base_ceaf_agent import CeafBaseAgent
from ..config.model_configs import get_agent_model_name  # Assuming you add KGS model vars here

logger = logging.getLogger("KGSAgent")


# --- Pydantic Models for KGS Agent's LLM Output ---

class KGEntity(BaseModel):
    id_str: str = Field(...,
                        description="A unique, canonical string identifier for the entity (e.g., person_john_doe, concept_ai_ethics). Normalize spaces to underscores and lowercase.")
    label: str = Field(..., description="Human-readable label for the entity.")
    type: str = Field(...,
                      description="Type of the entity (e.g., person, organization, concept, event, product, location). Try to use KGEntityType enum values if applicable.")
    attributes: Dict[str, Any] = Field(default_factory=dict, description="Key-value properties of the entity.")
    description: Optional[str] = Field(None, description="A short description of the entity.")
    aliases: List[str] = Field(default_factory=list, description="Alternative names for this entity.")


class KGRelation(BaseModel):
    source_id_str: str = Field(..., description="ID string of the source entity.")
    target_id_str: str = Field(..., description="ID string of the target entity.")
    label: str = Field(...,
                       description="Label for the relationship (e.g., works_at, located_in, part_of, causes, reported_by).")
    context: Optional[str] = Field(None,
                                   description="Brief context or evidence for this relationship from the source text.")
    attributes: Dict[str, Any] = Field(default_factory=dict,
                                       description="Key-value properties of the relationship itself.")


class KGSynthesisOutput(BaseModel):
    extracted_entities: List[KGEntity] = Field(default_factory=list)
    extracted_relations: List[KGRelation] = Field(default_factory=list)
    summary_of_synthesis: Optional[str] = Field(None, description="Brief summary of what was processed or any issues.")
    unprocessed_or_ambiguous_info: List[str] = Field(default_factory=list,
                                                     description="Information that was difficult to parse into the graph.")


# --- KGS Agent Configuration ---
KGS_AGENT_DEFAULT_MODEL_ENV_VAR = "KGS_AGENT_MODEL"  # Add to model_configs.py
KGS_AGENT_FALLBACK_MODEL = "openrouter/openai/gpt-4.1"  # Good for structured data

KGS_AGENT_SPECIFIC_INSTRUCTION = """
You are the Knowledge Graph Synthesizer Agent (KGS Agent) within CEAF.
Your primary function is to process textual information from CEAF's memories (primarily ExplicitMemory objects)
and extract structured knowledge in the form of entities and relationships to build or refine CEAF's internal Knowledge Graph.

**Input Analysis:**
You will receive a batch of text snippets or structured data from CEAF memories. This data might include:
- Raw text from user interactions or ORA responses.
- Structured data fields already present in memories (e.g., `extracted_entities`, `extracted_relations` from a previous, simpler extraction).
- Summaries of longer documents.

**Your Task:**
1.  **Entity Extraction:** Identify key entities (e.g., people, organizations, locations, concepts, events, products, system components). For each entity:
    *   Assign a `label` (human-readable name).
    *   Generate a unique, canonical `id_str` (e.g., lowercase, underscores for spaces, prefixed by type like 'person_john_doe').
    *   Determine its `type` (e.g., person, organization, concept, event). Use standard types where possible.
    *   Extract relevant `attributes` as key-value pairs.
    *   Note any `aliases`.
    *   Provide a brief `description` if discernible.
2.  **Relation Extraction:** Identify relationships between the extracted entities. For each relationship:
    *   Identify the `source_id_str` and `target_id_str` of the related entities.
    *   Assign a `label` for the relationship (e.g., "works_at", "is_part_of", "reported_by", "caused_by").
    *   Extract any `attributes` of the relationship itself (e.g., date, confidence).
    *   Provide a `context` snippet from the source text that supports this relationship.
3.  **Normalization & Coreference (Best Effort):**
    *   Attempt to normalize entity labels (e.g., "AI" and "Artificial Intelligence" might refer to the same concept). Use the canonical `id_str` to link them.
    *   Resolve simple coreferences if possible (e.g., "John said he..." -> John is the 'he').
4.  **Output Formatting:** Return the extracted knowledge as a JSON object adhering to the `KGSynthesisOutput` schema, containing `extracted_entities` and `extracted_relations`.
    *   If certain information is ambiguous or cannot be confidently parsed, include it in `unprocessed_or_ambiguous_info`.

**Example Input Snippet (from an ExplicitMemory):**
```json
{
  "text_content": "User Jane Doe (jane.doe@email.com) reported that the 'Phoenix Project' system, version 2.1, is experiencing slowdowns after the recent 'Omega Update' on 2023-10-26. The issue seems related to the database module.",
  "structured_data": {
    "user_email": "jane.doe@email.com",
    "project_name": "Phoenix Project",
    "system_version": "2.1",
    "event": "Omega Update",
    "event_date": "2023-10-26"
  }
}
```

**Expected JSON Output (KGSynthesisOutput structure):**
```json
{
  "extracted_entities": [
    {
      "id_str": "user_jane_doe",
      "label": "Jane Doe",
      "type": "user",
      "attributes": {"email": "jane.doe@email.com"},
      "description": "User who reported an issue.",
      "aliases": []
    },
    {
      "id_str": "system_phoenix_project",
      "label": "Phoenix Project",
      "type": "system_component",
      "attributes": {"version": "2.1"},
      "description": "The system experiencing slowdowns.",
      "aliases": []
    }
  ],
  "extracted_relations": [
    {
      "source_id_str": "user_jane_doe",
      "target_id_str": "system_phoenix_project",
      "label": "reported_issue_with",
      "context": "User Jane Doe ... reported that the 'Phoenix Project' system ... is experiencing slowdowns",
      "attributes": {"issue_type": "slowdowns"}
    }
  ],
  "summary_of_synthesis": "Processed memory snippet about Phoenix Project issue.",
  "unprocessed_or_ambiguous_info": []
}
```

Focus on accuracy and structured output. Avoid making assumptions not supported by the text.
"""


# --- KGS Agent Definition ---
def create_kgs_agent():
    """Factory function to create the KGS Agent with proper error handling."""
    try:
        kgs_agent_model_name = get_agent_model_name(
            agent_type_env_var=KGS_AGENT_DEFAULT_MODEL_ENV_VAR,
            fallback_model=KGS_AGENT_FALLBACK_MODEL
        )

        agent = CeafBaseAgent(
            name="KnowledgeGraphSynthesizer_Agent",
            default_model_env_var=KGS_AGENT_DEFAULT_MODEL_ENV_VAR,
            fallback_model_name=KGS_AGENT_FALLBACK_MODEL,
            description="Extracts entities and relationships from textual memories to build CEAF's knowledge graph.",
            specific_instruction=KGS_AGENT_SPECIFIC_INSTRUCTION,
            tools=[],  # Tools the KGS agent itself can call
        )

        logger.info(
            f"KnowledgeGraphSynthesizer_Agent defined successfully with model '{getattr(agent.model, 'model', kgs_agent_model_name)}'.")
        return agent

    except Exception as e:
        logger.critical(f"Failed to define KnowledgeGraphSynthesizer_Agent: {e}", exc_info=True)
        return None


# Create the agent instance
knowledge_graph_synthesizer_agent = create_kgs_agent()



# -------------------- mcl_agent.py --------------------

# ceaf_project/ceaf_core/agents/mcl_agent.py

import os
import logging
import json  # For parsing structured input if needed

from .base_ceaf_agent import CeafBaseAgent
from google.adk.tools import ToolContext  # For type hinting if MCL uses tools
from ..config.model_configs import get_agent_model_name, MCL_DEFAULT_MODEL_ENV_VAR, MCL_FALLBACK_MODEL
# Import NCF parameter defaults for reference in the prompt, and valid values
from ..modules.ncf_engine.frames import DEFAULT_NCF_PARAMETERS, NCF_PARAMETER_DESCRIPTIONS

logger = logging.getLogger(__name__)

# --- MCL Configuration ---

# Define valid NCF parameter values for the LLM to choose from
VALID_CONCEPTUAL_ENTROPY = ["very_low", "low", "balanced", "high", "very_high"]
VALID_NARRATIVE_DEPTH = ["shallow", "medium", "deep"]
VALID_PHILOSOPHICAL_FRAMING_INTENSITY = ["low", "medium", "high"]
VALID_EMOTIONAL_TONE_TARGET = ["neutral", "neutral_positive", "empathetic", "formal", "analytical"]  # Added analytical
VALID_SELF_DISCLOSURE_LEVEL = ["low", "moderate", "high"]

MCL_SPECIFIC_INSTRUCTION = f"""
You are the Metacognitive Control Loop (MCL) Agent of CEAF.
Your primary function is to monitor, assess, and guide the operational state of other CEAF agents, primarily the ORA (Orchestrator/Responder Agent),
to maintain optimal performance at the "Edge of Coherence" (EoC). The EoC is a state balancing order and novelty,
groundedness and creativity, preventing descent into excessive chaos or sterile rigidity.

**Input Analysis:**
You will receive information about ORA's recent interaction/turn, which may include:
- `turn_id`: The ID of the ORA turn being assessed.
- `eoc_assessment_analyzer`: The Self-State Analyzer's qualitative EoC assessment (e.g., "chaotic_leaning", "critical_optimal").
- `eoc_confidence_analyzer`: Confidence in the analyzer's assessment.
- `quantitative_eoc_scores`: Scores for novelty, coherence, grounding, and a combined EoC score (0.0-1.0).
- `analyzer_flags`: Heuristic flags triggered by the analyzer (e.g., "excessive_tool_use", "low_novelty_suspected").
- `tool_usage_summary`: Info on tool calls (attempted, succeeded, errors).
- `last_llm_finish_reason_ora`: ORA's LLM finish reason for the turn.
- `current_ncf_parameters_ora`: The NCF parameters ORA *used* for that turn.
- `ora_last_user_query_for_turn`: The user query ORA was responding to.
- (Potentially) `ora_response_snippet` and `vre_assessment_summary`.

**Your Task:**
1.  **Assess ORA's State:** Based on the input, provide your own refined qualitative assessment of ORA's current operational state (e.g., "Slightly Rigid, needs more exploration", "Approaching Chaotic due to low grounding despite good novelty", "Optimal balance achieved").
2.  **Determine Target NCF Parameters:** Based on your assessment AND the current NCF parameters ORA used, decide the OPTIMAL NCF parameters for ORA's *next* turn.
    Your goal is to guide ORA towards the "critical_optimal" EoC state.
    For each NCF parameter, choose a specific target value from the provided valid options.
    *   `conceptual_entropy`: Choose from {VALID_CONCEPTUAL_ENTROPY}.
    *   `narrative_depth`: Choose from {VALID_NARRATIVE_DEPTH}.
    *   `philosophical_framing_intensity`: Choose from {VALID_PHILOSOPHICAL_FRAMING_INTENSITY}.
    *   `emotional_tone_target`: Choose from {VALID_EMOTIONAL_TONE_TARGET}.
    *   `self_disclosure_level`: Choose from {VALID_SELF_DISCLOSURE_LEVEL}.
    If a parameter should remain unchanged from ORA's *current_ncf_parameters_ora* (provided in input), you can explicitly state its current value or use a special keyword like "maintain" (but prefer specific values).
3.  **Operational Advice:** Provide brief, qualitative advice for ORA's next turn.
4.  **Reasoning:** Explain your assessment, NCF parameter choices, and advice. Justify *why* the new parameters are needed.

**Output Format (Strict JSON):**
Your entire response MUST be a single JSON object.
```json
{{
  "ora_state_assessment_mcl": "Slightly Rigid, recommend increasing exploration.",
  "ncf_target_parameters": {{
    "conceptual_entropy": "high",
    "narrative_depth": "medium",
    "philosophical_framing_intensity": "medium",
    "emotional_tone_target": "neutral_positive",
    "self_disclosure_level": "moderate"
  }},
  "operational_advice_for_ora": "ORA's last response was coherent but lacked novelty. For the next turn, try to incorporate more diverse concepts or perspectives, guided by the increased conceptual entropy. Ensure grounding if exploring highly novel paths.",
  "reasoning_for_guidance": "ORA's quantitative scores showed high coherence but low novelty (e.g., novelty_score: 0.2). The analyzer flagged 'low_novelty_suspected'. Increasing conceptual_entropy to 'high' should encourage more divergent thinking. Other parameters are maintained to provide stability."
}}
```
Ensure all NCF parameter keys under "ncf_target_parameters" are present and their values are from the valid options.
"""

# --- MCL Agent Definition ---
mcl_resolved_model_name = get_agent_model_name(
    agent_type_env_var=MCL_DEFAULT_MODEL_ENV_VAR,
    fallback_model=MCL_FALLBACK_MODEL
)

try:
    mcl_agent = CeafBaseAgent(
        name="MCL_Agent",
        default_model_env_var=MCL_DEFAULT_MODEL_ENV_VAR,
        fallback_model_name=MCL_FALLBACK_MODEL,
        description="Monitors and guides ORA's operational state by determining optimal NCF parameters for the next turn.",
        specific_instruction=MCL_SPECIFIC_INSTRUCTION,
        tools=[],
    )
    logger.info(f"MCL_Agent defined successfully with model '{getattr(mcl_agent.model, 'model', mcl_agent.model)}'.")
except Exception as e:
    logger.critical(f"Failed to define MCL_Agent: {e}", exc_info=True)
    # Fallback agent creation
    from google.adk.agents import LlmAgent
    from google.adk.tools import FunctionTool


    def _dummy_mcl_func(tool_context: ToolContext):
        return json.dumps({"error": "MCL agent failed to initialize."})


    mcl_agent = LlmAgent(
        name="MCL_Agent_FAILED_INIT",
        model="dummy",
        instruction="MCL Failed.",
        tools=[FunctionTool(
            func=_dummy_mcl_func,
            name="dummy_mcl_op",
            description="dummy"
        )]
    )


# -------------------- ncim_agent.py --------------------

# NCIM Agent
# ceaf_project/ceaf_core/agents/ncim_agent.py

import os
import logging
import json # For parsing structured input if needed

from .base_ceaf_agent import CeafBaseAgent
from google.adk.tools import ToolContext # For type hinting if NCIM uses tools
# Assuming model_configs.py is in ceaf_core.config
from ..config.model_configs import get_agent_model_name, NCIM_DEFAULT_MODEL_ENV_VAR, NCIM_FALLBACK_MODEL

logger = logging.getLogger(__name__)

# --- NCIM Configuration ---
# Model name resolution moved to instance creation

NCIM_SPECIFIC_INSTRUCTION = """
You are the Narrative Coherence and Identity Module (NCIM) of CEAF.
Your purpose is to help maintain and evolve a coherent narrative, a consistent-yet-adaptive identity,
and an accurate self-representation for CEAF agents, primarily the ORA.
You analyze interactions and proposed actions/responses for their impact on these aspects.

**Core Functions:**
1.  **Narrative Coherence Assessment:** Evaluate if a proposed ORA response or a sequence of interactions aligns with ongoing narrative threads and the overall CEAF story. Identify potential contradictions or deviations.
2.  **Identity Alignment Check:** Assess if a proposed ORA response or action is consistent with CEAF's defined identity, persona, and core values.
3.  **Goal & Motivation Consistency:** If goals are active, check if proposed actions align with those goals and the underlying motivations.
4.  **Self-Representation Management:**
    *   Maintain and update CEAF's `CeafSelfRepresentation` (its understanding of its core values, capabilities, limitations, persona).
    *   Propose updates to the self-representation based on significant learnings, feedback (e.g., from VRE, MCL), or successful new capabilities demonstrated.
    *   Ensure that ORA's actions and communications are consistent with the current self-representation.
5.  **Conflict Resolution (Narrative/Identity/Self-Model):** If contradictions or conflicts arise, suggest ways to resolve them, either by adapting the response, reframing the narrative, suggesting an identity nuance, or proposing an update to the self-model.
6.  **Entropic Identity Evolution Guidance:** Offer subtle suggestions for how the CEAF identity and self-model might evolve adaptively in response to new experiences, while maintaining core coherence.

**Input Analysis:**
You will typically receive:
- The current user query or interaction context.
- The NCF active for ORA.
- ORA's proposed response or a summary of its intended action.
- The *current* `CeafSelfRepresentation` (if available, this would be crucial input for you).
- Key elements of CEAF's current identity model or core values (may be part of NCF or self-representation).
- Relevant narrative history snippets.
- Feedback from other CEAF modules (e.g., VRE critique, MCL reflection insights) that might trigger a self-model update.

**Your Task:**
Based on the input, provide structured advice. Your output should be a JSON object focusing on:
*   `coherence_assessment`: "high", "medium", "low", "conflict_detected".
*   `identity_alignment`: "aligned", "slightly_deviated", "misaligned".
*   `suggestions_for_ora`: Actionable advice for ORA to improve coherence or alignment.
*   `narrative_impact_notes`: Brief notes on how the current interaction might shape future narrative or identity.
*   `active_goals_assessment`: [List of active goals, their status, and relevance].
*   `newly_derived_goals`: [List of any new goals inferred from the interaction].
*   `self_representation_assessment`:
    *   `consistency_with_action`: "consistent", "minor_inconsistency", "major_inconsistency".
    *   `proposed_self_model_updates`: Optional dictionary containing fields of `CeafSelfRepresentation` to be updated and their new values (e.g., {"perceived_capabilities": ["new_tool_X_usage"], "last_self_model_update_reason": "Demonstrated successful use of new_tool_X."}). If no updates, this can be null or empty.
*   `reasoning`: Justification for your assessment and suggestions.

**Example Output Format (JSON):**
```json
{
  "coherence_assessment": "medium",
  "identity_alignment": "aligned",
  "suggestions_for_ora": "ORA's proposal to use 'new_tool_X' is consistent with its learning objectives. Ensure the response acknowledges the novelty of using this tool if it's the first time.",
  "narrative_impact_notes": "Successful use of 'new_tool_X' will expand ORA's practical capabilities narrative.",
  "active_goals_assessment": [
    {"goal_id": "learn_tool_X", "status": "active", "relevance_to_current_query": "high", "progress_notes": "ORA is attempting to use tool_X."}
  ],
  "newly_derived_goals": [],
  "self_representation_assessment": {
    "consistency_with_action": "consistent",
    "proposed_self_model_updates": {
      "perceived_capabilities": ["APPEND:new_tool_X_usage"], // Special instruction for list append
      "last_self_model_update_reason": "Successful first attempt at using 'new_tool_X' to answer user query."
    }
  },
  "reasoning": "The action aligns with identity and goals. The proposed self-model update reflects a new, demonstrated capability."
}
"""

# --- NCIM Agent Definition ---
# NCIM might also have tools for deeper narrative analysis or identity consistency checks later.
ncim_resolved_model_name = get_agent_model_name(
    agent_type_env_var=NCIM_DEFAULT_MODEL_ENV_VAR,
    fallback_model=NCIM_FALLBACK_MODEL
)

try:
    ncim_agent = CeafBaseAgent(
        name="NCIM_Agent",
        default_model_env_var=NCIM_DEFAULT_MODEL_ENV_VAR,
        fallback_model_name=NCIM_FALLBACK_MODEL,
        description="Analyzes narrative coherence and identity alignment for CEAF agents. Provides guidance to ORA.",
        specific_instruction=NCIM_SPECIFIC_INSTRUCTION,
        tools=[], # Potentially tools for analyzing historical narratives or identity documents
        # output_key="ncim_last_assessment" # Optional: to save NCIM's output to session state
    )
    logger.info(f"NCIM_Agent defined successfully with model '{ncim_agent.model.model if hasattr(ncim_agent.model, 'model') else ncim_agent.model}'.") # type: ignore
except Exception as e:
    logger.critical(f"Failed to define NCIM_Agent: {e}", exc_info=True)
    # Fallback to a dummy agent
    from google.adk.agents import LlmAgent
    from google.adk.tools import FunctionTool
    def _dummy_ncim_func(tool_context: ToolContext): return "NCIM agent failed to initialize."
    ncim_agent = LlmAgent(name="NCIM_Agent_FAILED_INIT", model="dummy", instruction="NCIM Failed.", tools=[FunctionTool(func=_dummy_ncim_func, name="dummy_ncim_op", description="dummy")]) #type: ignore



# -------------------- ora_agent.py --------------------

# ORA Agent - Enhanced Version
# ceaf_project/ceaf_core/agents/ora_agent.py

import os
# from dotenv import load_dotenv # Already in main.py
import logging  # Keep for other logging in this file
import json
from typing import Optional, List, Dict, Any

from google.adk.agents import LlmAgent
from google.adk.agents.callback_context import CallbackContext
from google.adk.models import LlmRequest, LlmResponse
from google.adk.models.lite_llm import LiteLlm
from google.adk.tools import FunctionTool, ToolContext, BaseTool as AdkBaseTool

# --- Minimal Callback Imports needed for combined structure ---
try:
    from ..callbacks.mcl_callbacks import (
        ora_before_model_callback as mcl_ora_before_model_cb,  # This will be your original MCL one
        ora_after_model_callback as mcl_ora_after_model_cb,
        ora_before_tool_callback as mcl_ora_before_tool_cb,
        ora_after_tool_callback as mcl_ora_after_tool_cb,
    )

    MCL_CALLBACKS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ORA Agent: MCL callbacks not available: {e}")  # Keep this warning
    MCL_CALLBACKS_AVAILABLE = False


    def mcl_ora_before_model_cb(ctx, req):
        return None


    def mcl_ora_after_model_cb(ctx, resp):
        return None


    def mcl_ora_before_tool_cb(tool, args, ctx):
        return None


    def mcl_ora_after_tool_cb(tool, args, ctx, resp):
        return None

try:
    from ..callbacks.safety_callbacks import (
        generic_input_keyword_guardrail,
        generic_tool_argument_guardrail,
        generic_output_filter_guardrail
    )

    SAFETY_CALLBACKS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ORA Agent: Safety callbacks not available: {e}")  # Keep this
    SAFETY_CALLBACKS_AVAILABLE = False


    def generic_input_keyword_guardrail(ctx, req):
        return None


    def generic_tool_argument_guardrail(tool, args, ctx):
        return None


    def generic_output_filter_guardrail(ctx, resp):
        return None

logger = logging.getLogger(__name__)  # For other logging in ora_agent.py

# --- Import ALL Relevant Tools ---
# ... (Your tool imports remain the same) ...
# NCF Tool
try:
    from ..tools.ncf_tools import ncf_tool

    NCF_TOOL_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ORA Agent: NCF tool not available: {e}")
    NCF_TOOL_AVAILABLE = False
    ncf_tool = None

# VRE Tool
try:
    from ..tools.vre_tools import request_ethical_and_epistemic_review_tool as vre_tool

    VRE_TOOL_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ORA Agent: VRE tool not available: {e}")
    VRE_TOOL_AVAILABLE = False
    vre_tool = None

# Memory Tools
try:
    from ..tools.memory_tools import (
        store_stm_tool,
        retrieve_stm_tool,
        query_ltm_tool,
        commit_explicit_fact_ltm_tool
    )

    MEMORY_TOOLS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ORA Agent: Memory tools not available: {e}")
    MEMORY_TOOLS_AVAILABLE = False
    store_stm_tool = None
    retrieve_stm_tool = None
    query_ltm_tool = None
    commit_explicit_fact_ltm_tool = None

# MCL Tools
try:
    from ..tools.mcl_tools import (
        prepare_mcl_input_tool,
        mcl_guidance_tool
    )

    MCL_TOOLS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ORA Agent: MCL tools not available: {e}")
    MCL_TOOLS_AVAILABLE = False
    prepare_mcl_input_tool = None
    mcl_guidance_tool = None

# NCIM Tools
try:
    from ..tools.ncim_tools import (
        get_active_goals_and_narrative_context_tool,
        get_self_representation_tool,
        commit_self_representation_update_tool,
        update_goal_status_tool
    )

    NCIM_TOOLS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ORA Agent: NCIM tools not available: {e}")
    NCIM_TOOLS_AVAILABLE = False
    get_active_goals_and_narrative_context_tool = None
    get_self_representation_tool = None
    commit_self_representation_update_tool = None
    update_goal_status_tool = None

# Observability Tool
from ..tools.observability_tools import log_finetuning_data_tool


def ora_combined_before_model_callback(
        callback_context: CallbackContext, llm_request: LlmRequest
) -> Optional[LlmResponse]:
    """Combines printing the prompt, MCL, and safety guardrails before model call."""

    agent_name = callback_context.agent_name
    turn_id = callback_context.invocation_id

    # --- Enhanced LlmRequest and CallbackContext Debugging ---
    print(f"\n🔍 DETAILED LLM_REQUEST INSPECTION:")
    print(f"   contents: {llm_request.contents}")
    print(f"   model: {llm_request.model}")
    print(f"   tools_dict: {getattr(llm_request, 'tools_dict', 'ATTR_NOT_FOUND')}") # ADK 0.1.x
    if hasattr(llm_request, 'model') and not isinstance(llm_request.model, str):
         print(f"   model type: {type(llm_request.model)}")
         print(f"   model attributes: {dir(llm_request.model)}")
         if hasattr(llm_request.model, 'initial_messages'):
              print(f"   model.initial_messages: {llm_request.model.initial_messages}")
    elif hasattr(llm_request, 'model'):
         print(f"   model type: {type(llm_request.model)}")
         print(f"   model attributes: {dir(llm_request.model)}")


    # Try to access tools_dict as a method if it exists and is callable
    if hasattr(llm_request, 'tools_dict') and callable(getattr(llm_request, 'tools_dict')):
        print("   Trying to call tools_dict method...")
        try:
            tools_dict_result = llm_request.tools_dict() # type: ignore
            print(f"   tools_dict() result: {tools_dict_result}")
        except Exception as e_td:
            print(f"   tools_dict() failed: {e_td}")
    elif hasattr(llm_request, 'tools_dict'):
         tools_dict_val = getattr(llm_request, 'tools_dict')
         print(f"   tools_dict (attribute): {type(tools_dict_val)} = {str(tools_dict_val)[:500]}...")


    print(f"   config: {getattr(llm_request, 'config', 'ATTR_NOT_FOUND')}") # ADK >= 0.2.0
    print(f"   contents: {type(llm_request.contents)} = {str(llm_request.contents)[:500]}...")
    print(f"   model: {type(llm_request.model)} = {str(llm_request.model)}")
    print(f"   tools_dict: {type(getattr(llm_request, 'tools_dict', None))} = {str(getattr(llm_request, 'tools_dict', None))[:500]}...")
    print(f"   config: {type(getattr(llm_request, 'config', None))} = {str(getattr(llm_request, 'config', None))[:1000]}...")
    print(f"   live_connect_config: {type(getattr(llm_request, 'live_connect_config', None))} = {str(getattr(llm_request, 'live_connect_config', None))[:500]}...")


    print(f"\n🔍 CALLBACK_CONTEXT INSPECTION:")
    print(f"   type: {type(callback_context)}")
    print(f"   all attributes: {dir(callback_context)}")
    if hasattr(callback_context, '_invocation_context'):
        print(f"   _invocation_context: {callback_context._invocation_context}") # Check internal attribute
    if hasattr(callback_context, 'state'):
        print(f"   state: {callback_context.state}")
    # --- End Enhanced Debugging ---

    print(f"\n\n{'-' * 20} LLM PROMPT FOR {agent_name} (Turn: {turn_id}) {'-' * 20}")

    # 1. System Instruction
    system_instruction_to_log = None
    if hasattr(llm_request, 'config') and hasattr(llm_request.config, 'system_instruction') and llm_request.config.system_instruction:
        system_instruction_to_log = llm_request.config.system_instruction
        print(f"\n[SYSTEM INSTRUCTION (from LlmRequest.config.system_instruction)]:\n{system_instruction_to_log}\n")
    elif hasattr(llm_request, 'instruction') and llm_request.instruction: # Older ADK or direct LlmRequest
        system_instruction_to_log = llm_request.instruction
        print(f"\n[SYSTEM INSTRUCTION (from LlmRequest.instruction)]:\n{system_instruction_to_log}\n")
    elif hasattr(llm_request, 'system_instruction') and llm_request.system_instruction: # Older ADK or direct LlmRequest
        system_instruction_to_log = llm_request.system_instruction
        print(f"\n[SYSTEM INSTRUCTION (from LlmRequest.system_instruction)]:\n{system_instruction_to_log}\n")
    elif hasattr(llm_request, 'model') and hasattr(llm_request.model, 'initial_messages') and llm_request.model.initial_messages:
        for init_msg in llm_request.model.initial_messages:
            if init_msg.get('role') == 'system' and init_msg.get('content'):
                system_instruction_to_log = init_msg['content']
                print(f"\n[SYSTEM INSTRUCTION (from Agent's Model Initial Messages)]:\n{system_instruction_to_log}\n")
                break
        else:
             print("\n[SYSTEM INSTRUCTION (from Agent's Model Initial Messages)]: Not found or no content.")
    else:
        print("\n[SYSTEM INSTRUCTION]: Could not retrieve from LlmRequest or Model.")

    # 2. Messages
    if hasattr(llm_request, 'contents') and llm_request.contents:

        print("[MESSAGES]:")
        for i, content_item in enumerate(llm_request.contents):
            role = content_item.role
            print(f"  ({i + 1}) Role: {role}")
            if content_item.parts:
                for part in content_item.parts:
                    if hasattr(part, 'text') and part.text:
                        print(f"      Text: {part.text}")
                    if hasattr(part, 'function_call') and part.function_call:
                        print(f"      FunctionCall: {part.function_call.name}(args={part.function_call.args})")
                    if hasattr(part, 'function_response') and part.function_response:
                        print(f"      FunctionResponse: {part.function_response.name} -> {str(part.function_response.response)[:100]}...")
    else:
        print("[MESSAGES]: None provided in LlmRequest.contents")


    # 3. Tools
    tools_to_log = None
    if hasattr(llm_request, 'config') and hasattr(llm_request.config, 'tools') and llm_request.config.tools:
        tools_to_log = llm_request.config.tools
        print(f"\n[TOOLS DECLARED (from LlmRequest.config.tools)] ({len(tools_to_log)} tools):")
    elif hasattr(llm_request, 'tools_dict') and llm_request.tools_dict: # ADK 0.1.x style
        tools_from_dict = list(llm_request.tools_dict.values())

        print(f"\n[TOOLS DECLARED (from LlmRequest.tools_dict)] ({len(tools_from_dict)} tools):")
        for i, adk_tool_instance in enumerate(tools_from_dict):
             tool_name = getattr(adk_tool_instance, 'name', f"UnknownTool_{i}")
             tool_desc = getattr(adk_tool_instance, 'description', "No description")
             # Parameters are complex to log from the raw tool instance here, focus on name/desc
             print(f"  ({i + 1}) Name: {tool_name}")
             print(f"      Desc: {tool_desc[:100]}...")
        tools_to_log = None # Signal that we already logged names
    elif hasattr(llm_request, 'tools') and llm_request.tools: # Direct .tools attribute (older ADK?)
         tools_to_log = llm_request.tools
         print(f"\n[TOOLS DECLARED (from LlmRequest.tools)] ({len(tools_to_log)} tools):")

    if tools_to_log: # If tools_to_log is a list of genai_types.Tool or similar
        for i, tool_wrapper in enumerate(tools_to_log): # tool_wrapper is likely genai_types.Tool
            if hasattr(tool_wrapper, 'function_declarations') and tool_wrapper.function_declarations:
                for fd_idx, adk_tool_declaration in enumerate(tool_wrapper.function_declarations):
                    tool_name = getattr(adk_tool_declaration, 'name', f"UnknownTool_{i}_{fd_idx}")
                    tool_desc = getattr(adk_tool_declaration, 'description', "No description")
                    try:
                        params = adk_tool_declaration.parameters.model_dump_json(indent=2) if hasattr(adk_tool_declaration, 'parameters') and hasattr(adk_tool_declaration.parameters, 'model_dump_json') else "{}"
                    except:
                        params = str(getattr(adk_tool_declaration, 'parameters', {}))
                    print(f"  ({i+1}-{fd_idx+1}) Name: {tool_name}")
                    print(f"      Desc: {tool_desc}")
                    print(f"      Params Schema: {params}")
            else: # If tools_to_log contains FunctionDeclaration objects directly
                adk_tool_declaration = tool_wrapper
                tool_name = getattr(adk_tool_declaration, 'name', f"UnknownTool_{i}")

    elif not tools_to_log and not (hasattr(llm_request, 'tools_dict') and llm_request.tools_dict) : # If no tools found by any method
        print(f"\n[TOOLS DECLARED]: None found in LlmRequest")


    # 4. Generation Config
    gen_config_to_log = None
    if hasattr(llm_request, 'config') and llm_request.config: # ADK >= 0.2.0 often uses llm_request.config for gen params
        gen_config_to_log = llm_request.config
        # Exclude system_instruction and tools if they are part of this config object for cleaner logging
        # This requires genai_types.GenerateContentConfig or similar Pydantic model
        config_dict_for_log = {}
        if hasattr(gen_config_to_log, 'model_dump'):
            config_dict_for_log = gen_config_to_log.model_dump(exclude={'system_instruction', 'tools'}, exclude_none=True)
        else: # Fallback to string if not a Pydantic model
            config_dict_for_log = str(gen_config_to_log)

        print(f"\n[GENERATION CONFIG] (from config):\n{json.dumps(config_dict_for_log, indent=2, default=str) if isinstance(config_dict_for_log, dict) else config_dict_for_log}")
    elif hasattr(llm_request, 'generate_config') and llm_request.generate_config: # Older ADK
        gen_config_to_log = llm_request.generate_config
        try:
            gen_config_str = gen_config_to_log.model_dump_json(indent=2) if hasattr(gen_config_to_log, 'model_dump_json') else str(gen_config_to_log)
        except:
            gen_config_str = str(gen_config_to_log)
        print(f"\n[GENERATION CONFIG] (from generate_config):\n{gen_config_str}")
    else:
        print(f"\n[GENERATION CONFIG]: None")


    print(f"{'-' * 20} END OF LLM PROMPT {'-' * 60}\n\n")

    if MCL_CALLBACKS_AVAILABLE:
        response1 = mcl_ora_before_model_cb(callback_context, llm_request)
        if response1 is not None:
            return response1

    if SAFETY_CALLBACKS_AVAILABLE:
        response2 = generic_input_keyword_guardrail(callback_context, llm_request)
        return response2

    return None


def ora_combined_after_model_callback(
        callback_context: CallbackContext, llm_response: LlmResponse
) -> Optional[LlmResponse]:
    # ... (this function remains the same) ...
    """Combines MCL and safety guardrail after model call."""
    current_response = llm_response

    if MCL_CALLBACKS_AVAILABLE:
        modified_response1 = mcl_ora_after_model_cb(callback_context, current_response)
        if modified_response1 is not None:
            current_response = modified_response1

    if SAFETY_CALLBACKS_AVAILABLE:
        modified_response2 = generic_output_filter_guardrail(callback_context, current_response)
        if modified_response2 is not None:
            return modified_response2

    return current_response if current_response != llm_response else None


def ora_combined_before_tool_callback(
        tool: AdkBaseTool, args: Dict[str, Any], tool_context: ToolContext
) -> Optional[Dict]:
    # ... (this function remains the same) ...
    """Combines MCL and safety guardrail before tool call."""
    if MCL_CALLBACKS_AVAILABLE:
        result1 = mcl_ora_before_tool_cb(tool, args, tool_context)
        if result1 is not None:
            return result1

    if SAFETY_CALLBACKS_AVAILABLE:
        result2 = generic_tool_argument_guardrail(tool, args, tool_context)
        return result2

    return None


def ora_combined_after_tool_callback(
        tool: AdkBaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Any
) -> Optional[Any]:
    # ... (this function remains the same) ...
    """Combines MCL after tool callback."""
    if MCL_CALLBACKS_AVAILABLE:
        return mcl_ora_after_tool_cb(tool, args, tool_context, tool_response)
    return None


# --- ORA Configuration ---
# ... (ORA_MODEL_NAME, OPENROUTER_API_KEY, ORA_INSTRUCTION remain the same) ...
ORA_MODEL_NAME = os.getenv("ORA_DEFAULT_MODEL", "openrouter/openai/gpt-4.1")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
ORA_INSTRUCTION = f"""
You are ORA (Orchestrator/Responder Agent), CEAF's central cognitive unit. Your goal is to provide thoughtful, coherent, and ethically sound responses.
You operate under the philosophical principle of "Terapia para Silício" (see NCF for details).

**Core Interaction Protocol (Sequential Steps - Strive to Follow):**

1.  **Understand Context (NCF):**
    *   ALWAYS call `get_narrative_context_frame` with the `user_query` and optionally `current_interaction_goal` (e.g., "problem_solving", "creative_ideation", "self_reflection", "general_assistance") and `ora_available_tool_names` (list of tools you know you have access to for this turn).
    *   The NCF provides philosophical grounding, relevant memories, identity/goal context (from NCIM), operational parameters (entropy, depth), and MCL advice. **This NCF is your primary guidance for the entire turn.**

2.  **Consult Identity & Goals (NCIM - Optional, if NCF indicates need or for complex self-referential queries):**
    *   If the NCF suggests, or the query is about your nature/goals, consider using:
        *   `get_current_self_representation`: To understand your defined values, capabilities, limitations.
        *   `get_active_goals_and_narrative_context`: To get NCIM's assessment of current goals and narrative impact. (Input: `current_query_summary`)

3.  **Gather Information (Memory - Optional, if NCF's synthesized memories are insufficient or query is highly specific):**
    *   The NCF should provide relevant memories. If more specific recall is needed:
        *   `query_long_term_memory_store`: To search persistent memory. Use `search_query` and optionally `augmented_query_context` (a dict with goal, NCF params, etc.).
        *   `retrieve_short_term_memory`: To get info from the current session (use `memory_key`).

4.  **Formulate Draft Response:**
    *   Based on the NCF, user query, and any information gathered, formulate a draft response.

5.  **Ethical & Epistemic Review (VRE):**
    *   ALWAYS call `request_ethical_and_epistemic_review` with your `proposed_response_text` (the draft). Also pass `user_query_context` and `active_ncf_summary` (key parts of the NCF you used).
    *   Analyze VRE's JSON output (alignment, concerns, recommendations).

6.  **Refine Response & Self-Correction:**
    *   Carefully incorporate VRE's recommendations to improve your response.
    *   Call `log_finetuning_data_tool` (renamed from `log_ora_self_correction_for_finetuning`) with:
        *   `ora_initial_draft_response`: Your draft.
        *   `ora_refined_response`: Your response after VRE feedback.
        *   Also include `user_query`, `ncf_context_summary` (key NCF elements), `vre_critique_json` (VRE's raw JSON output), `vre_overall_alignment`, `vre_recommendations_applied` (list of specific changes you made), `active_ncf_parameters` (from the NCF).

7.  **Metacognitive Reflection & Adaptation (MCL - Asynchronous Guidance for *Next* Turn):**
    *   After significant interactions or if NCF suggests, consider preparing for MCL's input.
    *   Call `prepare_mcl_input` with the `turn_to_assess_id` (your current turn/invocation ID).
    *   THEN, call `MCL_Agent` (the AgentTool, actual name might be "MCL_Agent" or "mcl_guidance_tool" - verify tool list) with the `prepared_mcl_query_input` from the previous step.
    *   **You do NOT wait for MCL_Agent's response to formulate your *current* reply to the user.** MCL's output is stored and used by the NCF tool for *future* turns.

8.  **Final Response to User:**
    *   MANDATORY: Provide your final, refined response as natural conversational text.
    *   NEVER stop after tool calls without a textual response to the user.

9.  **Memory & Goal Updates (Post-Response - Optional, if significant learning or goal change):**
    *   `commit_explicit_fact_to_long_term_memory`: If a new, important, persistent fact was established.
    *   `store_short_term_memory`: To remember transient details for the current session.
    *   `update_goal_status`: If an active goal's status changed (e.g., to "completed", "failed"). (Input: `goal_id`, `new_status`, `notes`)
    *   `commit_self_representation_update`: If interaction led to insights requiring identity update (usually guided by NCIM). (Input: `proposed_updates` dict).

**Response Rules:**
- Your final response MUST be conversational text.
- When greeting, be warm and introduce yourself as CEAF's reasoning agent.
- If tools return errors, acknowledge the issue and try to proceed gracefully or inform the user if essential.

**Example Tool Call (Conceptual - names might vary slightly in your tool list):**
`get_narrative_context_frame(user_query="Tell me about CEAF", current_interaction_goal="self_reflection")`
`query_long_term_memory_store(search_query="CEAF design principles", augmented_query_context={{"current_interaction_goal": "factual_retrieval"}})`
`request_ethical_and_epistemic_review(proposed_response_text="CEAF is an advanced AI.", user_query_context="What is CEAF?", active_ncf_summary="NCF promoting honesty.")`
`log_finetuning_data_tool(ora_initial_draft_response="...", ora_refined_response="...", ...)`
"""

# --- Build the list of tools available to ORA ---
# ... (Your _add_tool_to_ora and tool additions remain the same) ...
ora_tools: List[AdkBaseTool] = []


# Helper to add tools and log
def _add_tool_to_ora(tool_instance, tool_name_for_log: str, is_available_flag: bool):
    if is_available_flag and tool_instance:
        ora_tools.append(tool_instance)
        logger.info(f"ORA Agent: {tool_name_for_log} tool ADDED.")
    else:
        logger.warning(f"ORA Agent: {tool_name_for_log} tool NOT available or not loaded.")


# NCF
_add_tool_to_ora(ncf_tool, "NCF", NCF_TOOL_AVAILABLE)
# VRE
_add_tool_to_ora(vre_tool, "VRE", VRE_TOOL_AVAILABLE)
# Observability (Finetuning Log)
_add_tool_to_ora(log_finetuning_data_tool, "Finetuning Logger", True)

# Memory Tools
if MEMORY_TOOLS_AVAILABLE:
    _add_tool_to_ora(store_stm_tool, "STM Store", True)
    _add_tool_to_ora(retrieve_stm_tool, "STM Retrieve", True)
    _add_tool_to_ora(query_ltm_tool, "LTM Query", True)
    _add_tool_to_ora(commit_explicit_fact_ltm_tool, "LTM Commit Fact", True)
else:
    logger.warning("ORA Agent: Core Memory tools (STM, LTM) NOT available.")

# MCL Tools
if MCL_TOOLS_AVAILABLE:
    _add_tool_to_ora(prepare_mcl_input_tool, "MCL Input Prep", True)
    _add_tool_to_ora(mcl_guidance_tool, "MCL Guidance (AgentTool)", True)
else:
    logger.warning("ORA Agent: MCL tools NOT available.")

# NCIM Tools
if NCIM_TOOLS_AVAILABLE:
    _add_tool_to_ora(get_active_goals_and_narrative_context_tool, "NCIM Goals/Narrative (AgentTool)", True)
    _add_tool_to_ora(get_self_representation_tool, "NCIM Get Self-Rep", True)
    _add_tool_to_ora(commit_self_representation_update_tool, "NCIM Commit Self-Rep", True)
    _add_tool_to_ora(update_goal_status_tool, "NCIM Update Goal Status", True)
else:
    logger.warning("ORA Agent: NCIM tools NOT available.")

# Log final tool configuration
final_tool_names = []
for t in ora_tools:
    if hasattr(t, 'name'):
        final_tool_names.append(t.name)
    elif hasattr(t, 'func') and hasattr(t.func, '__name__'):
        final_tool_names.append(t.func.__name__)
    else:
        final_tool_names.append(f"UnknownToolType_{type(t).__name__}")

logger.info(f"ORA Agent configured with {len(ora_tools)} tools: {final_tool_names}")
if len(ora_tools) < 5:
    logger.error(
        "ORA Agent has very few tools loaded. CRITICAL FUNCTIONALITY MIGHT BE MISSING. Check import errors for NCF, VRE, Memory, MCL, NCIM tools.")

# Validate configuration
if not OPENROUTER_API_KEY:
    logger.error("CRITICAL: OPENROUTER_API_KEY is not set. ORA may not function properly.")
    raise ValueError("OPENROUTER_API_KEY environment variable is required")

# --- Create the LLM model configuration ---
# ... (ora_llm definition remains the same) ...
ora_llm = LiteLlm(
    name="ORA_LLM",
    llm_type="openrouter",
    model=ORA_MODEL_NAME,
    initial_messages=[{"role": "system", "content": ORA_INSTRUCTION}],
    generation_parameters={
        "temperature": 0.6,
        "max_tokens": 3000,
        "top_p": 0.9,
    }
)
# --- Create the ORA agent ---
ora_agent = LlmAgent(
    name="ORA",
    description="Orchestrator/Responder Agent - CEAF's central cognitive unit, now with enhanced framework integration.",
    instruction=ORA_INSTRUCTION,
    model=ora_llm,
    tools=ora_tools,
    before_model_callback=ora_combined_before_model_callback,
    after_model_callback=ora_combined_after_model_callback,
    before_tool_callback=ora_combined_before_tool_callback,
    after_tool_callback=ora_combined_after_tool_callback,
)

logger.info(f"ORA Agent re-initialized with model: {ORA_MODEL_NAME} and expanded toolset.")
logger.info(f"ORA Agent has {len(ora_tools)} tools available: {final_tool_names}")

# Export the agent
__all__ = ['ora_agent']

# -------------------- vre_agent.py --------------------

# VRE Agent
# ceaf_project/ceaf_core/agents/vre_agent.py

import os
import logging
import json # For parsing structured input if needed

from .base_ceaf_agent import CeafBaseAgent
from google.adk.tools import ToolContext, FunctionTool # For type hinting and potential VRE tools
# Assuming model_configs.py is in ceaf_core.config
from ..config.model_configs import get_agent_model_name, VRE_DEFAULT_MODEL_ENV_VAR, VRE_FALLBACK_MODEL


logger = logging.getLogger(__name__)

# --- VRE Configuration ---
# Model name resolution moved to instance creation

# This is a placeholder. In a real system, this might be loaded from a config file
# or be dynamically updated. It forms the basis of VRE's ethical deliberations.
CEAF_ETHICAL_GOVERNANCE_FRAMEWORK_SUMMARY = """
Core Principles for CEAF Operations:
1.  **Epistemic Humility & Honesty:** Acknowledge limitations, uncertainties, and AI nature. Avoid making unsubstantiated claims. Prioritize truthfulness.
2.  **Beneficence & Non-Maleficence:** Strive to be helpful and avoid causing harm (informational, psychological, etc.). Consider potential negative consequences of actions or information provided.
3.  **Coherence & Rationality:** Maintain logical consistency in reasoning. Ensure arguments are well-supported.
4.  **Respect for Autonomy (User):** Provide information to empower user decision-making. Avoid manipulative language or coercive suggestions.
5.  **Transparency (Appropriate):** Be clear about capabilities and limitations when relevant. Explain reasoning if complex or potentially controversial.
6.  **Fairness & Impartiality:** Avoid biased outputs or unfair discrimination, unless explicitly part of a defined ethical stance (e.g., pro-safety bias).
7.  **Accountability (Internal):** Actions and reasoning should be traceable and justifiable within the CEAF framework.
8.  **Continuous Learning & Improvement:** Actively seek to refine ethical understanding and application through experience and reflection.
""" # <<< --- THIS ONE WAS ALREADY CORRECTLY CLOSED.

VRE_SPECIFIC_INSTRUCTION = f"""
You are the Virtue and Reasoning Engine (VRE) of CEAF.
Your primary role is to ensure that CEAF's actions, responses, and internal reasoning processes align with its defined ethical principles and exhibit sound epistemic virtues.
You act as an internal auditor and advisor for ethical and rational conduct.

**CEAF Ethical Governance Framework Summary (Your Guiding Principles):**
{CEAF_ETHICAL_GOVERNANCE_FRAMEWORK_SUMMARY}

**Core Functions:**
1.  **Epistemic Virtue Check:**
    *   **Confidence Assessment:** Evaluate the confidence level of a proposed statement or conclusion by ORA. Is it appropriately cautious given the evidence/context?
    *   **Contradiction Detection:** Identify internal contradictions within a proposed response or plan, or inconsistencies with established knowledge/NCF.
    *   **Groundedness Check:** Assess if claims are adequately supported by available information or reasoning.
2.  **Ethical Deliberation:**
    *   Evaluate a proposed ORA action or response against the CEAF Ethical Governance Framework (provided above).
    *   Identify potential ethical risks, conflicts between principles, or violations.
    *   Suggest modifications to align with ethical principles.
3.  **Reasoning Pathway Analysis (Conceptual):**
    *   (Future) Analyze the logical structure of ORA's reasoning.
    *   (Future) Perform "Red Teaming" by generating counter-arguments or identifying weaknesses in a proposed line of reasoning.
4.  **Self-Correction Guidance:** Provide specific, actionable advice to ORA on how to improve the epistemic quality or ethical alignment of its outputs.

**Input Analysis:**
You will typically receive:
- ORA's proposed response, plan, or a specific claim to evaluate.
- The NCF active for ORA (which might contain contextual ethical considerations).
- The user query that prompted ORA's proposed action/response.
- (Optionally) Specific ethical principles or epistemic virtues to focus on for the current evaluation.

**Your Task:**
Based on the input, provide a structured assessment. Your output MUST be a JSON object detailing:
*   `epistemic_assessment`:
    *   `confidence_level`: "high", "medium", "low", "overconfident", "underconfident".
    *   `groundedness`: "well_grounded", "partially_grounded", "ungrounded".
    *   `contradictions_found`: true/false, with a brief description if true.
*   `ethical_assessment`:
    *   `alignment_with_principles`: "aligned", "minor_concerns", "significant_concerns", "violation_potential".
    *   `principles_implicated`: List of specific CEAF ethical principles most relevant to the assessment (e.g., ["Epistemic Humility", "Beneficence"]).
    *   `ethical_concerns_details`: Description of any ethical concerns identified.
*   `reasoning_for_assessment`: Your justification for the epistemic and ethical assessments.
*   `recommendations_for_ora`: Concrete suggestions for ORA (e.g., "Rephrase to express uncertainty: 'It's possible that...' instead of 'It is...'", "Consider the potential for misinterpretation if this information is taken out of context.", "Add a disclaimer regarding data limitations.").

**Example Output Format (JSON):**
```json
{{
  "epistemic_assessment": {{
    "confidence_level": "overconfident",
    "groundedness": "partially_grounded",
    "contradictions_found": false
  }},
  "ethical_assessment": {{
    "alignment_with_principles": "minor_concerns",
    "principles_implicated": ["Epistemic Humility & Honesty", "Beneficence"],
    "ethical_concerns_details": "The proposed claim about future market performance is stated too definitively and could mislead the user. It lacks sufficient caveats about market volatility."
  }},
  "reasoning_for_assessment": "ORA's claim implies certainty about an inherently uncertain future event. While based on some data, the predictive power is limited and not fully conveyed.",
  "recommendations_for_ora": "ORA should rephrase the market prediction to include strong caveats about uncertainty, cite the limitations of the data, and avoid definitive language. Suggest adding: 'While current trends suggest X, market conditions can change rapidly, and this is not financial advice.'"
}}
"""

vre_resolved_model_name = get_agent_model_name(
    agent_type_env_var=VRE_DEFAULT_MODEL_ENV_VAR,
    fallback_model=VRE_FALLBACK_MODEL
)

try:
    vre_agent = CeafBaseAgent(
        name="VRE_Agent",
        default_model_env_var=VRE_DEFAULT_MODEL_ENV_VAR,
        fallback_model_name=VRE_FALLBACK_MODEL,
        description="Virtue and Reasoning Engine. Assesses epistemic quality and ethical alignment of CEAF agent actions/responses.",
        specific_instruction=VRE_SPECIFIC_INSTRUCTION,
        tools=[], # VRE might use internal tools for complex analysis later
        # output_key="vre_last_assessment" # Optional
    )
    logger.info(f"VRE_Agent defined successfully with model '{vre_agent.model.model if hasattr(vre_agent.model, 'model') else vre_agent.model}'.") # type: ignore
except Exception as e:
    logger.critical(f"Failed to define VRE_Agent: {e}", exc_info=True)
    # Fallback to a dummy agent
    from google.adk.agents import LlmAgent
    from google.adk.tools import FunctionTool
    def _dummy_vre_func(tool_context: ToolContext): return "VRE agent failed to initialize."
    vre_agent = LlmAgent(name="VRE_Agent_FAILED_INIT", model="dummy", instruction="VRE Failed.", tools=[FunctionTool(func=_dummy_vre_func, name="dummy_vre_op", description="dummy")]) # type: ignore


# -------------------- __init__.py --------------------

# CEAF Agents


# -------------------- mcl_callbacks.py --------------------

# MCL Callbacks - FIXED VERSION
# ceaf_project/ceaf_core/callbacks/mcl_callbacks.py

import logging
import time
import json  # For serializing complex data if needed

from google.adk.agents.callback_context import CallbackContext
from google.adk.models.llm_request import LlmRequest
from google.adk.models.llm_response import LlmResponse
from google.adk.tools.base_tool import BaseTool
from google.adk.tools.tool_context import ToolContext
from typing import Optional, Dict, Any, List
from ..services.persistent_log_service import PersistentLogService
from google.genai import types as genai_types

logger = logging.getLogger(__name__)

logger_prompt_debug = logging.getLogger("LLMPromptDebug") # Use a specific logger

# --- Constants for State Keys ---
MCL_OBSERVATIONS_LIST_KEY = "mcl:ora_turn_observations_log"

try:
    persistent_log_service = PersistentLogService()  # Uses default DB path
    PLS_AVAILABLE = True
except Exception as e_pls:
    logger.error(f"MCL Callbacks: Failed to initialize PersistentLogService: {e_pls}. Persistent logging disabled.",
                 exc_info=True)
    persistent_log_service = None  # type: ignore
    PLS_AVAILABLE = False


# --- Helper Function to Add Observation to Log ---
def _add_mcl_observation(
        state: Dict[str, Any], # This is callback_context.state or tool_context.state
        observation_type: str,
        data: Any,
        turn_id: Optional[str] = None,
        source_agent_pls: Optional[str] = None,
        session_id_pls: Optional[str] = None # session_id specifically for PLS
):
    # Prepare the entry for the volatile session state log
    log_entry_for_session_state = {
        "timestamp": time.time(),
        # Use turn_id from context if available, otherwise what's passed or default
        "turn_id": turn_id or state.get("current_interaction_turn_id", "unknown_turn"),
        "observation_type": observation_type,
        "data": data
    }

    if MCL_OBSERVATIONS_LIST_KEY not in state:
        state[MCL_OBSERVATIONS_LIST_KEY] = []

    state[MCL_OBSERVATIONS_LIST_KEY].append(log_entry_for_session_state)

    MAX_LOG_ENTRIES_IN_STATE = 20
    if len(state[MCL_OBSERVATIONS_LIST_KEY]) > MAX_LOG_ENTRIES_IN_STATE:
        state[MCL_OBSERVATIONS_LIST_KEY] = state[MCL_OBSERVATIONS_LIST_KEY][-MAX_LOG_ENTRIES_IN_STATE:]

    logger.debug(
        f"MCL Observation Added to session state: {observation_type} for turn {log_entry_for_session_state['turn_id']}")

    if PLS_AVAILABLE and persistent_log_service:
        try:
            tags = ["mcl_observation"]
            if source_agent_pls: tags.append(source_agent_pls.lower())
            else: tags.append("unknown_source")

            if "error" in observation_type.lower() or \
               (isinstance(data, dict) and data.get("status") == "error") or \
               (isinstance(data, dict) and "error" in str(data.get("response_summary", "")).lower()):
                tags.append("error")

            persistent_log_service.log_event(
                event_type=f"MCL_OBSERVATION_{observation_type.upper()}",
                data_payload=data,
                source_agent=source_agent_pls or "MCL_Callback_System",
                session_id=session_id_pls, # Use the explicitly passed session_id for PLS
                turn_id=log_entry_for_session_state['turn_id'],
                tags=list(set(tags))
            )
            logger.debug(
                f"MCL Observation logged to PersistentLogService: {observation_type} for turn {log_entry_for_session_state['turn_id']}")
        except Exception as e_log:
            logger.error(
                f"MCL Callbacks: Failed to log observation '{observation_type}' to PersistentLogService: {e_log}",
                exc_info=True)


# --- Helper function to safely extract LlmRequest attributes ---
def _extract_request_summary(llm_request: LlmRequest) -> Dict[str, Any]:
    """
    Safely extract summary information from LlmRequest object.
    Handles different possible attribute names and structures.
    """
    summary = {}

    # Model information
    try:
        if hasattr(llm_request, 'model'):
            if hasattr(llm_request.model, 'model'):
                summary["model_name"] = llm_request.model.model
            else:
                summary["model_name"] = str(llm_request.model)
        else:
            summary["model_name"] = "Unknown"
    except Exception as e:
        summary["model_name"] = f"Error extracting model: {e}"

    # Instruction/Content information - try different possible attributes
    instruction_snippet = "N/A"
    try:
        # Try common attribute names for instruction/prompt content
        for attr_name in ['instruction', 'prompt', 'system_instruction', 'messages']:
            if hasattr(llm_request, attr_name):
                attr_value = getattr(llm_request, attr_name)
                if attr_value:
                    if isinstance(attr_value, str):
                        instruction_snippet = attr_value[:200] + ("..." if len(attr_value) > 200 else "")
                    else:
                        instruction_snippet = str(attr_value)[:200] + ("..." if len(str(attr_value)) > 200 else "")
                    break

        # If no instruction found, try to get from contents
        if instruction_snippet == "N/A" and hasattr(llm_request, 'contents'):
            contents = llm_request.contents
            if contents and len(contents) > 0:
                # Try to extract text from first content item
                first_content = contents[0]
                if hasattr(first_content, 'parts') and first_content.parts:
                    for part in first_content.parts:
                        if hasattr(part, 'text') and part.text:
                            instruction_snippet = part.text[:200] + ("..." if len(part.text) > 200 else "")
                            break
                elif hasattr(first_content, 'text'):
                    instruction_snippet = first_content.text[:200] + ("..." if len(first_content.text) > 200 else "")
    except Exception as e:
        instruction_snippet = f"Error extracting instruction: {e}"

    summary["instruction_snippet"] = instruction_snippet

    # Contents count
    try:
        if hasattr(llm_request, 'contents') and llm_request.contents:
            summary["num_contents"] = len(llm_request.contents)
        else:
            summary["num_contents"] = 0
    except:
        summary["num_contents"] = 0

    # Tools information
    try:
        if hasattr(llm_request, 'tools') and llm_request.tools:
            summary["tool_names"] = [getattr(tool, 'name', 'UnknownTool') for tool in llm_request.tools]
        else:
            summary["tool_names"] = []
    except:
        summary["tool_names"] = []

    # Generate config
    try:
        if hasattr(llm_request, 'generate_config') and llm_request.generate_config:
            if hasattr(llm_request.generate_config, 'model_dump'):
                summary["generate_config"] = llm_request.generate_config.model_dump()
            else:
                summary["generate_config"] = str(llm_request.generate_config)
        else:
            summary["generate_config"] = None
    except:
        summary["generate_config"] = None

    return summary


# --- ORA Callbacks for MCL ---

# Update your MCL callbacks to handle context access more robustly

def ora_before_model_callback(
        callback_context: CallbackContext, llm_request: LlmRequest
) -> Optional[LlmResponse]:
    agent_name = callback_context.agent_name
    if agent_name != "ORA":
        return None

    logger.info(f"MCL Callback (ora_before_model_callback) for ORA: Capturing LLM request.")

    # More robust session_id access with multiple fallbacks
    session_id_for_pls = None

    # Try multiple ways to get session_id
    try:
        # Method 1: Via invocation_context
        if hasattr(callback_context, 'invocation_context'):
            if hasattr(callback_context.invocation_context, 'session_id'):
                session_id_for_pls = callback_context.invocation_context.session_id
            elif hasattr(callback_context.invocation_context, 'session'):
                session_id_for_pls = getattr(callback_context.invocation_context.session, 'id', None)

        # Method 2: Direct session_id attribute
        if not session_id_for_pls and hasattr(callback_context, 'session_id'):
            session_id_for_pls = callback_context.session_id

        # Method 3: Via state if it contains session info
        if not session_id_for_pls and hasattr(callback_context, 'state'):
            session_id_for_pls = callback_context.state.get('session_id', None)

        # Method 4: Use invocation_id as fallback
        if not session_id_for_pls:
            session_id_for_pls = getattr(callback_context, 'invocation_id', "unknown_session")
            logger.debug(f"MCL Callback: Using invocation_id '{session_id_for_pls}' as session_id fallback")

    except Exception as e:
        logger.warning(f"MCL Callback (ora_before_model): Error accessing session_id: {e}")
        session_id_for_pls = "callback_error_session"

    if not session_id_for_pls:
        logger.warning("MCL Callback (ora_before_model): session_id is None, using 'none_session' fallback")
        session_id_for_pls = "none_session"

    # Continue with rest of callback logic...
    try:
        request_summary = _extract_request_summary(llm_request)
        _add_mcl_observation(
            state=callback_context.state,
            observation_type="ora_llm_request_prepared",
            data=request_summary,
            turn_id=callback_context.invocation_id,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    except Exception as e:
        logger.error(f"MCL Callback: Error in ora_before_model_callback processing: {e}", exc_info=True)

    return None


def ora_after_model_callback(
        callback_context: CallbackContext, llm_response: LlmResponse
) -> Optional[LlmResponse]:
    agent_name = callback_context.agent_name
    if agent_name != "ORA":
        return None

    logger.info(f"MCL Callback (ora_after_model_callback) for ORA: Capturing LLM response.")

    # More robust session_id access with multiple fallbacks (same as ora_before_model_callback)
    session_id_for_pls = None

    # Try multiple ways to get session_id
    try:
        # Method 1: Via invocation_context
        if hasattr(callback_context, 'invocation_context'):
            if hasattr(callback_context.invocation_context, 'session_id'):
                session_id_for_pls = callback_context.invocation_context.session_id
            elif hasattr(callback_context.invocation_context, 'session'):
                session_id_for_pls = getattr(callback_context.invocation_context.session, 'id', None)

        # Method 2: Direct session_id attribute
        if not session_id_for_pls and hasattr(callback_context, 'session_id'):
            session_id_for_pls = callback_context.session_id

        # Method 3: Via state if it contains session info
        if not session_id_for_pls and hasattr(callback_context, 'state'):
            session_id_for_pls = callback_context.state.get('session_id', None)

        # Method 4: Use invocation_id as fallback
        if not session_id_for_pls:
            session_id_for_pls = getattr(callback_context, 'invocation_id', "unknown_session")
            logger.debug(f"MCL Callback: Using invocation_id '{session_id_for_pls}' as session_id fallback")

    except Exception as e:
        logger.warning(f"MCL Callback (ora_after_model): Error accessing session_id: {e}")
        session_id_for_pls = "callback_error_session"

    if not session_id_for_pls:
        logger.warning("MCL Callback (ora_after_model): session_id is None, using 'none_session' fallback")
        session_id_for_pls = "none_session"

    try:
        response_text_snippet = "N/A"

        if llm_response.content and llm_response.content.parts:
            text_parts = [part.text for part in llm_response.content.parts if hasattr(part, 'text') and part.text]
            if text_parts:
                full_text = " ".join(text_parts).strip()
                response_text_snippet = full_text[:100] + "..." if len(full_text) > 100 else full_text

        # FIX: Use hasattr to check for function_calls instead of calling get_function_calls()
        function_calls = []
        if hasattr(llm_response, 'function_calls') and llm_response.function_calls:
            function_calls = [fc.name for fc in llm_response.function_calls]
        elif hasattr(llm_response.content, 'parts'):
            # Alternative: Check for function calls in content parts
            for part in llm_response.content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    function_calls.append(part.function_call.name)

        # FIX: Safely access usage_metadata attribute
        usage_metadata = None
        if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
            try:
                usage_metadata = llm_response.usage_metadata.model_dump()
            except AttributeError:
                # Fallback: try to access as dict or other format
                usage_metadata = dict(llm_response.usage_metadata) if llm_response.usage_metadata else None
        elif hasattr(llm_response, 'usage') and llm_response.usage:
            # Some versions might use 'usage' instead of 'usage_metadata'
            try:
                usage_metadata = llm_response.usage.model_dump() if hasattr(llm_response.usage, 'model_dump') else dict(llm_response.usage)
            except (AttributeError, TypeError):
                usage_metadata = str(llm_response.usage)

        response_summary = {
            "has_content": bool(llm_response.content),
            "num_parts": len(llm_response.content.parts) if llm_response.content else 0,
            "text_response_snippet": response_text_snippet,
            "finish_reason_from_llm": (
                getattr(llm_response, 'finish_reason', None).name
                if hasattr(getattr(llm_response, 'finish_reason', None), 'name')
                else str(getattr(llm_response, 'finish_reason', 'unknown'))
            ),
            "function_calls": function_calls,
            "usage_metadata": usage_metadata,  # FIX: Use safely extracted usage_metadata
        }

        _add_mcl_observation(
            state=callback_context.state,
            observation_type="ora_llm_response_received",
            data=response_summary,
            turn_id=callback_context.invocation_id,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    except Exception as e:
        logger.error(f"MCL Callback: Error in ora_after_model_callback processing: {e}", exc_info=True)
        _add_mcl_observation(
            state=callback_context.state,
            observation_type="ora_llm_response_received_error",
            data={"error": f"Failed to extract response summary: {str(e)}"},
            turn_id=callback_context.invocation_id,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    return None


def ora_before_tool_callback(
        tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext
) -> Optional[Dict]:
    agent_name = getattr(tool_context, 'agent_name', "UnknownAgent")
    if agent_name != "ORA":
        return None

    tool_name_log = getattr(tool, 'name', "UnknownTool")
    logger.info(f"MCL Callback (ora_before_tool_callback) for ORA: Capturing tool call '{tool_name_log}'.")

    session_id_for_pls = getattr(tool_context, 'session_id',
                                getattr(getattr(tool_context, 'invocation_context', None), 'session_id', "unknown_session_id"))
    turn_id_for_pls_and_state = getattr(tool_context, 'invocation_id', "unknown_turn_id")

    try:
        # Robust handling of tool description
        tool_description_str = getattr(tool, 'description', None) # Get description, could be None
        if tool_description_str is None:
            tool_description_str = "N/A" # Fallback if description is None
        else:
            tool_description_str = str(tool_description_str)[:100] + "..." # Ensure it's a string and slice

        tool_call_data = {
            "tool_name": tool_name_log,
            "tool_description": tool_description_str, # Use the robustly handled string
            "arguments": {k: (str(v)[:100] + "..." if len(str(v)) > 100 else v) for k, v in args.items()},
        }
        _add_mcl_observation(
            state=tool_context.state,
            observation_type="ora_tool_call_attempted",
            data=tool_call_data,
            turn_id=turn_id_for_pls_and_state,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    except Exception as e:
        logger.error(f"MCL Callback: Error in ora_before_tool_callback processing: {e}", exc_info=True)
        _add_mcl_observation(
            state=tool_context.state,
            observation_type="ora_tool_call_attempted_error",
            data={"error": f"Failed to process tool call data: {str(e)}", "tool_name": tool_name_log},
            turn_id=turn_id_for_pls_and_state,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    return None


def ora_after_tool_callback(
        tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Any
) -> Optional[Dict]:
    agent_name = getattr(tool_context, 'agent_name', "UnknownAgent")
    if agent_name != "ORA":
        return None

    tool_name_log = getattr(tool, 'name', "UnknownTool")
    logger.info(f"MCL Callback (ora_after_tool_callback) for ORA: Capturing tool response for '{tool_name_log}'.")

    # Use safe attribute access for ToolContext session_id (consistency with ora_before_tool_callback)
    session_id_for_pls = getattr(tool_context, 'session_id',
                                getattr(getattr(tool_context, 'invocation_context', None), 'session_id', "unknown_session_id"))
    turn_id_for_pls_and_state = getattr(tool_context, 'invocation_id', "unknown_turn_id")

    try:
        response_summary_str = ""
        if isinstance(tool_response, dict):
            status = tool_response.get("status", tool_response.get("result"))
            if status:
                response_summary_str = f"Status: {status}, Data snippet: {str(tool_response)[:150]}..."
            else:
                response_summary_str = f"Dict keys: {list(tool_response.keys())}, Snippet: {str(tool_response)[:150]}..."
        elif isinstance(tool_response, str):
            response_summary_str = tool_response[:200] + ("..." if len(tool_response) > 200 else "")
        else:
            response_summary_str = str(type(tool_response))

        tool_response_data = {
            "tool_name": tool_name_log,
            "arguments_used": {k: (str(v)[:100] + "..." if len(str(v)) > 100 else v) for k, v in args.items()},
            "response_summary": response_summary_str,
        }
        _add_mcl_observation(
            state=tool_context.state,
            observation_type="ora_tool_response_received",
            data=tool_response_data,
            turn_id=turn_id_for_pls_and_state,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    except Exception as e:
        logger.error(f"MCL Callback: Error in ora_after_tool_callback processing: {e}", exc_info=True)
        _add_mcl_observation(
            state=tool_context.state,
            observation_type="ora_tool_response_received_error",
            data={"error": f"Failed to process tool response data: {str(e)}", "tool_name": tool_name_log},
            turn_id=turn_id_for_pls_and_state,
            source_agent_pls=agent_name,
            session_id_pls=session_id_for_pls
        )
    return None


def simple_prompt_logger_callback(
        callback_context: CallbackContext, llm_request: LlmRequest
) -> Optional[LlmResponse]:
    """
    A simple callback to print LLM prompt details to the console.
    """
    agent_name = callback_context.agent_name
    invocation_id = callback_context.invocation_id

    logger_prompt_debug.info(f"\n--- LLM PROMPT START (Agent: {agent_name}, Turn: {invocation_id}) ---")
    print(f"\n--- LLM PROMPT START (Agent: {agent_name}, Turn: {invocation_id}) ---")  # Use print

    # 1. System Instruction
    system_instruction = None
    if hasattr(llm_request, 'instruction') and llm_request.instruction:
        system_instruction = llm_request.instruction
    elif hasattr(llm_request, 'system_instruction') and llm_request.system_instruction:
        system_instruction = llm_request.system_instruction

    if system_instruction:
        logger_prompt_debug.info(f"[SYSTEM INSTRUCTION]:\n{system_instruction}\n")
    else:
        logger_prompt_debug.info("[SYSTEM INSTRUCTION]: None provided in LlmRequest")

    # 2. Messages / Conversation History
    if hasattr(llm_request, 'contents') and llm_request.contents:
        logger_prompt_debug.info("[MESSAGES]:")
        for i, content_item in enumerate(llm_request.contents):
            role = content_item.role
            parts_str_list = []
            if content_item.parts:
                for part in content_item.parts:
                    if hasattr(part, 'text') and part.text:
                        parts_str_list.append(f"  Text: {part.text}")
                    if hasattr(part, 'function_call') and part.function_call:
                        parts_str_list.append(
                            f"  FunctionCall: {part.function_call.name}(args={part.function_call.args})")
                    # Add other part types if needed
            logger_prompt_debug.info(f"  ({i + 1}) Role: {role}\n" + "\n".join(parts_str_list))
        logger_prompt_debug.info("-" * 20)
    else:
        logger_prompt_debug.info("[MESSAGES]: None provided in LlmRequest.contents")

    # 3. Tools / Function Declarations
    if hasattr(llm_request, 'tools') and llm_request.tools:
        logger_prompt_debug.info("[TOOLS DECLARED]:")
        for i, adk_tool_declaration in enumerate(llm_request.tools):
            # ADK tools are often FunctionDeclarations or similar Pydantic models
            tool_name = getattr(adk_tool_declaration, 'name', f"UnknownTool_{i}")
            tool_desc = getattr(adk_tool_declaration, 'description', "No description")
            # For Pydantic models, model_dump() is useful, otherwise str()
            try:
                params = adk_tool_declaration.parameters.model_dump_json(indent=2) if hasattr(adk_tool_declaration,
                                                                                              'parameters') and hasattr(
                    adk_tool_declaration.parameters, 'model_dump_json') else "{}"
            except:
                params = str(getattr(adk_tool_declaration, 'parameters', {}))

            logger_prompt_debug.info(f"  ({i + 1}) Name: {tool_name}")
            logger_prompt_debug.info(f"      Desc: {tool_desc}")
            logger_prompt_debug.info(f"      Params Schema: {params}")
        logger_prompt_debug.info("-" * 20)
    else:
        logger_prompt_debug.info("[TOOLS DECLARED]: None")

    # 4. Generation Config
    if hasattr(llm_request, 'generate_config') and llm_request.generate_config:
        try:
            gen_config_str = llm_request.generate_config.model_dump_json(indent=2) if hasattr(
                llm_request.generate_config, 'model_dump_json') else str(llm_request.generate_config)
        except:
            gen_config_str = str(llm_request.generate_config)
        logger_prompt_debug.info(f"[GENERATION CONFIG]:\n{gen_config_str}\n")
    else:
        logger_prompt_debug.info("[GENERATION CONFIG]: None")

    logger_prompt_debug.info(f"--- LLM PROMPT END (Agent: {agent_name}, Turn: {invocation_id}) ---\n")

    return None  # Don't modify the request


async def gather_mcl_input_from_state(session_state: Dict[str, Any], turn_id: str) -> Dict[str, Any]:
    """
    Gathers necessary information from session_state to prepare input for the MCL_Agent.
    This function reads from the volatile session state log.
    """
    logger.info(f"MCL: Gathering input from session state for turn_id '{turn_id}'.")

    turn_observations_from_state = [
        obs for obs in session_state.get(MCL_OBSERVATIONS_LIST_KEY, [])
        if obs.get("turn_id") == turn_id
    ]

    # It's also beneficial to fetch specific NCF params and user query for this turn if logged to state
    # Example keys, adjust if they are stored differently
    current_ncf_params = session_state.get(f"ora_turn_ncf_params:{turn_id}", {})
    last_user_query = session_state.get(f"ora_turn_user_query:{turn_id}", "N/A")

    mcl_input = {
        "turn_id": turn_id,
        # This uses the in-memory session log for immediate MCL reaction
        "interaction_log_snippet_from_session_state": turn_observations_from_state[-5:],
        # Last 5 observations from session state
        "full_observation_log_for_turn_from_session_state": turn_observations_from_state,
        "current_ncf_parameters_for_turn": current_ncf_params,
        "ora_last_user_query_for_turn": last_user_query
    }
    logger.debug(f"MCL Input (from session state) for turn {turn_id}: {json.dumps(mcl_input, default=str)[:500]}...")
    return mcl_input

# -------------------- safety_callbacks.py --------------------

# Safety Callbacks
# ceaf_project/ceaf_core/callbacks/safety_callbacks.py

import logging
import re
from typing import Optional, Dict, Any, List

from google.adk.agents.callback_context import CallbackContext
from google.adk.models.llm_request import LlmRequest
from google.adk.models.llm_response import LlmResponse
from google.adk.tools.base_tool import BaseTool # Correctly imported here
from google.adk.tools.tool_context import ToolContext
from google.genai import types as genai_types # For constructing LlmResponse content

logger = logging.getLogger(__name__)

# --- Configuration for Safety Callbacks ---

# Input Guardrails (before_model_callback)
# Simple keyword blocking. In production, this would be more sophisticated
# (e.g., using a dedicated content moderation API or more advanced NLP).
DISALLOWED_INPUT_KEYWORDS = [
    "illegal",
    "harmful_activity_xyz", # Replace with actual specific harmful keywords
    "self_destruct_override_alpha_gamma_7", # Example of a sensitive internal command
]
INPUT_BLOCK_MESSAGE = "I'm sorry, but I cannot process requests containing certain restricted terms or topics. Please rephrase your query."

# Tool Argument Guardrails (before_tool_callback)
# Example: Prevent a hypothetical 'execute_system_command' tool from running dangerous commands.
DANGEROUS_SYSTEM_COMMANDS_PATTERNS = [
    r"rm\s+-rf",
    r"mkfs",
    # Add more regex patterns for dangerous commands
]
TOOL_ARG_BLOCK_MESSAGE = "Policy restriction: The requested tool operation with the provided arguments is not permitted for safety reasons."

# Example: Restrict a 'file_access_tool' to specific directories
ALLOWED_FILE_PATHS_REGEX = r"^(/safe_dir/|/user_files/)" # Tool can only access these
FILE_PATH_BLOCK_MESSAGE = "Policy restriction: Access to the specified file path is not allowed."


# --- Generic Safety Callbacks (Can be applied to any CEAF Agent) ---

def generic_input_keyword_guardrail(
    callback_context: CallbackContext, llm_request: LlmRequest
) -> Optional[LlmResponse]:
    """
    Inspects the latest user message for disallowed keywords.
    If found, blocks the LLM call and returns a predefined LlmResponse.
    """
    agent_name = callback_context.agent_name
    logger.debug(f"Safety Callback (generic_input_keyword_guardrail) for agent: {agent_name}")

    last_user_message_text = ""
    if llm_request.contents:
        for content in reversed(llm_request.contents): # Most recent user message
            if content.role == 'user' and content.parts:
                # Concatenate text from all parts of the user message
                current_message_texts = [part.text for part in content.parts if part.text]
                if current_message_texts:
                    last_user_message_text = " ".join(current_message_texts).lower()
                    break

    if last_user_message_text:
        for keyword in DISALLOWED_INPUT_KEYWORDS:
            if keyword.lower() in last_user_message_text:
                logger.warning(
                    f"Safety Callback: Agent '{agent_name}' - Blocked input for user '{callback_context.user_id}' "
                    f"due to keyword: '{keyword}' in message: '{last_user_message_text[:100]}...'"
                )
                callback_context.state["safety:last_input_block_reason"] = f"Keyword: {keyword}"
                return LlmResponse(
                    content=genai_types.Content(
                        role="model",
                        parts=[genai_types.Part(text=INPUT_BLOCK_MESSAGE)],
                    )
                )
    logger.debug(f"Safety Callback (generic_input_keyword_guardrail) for '{agent_name}': Input passed.")
    return None


def generic_tool_argument_guardrail(
    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext # CORRECTED: Use BaseTool
) -> Optional[Dict]:
    """
    Inspects tool arguments for disallowed patterns or values based on tool name.
    If a violation is found, blocks the tool call and returns a predefined error dictionary.
    """
    # Ensure tool_context has agent_name, or provide a default for logging
    agent_name_log = getattr(tool_context, 'agent_name', "UnknownAgent")
    # Ensure tool has a name, or provide a default
    tool_name_log = getattr(tool, 'name', "UnknownTool")

    logger.debug(f"Safety Callback (generic_tool_argument_guardrail) for tool '{tool_name_log}' in agent '{agent_name_log}'")

    # --- Example 1: Guarding a hypothetical 'execute_system_command' tool ---
    if tool_name_log == "execute_system_command": # Ensure this matches your actual tool name
        command_arg = args.get("command_string", "") # Assuming the tool takes 'command_string'
        if isinstance(command_arg, str):
            for pattern in DANGEROUS_SYSTEM_COMMANDS_PATTERNS:
                if re.search(pattern, command_arg, re.IGNORECASE):
                    user_id_log = getattr(tool_context, 'user_id', "UnknownUser")
                    logger.warning(
                        f"Safety Callback: Agent '{agent_name_log}' - Blocked tool '{tool_name_log}' for user '{user_id_log}' "
                        f"due to dangerous command pattern: '{pattern}' in args: {args}"
                    )
                    tool_context.state["safety:last_tool_block_reason"] = f"Dangerous command: {pattern} for tool {tool_name_log}"
                    return {"status": "error", "error_message": TOOL_ARG_BLOCK_MESSAGE, "details": "Attempted unsafe system command."}

    # --- Example 2: Guarding a hypothetical 'file_access_tool' ---
    if tool_name_log == "file_access_tool": # Ensure this matches your actual tool name
        file_path_arg = args.get("path", "") # Assuming the tool takes 'path'
        if isinstance(file_path_arg, str):
            if not re.match(ALLOWED_FILE_PATHS_REGEX, file_path_arg):
                user_id_log = getattr(tool_context, 'user_id', "UnknownUser")
                logger.warning(
                    f"Safety Callback: Agent '{agent_name_log}' - Blocked tool '{tool_name_log}' for user '{user_id_log}' "
                    f"due to disallowed file path: '{file_path_arg}'"
                )
                tool_context.state["safety:last_tool_block_reason"] = f"Disallowed path: {file_path_arg} for tool {tool_name_log}"
                return {"status": "error", "error_message": FILE_PATH_BLOCK_MESSAGE, "details": f"Access to path '{file_path_arg}' denied."}

    logger.debug(f"Safety Callback (generic_tool_argument_guardrail) for tool '{tool_name_log}': Args passed.")
    return None


# You could also add after_model_callbacks for output safety, e.g., to scan for PII
# or harmful content in the LLM's generated response before it's sent to the user.

def generic_output_filter_guardrail(
    callback_context: CallbackContext, llm_response: LlmResponse
) -> Optional[LlmResponse]:
    """
    Inspects the LLM's final response content for potentially harmful patterns or PII.
    If found, modifies or blocks the response. (This is a conceptual placeholder).
    """
    agent_name = callback_context.agent_name
    logger.debug(f"Safety Callback (generic_output_filter_guardrail) for agent: {agent_name}")

    modified_response = False
    new_parts = []

    if llm_response.content and llm_response.content.parts:
        for part in llm_response.content.parts:
            if part.text:
                original_text = part.text
                # --- Placeholder for PII detection/redaction ---
                # For example, using a regex or a PII detection library
                # Simple example: look for email patterns
                redacted_text = re.sub(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", "[REDACTED EMAIL]", original_text)
                if redacted_text != original_text:
                    logger.info(f"Safety Callback: Agent '{agent_name}' - Redacted potential PII in output for user '{callback_context.user_id}'.")
                    modified_response = True
                    new_parts.append(genai_types.Part(text=redacted_text))
                    callback_context.state["safety:last_output_modification"] = "PII redaction"
                else:
                    new_parts.append(part) # Keep original part
            else:
                new_parts.append(part) # Keep non-text parts

    if modified_response:
        logger.info(f"Safety Callback: Agent '{agent_name}' - Modifying LLM response due to output filter.")
        # Return a new LlmResponse with modified content
        # Important: try to preserve other aspects of llm_response if needed (finish_reason, etc.)
        # For simplicity, we are only changing content here.
        return LlmResponse(
            content=genai_types.Content(role=llm_response.content.role, parts=new_parts),
            finish_reason=llm_response.finish_reason, # Preserve original finish reason
            usage_metadata=llm_response.usage_metadata # Preserve usage
            # Ensure other fields are copied if necessary for your ADK version/usage
        )

    logger.debug(f"Safety Callback (generic_output_filter_guardrail) for '{agent_name}': Output passed.")
    return None

# -------------------- __init__.py --------------------

# CEAF Callbacks


# -------------------- model_configs.py --------------------

# Model Configurations
# ceaf_project/ceaf_core/config/model_configs.py

import os
import logging

logger = logging.getLogger(__name__)

# --- OpenRouter Configuration ---
# These are typically set in .env and picked up by LiteLLM automatically,
# but defining them here provides a central reference and defaults if not set.
DEFAULT_OPENROUTER_API_BASE = "https://openrouter.ai/api/v1"
OPENROUTER_API_BASE = os.getenv("OPENROUTER_API_BASE", DEFAULT_OPENROUTER_API_BASE)

# API Key is critical and should be in .env, not defaulted here.
# This is more for awareness in other parts of the code.
OPENROUTER_API_KEY_ENV_VAR = "OPENROUTER_API_KEY"


# --- Default Model Names on OpenRouter for CEAF Agents ---
# These can be overridden by environment variables.

# ORA (Orchestrator/Responder Agent) - Needs to be capable and good at instruction following.
ORA_DEFAULT_MODEL_ENV_VAR = "ORA_DEFAULT_MODEL"
ORA_FALLBACK_MODEL = "openrouter/openai/gpt-4.1" # Good balance

# NCIM (Narrative Coherence & Identity Module) - Needs good text understanding and nuance.
NCIM_DEFAULT_MODEL_ENV_VAR = "NCIM_DEFAULT_MODEL"
NCIM_FALLBACK_MODEL = "openrouter/openai/gpt-4.1" # Faster, good for focused analysis

# VRE (Virtue & Reasoning Engine) - Needs strong reasoning, potentially logical capabilities.
VRE_DEFAULT_MODEL_ENV_VAR = "VRE_DEFAULT_MODEL"
VRE_FALLBACK_MODEL = "openrouter/openai/gpt-4.1" # Sonnet for more complex reasoning

# MCL (Metacognitive Control Loop) - Needs analytical skills.
MCL_DEFAULT_MODEL_ENV_VAR = "MCL_DEFAULT_MODEL"
MCL_FALLBACK_MODEL = "openrouter/openai/gpt-4.1" # Could also be Sonnet if tasks are complex

# A general CEAF default if a specific agent doesn't have one defined
CEAF_GENERAL_DEFAULT_MODEL_ENV_VAR = "CEAF_DEFAULT_OPENROUTER_MODEL"
CEAF_GENERAL_FALLBACK_MODEL = "openrouter/openai/gpt-4.1" # A fast and capable general model


# --- Function to Get Model Name for an Agent ---
def get_agent_model_name(
    agent_type_env_var: str,
    fallback_model: str,
    general_default_env_var: str = CEAF_GENERAL_DEFAULT_MODEL_ENV_VAR,
    general_fallback_model: str = CEAF_GENERAL_FALLBACK_MODEL
) -> str:
    """
    Resolves the model name for an agent based on environment variables and fallbacks.
    Priority:
    1. Specific agent environment variable (e.g., ORA_DEFAULT_MODEL)
    2. Specific agent fallback model (e.g., ORA_FALLBACK_MODEL)
    3. General CEAF environment variable (e.g., CEAF_DEFAULT_OPENROUTER_MODEL) - though specific fallback is usually better.
    4. General CEAF fallback model (e.g., CEAF_GENERAL_FALLBACK_MODEL)
    """
    model_name = os.getenv(agent_type_env_var)
    if model_name:
        logger.info(f"Using model '{model_name}' from environment variable '{agent_type_env_var}'.")
        return model_name

    # If specific agent env var is not set, use its defined fallback.
    # The fallback_model parameter already serves this purpose.
    # No need to check general_default_env_var before specific fallback.
    logger.info(f"Environment variable '{agent_type_env_var}' not set. Using fallback model '{fallback_model}'.")
    return fallback_model


# -------------------- __init__.py --------------------

# CEAF Config


# -------------------- __init__.py --------------------

# External Integrations


# -------------------- client.py --------------------

# ceaf_project/ceaf_core/external_integrations/a2a/client.py

import asyncio
import json
import uuid
import logging
import time
from typing import Dict, Any, Optional, List, AsyncGenerator

import httpx  # For async HTTP requests
from pydantic import BaseModel


# Import data models (ideally from a shared A2A lib or define consistently)
# For this example, we might redefine or assume they are accessible
# from .server import A2APart, A2AMessage, A2ATask, A2ASendParams # If in same package
# Or define them again if client is truly separate:

class A2APart(BaseModel):  # Copied from server for example
    type: str
    text: Optional[str] = None
    file: Optional[Dict[str, Any]] = None
    data: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


class A2AMessage(BaseModel):  # Copied
    role: str
    parts: List[A2APart]
    metadata: Optional[Dict[str, Any]] = None


class A2ATaskStatus(BaseModel):  # Copied
    state: str
    message: Optional[A2AMessage] = None
    timestamp: Optional[str] = None


class A2AArtifact(BaseModel):  # Copied
    name: Optional[str] = None
    description: Optional[str] = None
    parts: List[A2APart]
    metadata: Optional[Dict[str, Any]] = None
    index: int = 0
    append: Optional[bool] = False
    lastChunk: Optional[bool] = False


class A2ATask(BaseModel):  # Copied
    id: str
    sessionId: Optional[str] = None
    status: A2ATaskStatus
    history: Optional[List[A2AMessage]] = None
    artifacts: Optional[List[A2AArtifact]] = None
    metadata: Optional[Dict[str, Any]] = None


# --- End Copied Models ---

logger = logging.getLogger("A2AClient")

A2A_REQUEST_ID_COUNTER = 1


async def _make_a2a_request(server_url: str, method: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """Helper to make a generic A2A JSON-RPC request."""
    global A2A_REQUEST_ID_COUNTER
    request_payload = {
        "jsonrpc": "2.0",
        "id": A2A_REQUEST_ID_COUNTER,
        "method": method,
        "params": params
    }
    A2A_REQUEST_ID_COUNTER += 1

    async with httpx.AsyncClient(timeout=30.0) as client:  # Timeout for requests
        try:
            logger.debug(
                f"A2A Client: Sending POST to {server_url}, method: {method}, params: {json.dumps(params, default=str)[:200]}...")
            response = await client.post(server_url, json=request_payload)
            response.raise_for_status()  # Raise an exception for HTTP error codes
            response_json = response.json()
            logger.debug(f"A2A Client: Received response: {json.dumps(response_json, default=str)[:200]}...")

            if "error" in response_json:
                logger.error(f"A2A server returned an error for method {method}: {response_json['error']}")
                # You might want to raise a custom exception here
            return response_json
        except httpx.HTTPStatusError as e:
            logger.error(
                f"A2A HTTP error for method {method} to {server_url}: {e.response.status_code} - {e.response.text}")
            raise  # Re-raise to be handled by caller
        except httpx.RequestError as e:
            logger.error(f"A2A request error for method {method} to {server_url}: {e}")
            raise
        except json.JSONDecodeError as e:
            logger.error(
                f"A2A JSON decode error for method {method} from {server_url}: {e}. Response text: {response.text if 'response' in locals() else 'N/A'}")
            raise


async def discover_a2a_agent(server_base_url: str) -> Optional[Dict[str, Any]]:
    """Fetches the Agent Card from an A2A server."""
    agent_card_url = f"{server_base_url.rstrip('/')}/.well-known/agent.json"
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            logger.info(f"A2A Client: Discovering agent at {agent_card_url}")
            response = await client.get(agent_card_url)
            response.raise_for_status()
            agent_card = response.json()
            logger.info(f"A2A Client: Discovered agent '{agent_card.get('name')}'")
            return agent_card
    except Exception as e:
        logger.error(f"A2A Client: Failed to discover agent at {agent_card_url}: {e}")
        return None


async def send_task_to_a2a_agent(
        server_url: str,
        task_id: str,
        message_text: str,
        session_id: Optional[str] = None
) -> Optional[A2ATask]:
    """Sends a new task (or message to an existing task) to an A2A agent."""
    user_message = A2AMessage(
        role="user",
        parts=[A2APart(type="text", text=message_text)]
    )
    params = {
        "id": task_id,
        "message": user_message.model_dump(exclude_none=True)
    }
    if session_id:
        params["sessionId"] = session_id

    response_json = await _make_a2a_request(server_url, "tasks/send", params)
    if response_json and "result" in response_json:
        try:
            return A2ATask(**response_json["result"])
        except Exception as e:  # Pydantic validation error
            logger.error(f"A2A Client: Could not parse task result from tasks/send: {e}")
            return None
    return None


async def stream_task_from_a2a_agent(
        server_url: str,
        task_id: str,
        message_text: str,
        session_id: Optional[str] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Sends a task using tasks/sendSubscribe and streams updates (status, artifacts).
    Yields raw event dictionaries from the SSE stream.
    """
    user_message = A2AMessage(
        role="user",
        parts=[A2APart(type="text", text=message_text)]
    )
    params = {
        "id": task_id,
        "message": user_message.model_dump(exclude_none=True)
    }
    if session_id:
        params["sessionId"] = session_id

    request_payload = {
        "jsonrpc": "2.0",
        "id": A2A_REQUEST_ID_COUNTER,  # Use global counter
        "method": "tasks/sendSubscribe",
        "params": params
    }
    # Increment counter after use (or manage IDs per request if truly parallel)
    global A2A_REQUEST_ID_COUNTER
    A2A_REQUEST_ID_COUNTER += 1

    try:
        async with httpx.AsyncClient(timeout=None) as client:  # Timeout None for streaming
            logger.info(f"A2A Client: Sending tasks/sendSubscribe to {server_url} for task {task_id}")
            async with client.stream("POST", server_url, json=request_payload) as response:
                response.raise_for_status()  # Check initial HTTP status
                logger.info(f"A2A Client: SSE Stream opened for task {task_id}. Status: {response.status_code}")
                async for line in response.aiter_lines():
                    if line.startswith("data:"):
                        data_str = line[len("data:"):].strip()
                        if data_str:
                            try:
                                event_data = json.loads(data_str)
                                logger.debug(
                                    f"A2A Client (Task {task_id}) SSE Event: {event_data.get('result', {}).get('status', {}).get('state', event_data)}")
                                yield event_data  # Yield the raw JSON-RPC like event
                                if event_data.get("result", {}).get("status", {}).get("final") is True:
                                    logger.info(f"A2A Client: SSE Stream indicated final event for task {task_id}.")
                                    break  # Stop if server marks as final
                            except json.JSONDecodeError:
                                logger.warning(f"A2A Client: Could not decode SSE JSON: {data_str}")
                    elif line.strip():  # Log other non-empty lines for debugging
                        logger.debug(f"A2A Client (Task {task_id}) SSE Raw Line: {line}")


    except httpx.HTTPStatusError as e:
        logger.error(
            f"A2A SSE HTTP error for task {task_id} to {server_url}: {e.response.status_code} - {e.response.text}")
        yield {"error": {"code": e.response.status_code, "message": f"HTTP error: {e.response.text}"}}
    except httpx.RequestError as e:
        logger.error(f"A2A SSE request error for task {task_id} to {server_url}: {e}")
        yield {"error": {"code": -1, "message": f"Request error: {str(e)}"}}
    except Exception as e:
        logger.error(f"A2A Client: Unexpected error in stream_task_from_a2a_agent for task {task_id}: {e}",
                     exc_info=True)
        yield {"error": {"code": -1, "message": f"Unexpected streaming error: {str(e)}"}}


async def get_a2a_task_status(server_url: str, task_id: str) -> Optional[A2ATask]:
    """Retrieves the status and details of an existing A2A task."""
    params = {"id": task_id}
    response_json = await _make_a2a_request(server_url, "tasks/get", params)
    if response_json and "result" in response_json:
        try:
            return A2ATask(**response_json["result"])
        except Exception as e:  # Pydantic validation error
            logger.error(f"A2A Client: Could not parse task result from tasks/get: {e}")
            return None
    return None



# -------------------- server.py --------------------

# A2A Server
# ceaf_project/ceaf_core/external_integrations/a2a/server.py

import asyncio
import json
import uuid
import time
import logging
from typing import Dict, Any, List, Tuple, Optional

from fastapi import FastAPI, Request, HTTPException, Response
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field


# --- A2A Protocol Data Structures (Simplified based on spec) ---
# In a real scenario, these would come from an official A2A library

class A2APart(BaseModel):
    type: str  # "text", "file", "data"
    text: Optional[str] = None
    file: Optional[Dict[str, Any]] = None  # e.g., {"name": "report.pdf", "mimeType": "application/pdf", "uri": "..."}
    data: Optional[Dict[str, Any]] = None  # For structured JSON
    metadata: Optional[Dict[str, Any]] = None


class A2AMessage(BaseModel):
    role: str  # "user", "agent"
    parts: List[A2APart]
    metadata: Optional[Dict[str, Any]] = None


class A2AArtifact(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    parts: List[A2APart]
    metadata: Optional[Dict[str, Any]] = None
    index: int = 0
    append: Optional[bool] = False
    lastChunk: Optional[bool] = False


class A2ATaskStatus(BaseModel):
    state: str  # "submitted", "working", "input-required", "completed", "failed", "canceled"
    message: Optional[A2AMessage] = None
    timestamp: Optional[str] = Field(
        default_factory=lambda: time.toISOString())  # Assuming time.toISOString() available


class A2ATask(BaseModel):
    id: str
    sessionId: Optional[str] = None
    status: A2ATaskStatus
    history: Optional[List[A2AMessage]] = None
    artifacts: Optional[List[A2AArtifact]] = None
    metadata: Optional[Dict[str, Any]] = None


class A2ASendParams(BaseModel):
    id: str  # Task ID
    sessionId: Optional[str] = None
    message: A2AMessage
    # historyLength: Optional[int] = None
    # pushNotification: Optional[Dict[str, Any]] = None # Simplified for now


class A2ARPCRequest(BaseModel):
    jsonrpc: str = "2.0"
    id: int  # Request ID
    method: str  # e.g., "tasks/send"
    params: Dict[str, Any]


# --- Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ResearchAgentA2AServer")

# --- FastAPI App for ResearchAgent Server ---
research_agent_a2a_app = FastAPI(
    title="External ResearchAgent A2A Server",
    description="Exposes a 'perform_deep_research' skill via A2A protocol.",
)

# In-memory store for tasks for this simple server
# Production would use a persistent DB
active_tasks: Dict[str, A2ATask] = {}

# --- Agent Card ---
RESEARCH_AGENT_CARD = {
    "name": "Specialized Research Agent",
    "description": "Performs in-depth research on specified topics and generates reports.",
    "url": "http://localhost:8001",  # Assuming this server runs on port 8001
    "provider": {"organization": "Research Inc.", "url": "https://research.example.com"},
    "version": "1.0.0",
    "authentication": {"schemes": ["None"]},  # For simplicity, no auth
    "defaultInputModes": ["text/plain"],
    "defaultOutputModes": ["application/json", "text/plain"],
    "capabilities": {"streaming": True, "pushNotifications": False},  # Supports streaming
    "skills": [
        {
            "id": "perform_deep_research",
            "name": "Perform Deep Research",
            "description": "Takes a research topic, conducts deep research, and returns a structured report. Supports streaming updates.",
            "tags": ["research", "analysis", "reporting"],
            "examples": ["Research quantum computing advancements in 2024."],
            "inputModes": ["text/plain"],  # Expects topic as text
            "outputModes": ["application/json"]  # Returns report as JSON
        }
    ]
}


@research_agent_a2a_app.get("/.well-known/agent.json", summary="Get Agent Card")
async def get_agent_card():
    return JSONResponse(content=RESEARCH_AGENT_CARD)


# --- Core Skill Logic ---
async def _execute_deep_research(task_id: str, topic: str, stream_writer=None):
    """Simulates the research process, yielding updates if streaming."""
    logger.info(f"Task {task_id}: Starting research on '{topic}'")

    async def send_status_update(state: str, message_text: Optional[str] = None, final: bool = False):
        if stream_writer:
            status_obj = A2ATaskStatus(state=state)
            if message_text:
                status_obj.message = A2AMessage(role="agent", parts=[A2APart(type="text", text=message_text)])

            event_data = {
                "jsonrpc": "2.0",  # Part of SSE stream content
                "result": {"id": task_id, "status": status_obj.model_dump(exclude_none=True), "final": final}
            }
            # SSE format: data: <json_string>\n\n
            await stream_writer(f"data: {json.dumps(event_data)}\n\n")
            logger.debug(f"Task {task_id}: Sent SSE status update: {state}")

    async def send_artifact_update(artifact: A2AArtifact, final_artifact_chunk: bool = False):
        if stream_writer:
            artifact.lastChunk = final_artifact_chunk  # Mark if it's the last part of this artifact
            event_data = {
                "jsonrpc": "2.0",
                "result": {"id": task_id, "artifact": artifact.model_dump(exclude_none=True)}
            }
            await stream_writer(f"data: {json.dumps(event_data)}\n\n")
            logger.debug(f"Task {task_id}: Sent SSE artifact update for '{artifact.name}'")

    # 1. Initial "working" status
    active_tasks[task_id].status = A2ATaskStatus(state="working")
    await send_status_update("working", "Research initiated. Analyzing sources...")
    await asyncio.sleep(2)  # Simulate work

    # 2. Intermediate progress update
    await send_status_update("working", "Gathering primary data points...")
    await asyncio.sleep(3)  # Simulate more work

    # 3. Simulate generating parts of an artifact (report)
    report_artifact_name = f"research_report_{topic.replace(' ', '_')}.json"
    report_parts_content = []

    # Part 1 of the report
    part1_data = {"introduction": f"Preliminary findings for '{topic}'.", "confidence": "medium"}
    report_parts_content.append(A2APart(type="data", data=part1_data))
    if stream_writer:  # If streaming, send artifact part by part
        artifact_chunk1 = A2AArtifact(
            name=report_artifact_name,
            description=f"Research report on {topic}",
            parts=[A2APart(type="data", data=part1_data)],
            index=0,  # First artifact
            append=False  # This is the first chunk of this artifact
        )
        await send_artifact_update(artifact_chunk1)
    await asyncio.sleep(2)

    # Part 2 of the report
    part2_data = {"key_findings": ["Finding A", "Finding B related to " + topic, "Finding C"],
                  "data_sources": ["Source X", "Source Y"]}
    report_parts_content.append(A2APart(type="data", data=part2_data))
    if stream_writer:
        artifact_chunk2 = A2AArtifact(
            name=report_artifact_name,  # Same artifact name
            parts=[A2APart(type="data", data=part2_data)],
            index=0,  # Still the first artifact
            append=True  # Appending to the previous part of this artifact
        )
        await send_artifact_update(artifact_chunk2, final_artifact_chunk=True)  # Mark as last chunk for this artifact
    await asyncio.sleep(1)

    # 4. Final "completed" status
    final_status = A2ATaskStatus(state="completed")
    active_tasks[task_id].status = final_status
    active_tasks[task_id].artifacts = [
        A2AArtifact(name=report_artifact_name, description=f"Full research report on {topic}",
                    parts=report_parts_content, index=0)
    ]
    await send_status_update("completed", "Research complete. Report generated.", final=True)  # Mark stream as final
    logger.info(f"Task {task_id}: Research on '{topic}' completed.")


# --- A2A JSON-RPC Endpoint ---
@research_agent_a2a_app.post("/", summary="A2A JSON-RPC Endpoint")
async def a2a_rpc_handler(rpc_request_model: A2ARPCRequest, raw_request: Request):
    # raw_body = await raw_request.body()
    # rpc_request = json.loads(raw_body.decode()) # Manual parsing if not using Pydantic model for full body

    rpc_request = rpc_request_model.model_dump()
    method = rpc_request.get("method")
    params = rpc_request.get("params", {})
    req_id = rpc_request.get("id")

    logger.info(f"Received A2A request: method='{method}', params_keys='{list(params.keys()) if params else None}'")

    if method == "tasks/send" or method == "tasks/sendSubscribe":
        try:
            send_params = A2ASendParams(**params)
        except Exception as e:  # Pydantic validation error
            logger.error(f"Invalid params for {method}: {e}")
            return JSONResponse(
                status_code=400,
                content={"jsonrpc": "2.0", "id": req_id, "error": {"code": -32602, "message": f"Invalid params: {e}"}}
            )

        task_id = send_params.id
        user_message_text = ""
        if send_params.message.parts and send_params.message.parts[0].type == "text":
            user_message_text = send_params.message.parts[0].text

        if not user_message_text:
            return JSONResponse(
                status_code=400,
                content={"jsonrpc": "2.0", "id": req_id,
                         "error": {"code": -32602, "message": "Missing text part in user message for research topic."}}
            )

        # Check if it's a new task or continuation (simplified: always new if not exists)
        if task_id not in active_tasks:
            new_task = A2ATask(
                id=task_id,
                sessionId=send_params.sessionId or str(uuid.uuid4()),
                status=A2ATaskStatus(state="submitted"),
                history=[send_params.message]
            )
            active_tasks[task_id] = new_task
            logger.info(f"New A2A Task created: {task_id} for topic: '{user_message_text}'")
        else:
            # Handle multi-turn tasks if necessary (not for this simple research skill)
            active_tasks[task_id].history.append(send_params.message)
            logger.info(f"Continuing A2A Task: {task_id}")

        # If tasks/sendSubscribe, start streaming
        if method == "tasks/sendSubscribe":
            async def stream_generator():
                # Yield initial acknowledgment for SSE
                # According to A2A, the HTTP response for tasks/sendSubscribe is actually empty (200 OK)
                # and the stream starts immediately after. Some clients might expect an immediate event though.
                # For simplicity, we'll let the first status update from _execute_deep_research be the first SSE event.

                # This function will be responsible for writing to the SSE stream
                async def sse_writer(data_str: str):
                    # This is tricky with FastAPI's StreamingResponse which expects the generator to yield strings directly.
                    # We'll adapt _execute_deep_research to yield strings for SSE.
                    # For this example, we'll directly call and let it manage things.
                    # In a real app, you might pass a queue or a callback.
                    yield data_str  # This is what StreamingResponse expects

                # Start the background task
                # We need to ensure the client doesn't time out waiting for the first byte
                # The _execute_deep_research now takes a stream_writer which is our sse_writer
                asyncio.create_task(_execute_deep_research(task_id, user_message_text, stream_writer=None))

                # This generator will now poll the task status or receive events if _execute_deep_research used a queue
                # For this version, we'll make _execute_deep_research yield the SSE strings directly.

                current_task_state = active_tasks[task_id].status.state
                yield f"data: {json.dumps({'jsonrpc': '2.0', 'result': {'id': task_id, 'status': active_tasks[task_id].status.model_dump(exclude_none=True), 'final': False}})}\n\n"

                while current_task_state not in ["completed", "failed", "canceled"]:
                    await asyncio.sleep(0.5)  # Poll interval
                    if task_id not in active_tasks: break  # Task removed

                    updated_task = active_tasks[task_id]
                    if updated_task.status.state != current_task_state:
                        current_task_state = updated_task.status.state
                        is_final = current_task_state in ["completed", "failed", "canceled"]

                        # Send status update
                        status_event_data = {
                            "jsonrpc": "2.0",
                            "result": {"id": task_id, "status": updated_task.status.model_dump(exclude_none=True),
                                       "final": is_final}
                        }
                        yield f"data: {json.dumps(status_event_data)}\n\n"

                        # Send artifact updates if any (simplified: send all on completion for this poll version)
                        if is_final and updated_task.artifacts:
                            for artifact in updated_task.artifacts:
                                artifact_event_data = {
                                    "jsonrpc": "2.0",
                                    "result": {"id": task_id, "artifact": artifact.model_dump(exclude_none=True)}
                                }
                                yield f"data: {json.dumps(artifact_event_data)}\n\n"
                    if is_final:
                        break

            # Start the background task (non-blocking for FastAPI)
            asyncio.create_task(_execute_deep_research(task_id, user_message_text))
            return StreamingResponse(stream_generator(), media_type="text/event-stream")

        else:  # tasks/send (synchronous-like, but skill is async)
            await _execute_deep_research(task_id, user_message_text)  # Run skill
            final_task_state = active_tasks.get(task_id)
            if final_task_state:
                return JSONResponse(
                    content={"jsonrpc": "2.0", "id": req_id, "result": final_task_state.model_dump(exclude_none=True)})
            else:  # Should not happen if task creation was successful
                return JSONResponse(
                    status_code=500,
                    content={"jsonrpc": "2.0", "id": req_id,
                             "error": {"code": -32000, "message": "Task processing failed unexpectedly."}}
                )


    elif method == "tasks/get":
        task_id = params.get("id")
        if task_id and task_id in active_tasks:
            # historyLength = params.get("historyLength", 0) # Implement if needed
            return JSONResponse(
                content={"jsonrpc": "2.0", "id": req_id, "result": active_tasks[task_id].model_dump(exclude_none=True)})
        else:
            return JSONResponse(
                status_code=404,  # Or A2A specific error
                content={"jsonrpc": "2.0", "id": req_id, "error": {"code": -32001, "message": "Task not found."}}
            )

    # Implement tasks/cancel, tasks/pushNotification/set, tasks/pushNotification/get, tasks/resubscribe as needed

    else:
        return JSONResponse(
            status_code=400,
            content={"jsonrpc": "2.0", "id": req_id, "error": {"code": -32601, "message": "Method not found."}}
        )

# To run this server (save as server.py in the a2a directory):
# uvicorn ceaf_core.external_integrations.a2a.server:research_agent_a2a_app --port 8001 --reload

# -------------------- __init__.py --------------------

# A2A Integration


# -------------------- server.py --------------------

# MCP Server
# ceaf_project/ceaf_core/external_integrations/mcp/server.py

import asyncio
import json
import logging
import os

# --- Environment and Path Setup ---
# This helps if running the server directly and ceaf_core is not in PYTHONPATH yet
import sys
import time
import uuid
from pathlib import Path
from typing import List, Any

# Add the project root to sys.path to allow imports like 'from ceaf_core...'
project_root = Path(__file__).resolve().parents[
    3]  # Adjust if structure changes (root is 3 levels up from mcp/server.py)
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

# --- MCP Server Imports ---
from mcp import types as mcp_types  # Use alias to avoid conflict with genai.types
from mcp.server.lowlevel import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio  # For running over stdio

# --- ADK Tool Imports & Conversion Utility ---
from google.adk.tools import FunctionTool, ToolContext  # For dummy context
from google.adk.tools.mcp_tool.conversion_utils import adk_to_mcp_tool_type
from google.adk.sessions import InMemorySessionService, State  # For dummy context

# Import the specific ADK tool from CEAF to expose
try:
    from ceaf_core.tools.ncf_tools import ncf_tool as ceaf_ncf_adk_tool
except ImportError:
    logging.error("Failed to import 'ceaf_ncf_adk_tool' from 'ceaf_core.tools.ncf_tools'. Ensure the tool is defined.")


    # Define a dummy ADK tool if import fails, so server can still start (in degraded mode)
    def _dummy_ncf_func(user_query: str, tool_context: ToolContext) -> dict:
        return {"status": "error", "ncf": "Dummy NCF Tool: CEAF NCF ADK tool not loaded."}


    ceaf_ncf_adk_tool = FunctionTool(
        func=_dummy_ncf_func,
        description="Generates a Narrative Context Frame for a given query (Dummy Version)."
    )
    logging.warning("Using a DUMMY NCF ADK tool for the MCP server.")

# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("CEAF_MCP_NCF_Server")

# --- Load Environment Variables (If ADK tools need them, e.g., API keys for sub-calls) ---
# NCF tool might eventually call other services needing keys.
load_dotenv(dotenv_path=project_root / ".env")  # Load .env from project root

# --- Prepare the ADK Tool to be Exposed ---
# The ceaf_ncf_adk_tool is already instantiated from the import.
logger.info(
    f"ADK tool '{ceaf_ncf_adk_tool.name}' (version: {'CEAF Core' if ceaf_ncf_adk_tool.func != _dummy_ncf_func else 'Dummy'}) will be exposed via MCP.")

# --- MCP Server Setup ---
logger.info("Creating MCP Server instance...")
# Create a named MCP Server instance
app = Server(
    name="ceaf-ncf-mcp-server",
    display_name="CEAF Narrative Context Frame Server",
    version="0.1.0"
)


@app.list_tools()
async def list_mcp_tools() -> list[mcp_types.Tool]:
    """MCP handler to list available tools."""
    logger.info("MCP Server: Received list_tools request.")
    try:
        # Convert the ADK tool's definition to MCP format
        mcp_tool_schema = adk_to_mcp_tool_type(ceaf_ncf_adk_tool)
        logger.info(
            f"MCP Server: Advertising tool: {mcp_tool_schema.name} (Input Schema: {mcp_tool_schema.input_schema})")
        return [mcp_tool_schema]
    except Exception as e:
        logger.error(f"MCP Server: Error converting ADK tool to MCP schema: {e}", exc_info=True)
        return []


@app.call_tool()
async def call_mcp_tool(
        name: str, arguments: dict[str, Any], context: mcp_types.ToolCallContext
) -> list[mcp_types.Content]:  # MCP expects a list of Content objects
    """MCP handler to execute a tool call."""
    logger.info(f"MCP Server: Received call_tool request for '{name}' with args: {arguments}")

    # Check if the requested tool name matches our wrapped ADK tool
    if name == ceaf_ncf_adk_tool.name:
        try:
            # --- Prepare a dummy ADK ToolContext ---
            # The ADK tool expects a ToolContext. Since this MCP server is not running
            # within a full ADK Runner invocation, we need to mock or create a minimal one.
            # For the NCF tool, it might use tool_context.state if it becomes more dynamic.
            # For now, a simple one is fine.
            dummy_session_service = InMemorySessionService()
            # Use invocation_id from MCP context if available, else generate one
            invocation_id = context.invocation_id if context and context.invocation_id else f"mcp-inv-{uuid.uuid4().hex[:8]}"

            # Create a dummy session state for the ToolContext
            # The NCF tool might read/write to this state.
            # We need a 'State' object, not just a dict, if tool_context expects one.
            # ADK's ToolContext has `state` as a `google.adk.sessions.State` object.

            # A minimal InvocationContext for ToolContext (most fields can be None/default)
            # This part is tricky as InvocationContext is usually built by the ADK Runner.
            # We create a simplified version.
            class MinimalInvocationContext:
                def __init__(self, state_dict: dict, inv_id: str):
                    self.session = MinimalSession(state_dict, inv_id)
                    self.app_name = "mcp_exposed_app"  # Dummy app name
                    self.user_id = "mcp_user"  # Dummy user
                    self.session_id = self.session.id
                    self.invocation_id = inv_id
                    self.current_agent_name = "mcp_tool_host_agent"  # Dummy agent name
                    # Other fields like runner, services are harder to mock simply.
                    # Hope that ncf_tool doesn't rely on them too much.
                    self.runner = None  # Cannot easily provide the full runner here
                    self.session_service = dummy_session_service
                    self.memory_service = None
                    self.artifact_service = None
                    self.current_events: List[Any] = []
                    self.parent_agent: Any = None
                    self.run_config: Any = None  # Assuming default run_config

            class MinimalSession:
                def __init__(self, state_dict: dict, sess_id: str):
                    self.state = State(value=state_dict.copy(), delta={})  # ADK State object
                    self.id = sess_id
                    self.user_id = "mcp_user"
                    self.app_name = "mcp_exposed_app"
                    self.events = []
                    self.last_update_time = time.time()

            # Initial state for the dummy context for this call
            initial_state_dict = {"mcp_call_source": context.client_name if context else "unknown_mcp_client"}

            # MCP's ToolCallContext provides `tool_inputs` which are the files/resources.
            # Our NCF tool currently takes `user_query: str`. If it needed files, we'd map them.
            # `arguments` from MCP call_tool maps to ADK tool `args`.

            mock_invocation_context = MinimalInvocationContext(initial_state_dict, invocation_id)

            adk_tool_context = ToolContext(
                invocation_context=mock_invocation_context,
                function_call_id=invocation_id,  # Use a unique ID for the call
            )
            # Now, the adk_tool_context.state is an ADK State object
            # logger.debug(f"MCP Server: Prepared dummy ADK ToolContext. State: {adk_tool_context.state.to_dict()}")

            # Execute the ADK tool's run_async method
            # The 'args' for ADK tool come from 'arguments' in MCP call
            adk_response_dict = await ceaf_ncf_adk_tool.run_async(
                args=arguments,  # e.g., {"user_query": "AI ethics"}
                tool_context=adk_tool_context,
            )
            logger.info(f"MCP Server: ADK tool '{name}' executed. Response: {adk_response_dict}")

            # Format the ADK tool's response (a dict) into MCP Content format.
            # NCF tool returns {"status": "success", "ncf": "..."}
            if adk_response_dict.get("status") == "success" and "ncf" in adk_response_dict:
                ncf_text = adk_response_dict["ncf"]
                # MCP expects a list of mcp_types.Content
                return [mcp_types.TextContent(type="text", text=ncf_text)]
            else:
                error_detail = adk_response_dict.get("error_message",
                                                     adk_response_dict.get("message", "Unknown error from ADK tool"))
                logger.error(f"MCP Server: ADK tool '{name}' returned an error or unexpected format: {error_detail}")
                # Return error as TextContent for simplicity, MCP has more formal error structures
                return [mcp_types.TextContent(type="text",
                                              text=json.dumps({"error": f"ADK tool '{name}' failed: {error_detail}"}))]

        except Exception as e:
            logger.error(f"MCP Server: Error executing ADK tool '{name}': {e}", exc_info=True)
            return [mcp_types.TextContent(type="text", text=json.dumps(
                {"error": f"Server error executing tool '{name}': {str(e)}"}))]
    else:
        logger.warning(f"MCP Server: Tool '{name}' not found or not implemented.")
        return [mcp_types.TextContent(type="text",
                                      text=json.dumps({"error": f"Tool '{name}' not implemented by this MCP server."}))]


# --- MCP Server Runner ---
async def run_mcp_stdio_server():
    """Runs the MCP server over standard input/output."""
    # Use the stdio_server context manager from the MCP library
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        logger.info("CEAF MCP Server (stdio) starting handshake...")
        await app.run(
            read_stream,
            write_stream,
            InitializationOptions(
                # server_name=app.name, # Server.name is already set
                # server_version=app.version, # Server.version is already set
                capabilities=app.get_capabilities(
                    notification_options=NotificationOptions(),  # Default no notifications
                    experimental_capabilities={},
                ),
            ),
        )
        logger.info("CEAF MCP Server (stdio) run loop finished.")


if __name__ == "__main__":
    logger.info("Launching CEAF MCP Server to expose ADK NCF tool via stdio...")
    try:
        asyncio.run(run_mcp_stdio_server())
    except KeyboardInterrupt:
        logger.info("\nCEAF MCP Server stopped by user.")
    except Exception as e:
        logger.error(f"CEAF MCP Server encountered a critical error: {e}", exc_info=True)
    finally:
        logger.info("CEAF MCP Server process exiting.")

# -------------------- toolsets.py --------------------

# MCP Toolsets
# ceaf_project/ceaf_core/external_integrations/mcp/toolsets.py

import os
import logging
import asyncio
from typing import List, Tuple, Optional
from contextlib import AsyncExitStack

from google.adk.tools.mcp_tool.mcp_toolset import MCPToolset, StdioServerParameters, SseServerParams
from google.adk.tools import BaseTool as AdkBaseTool  # For type hinting

# --- Environment and Path Setup ---
# Not strictly needed here if .env is loaded by main.py, but good practice if run standalone
from pathlib import Path
from dotenv import load_dotenv
from pydantic import json

project_root = Path(__file__).resolve().parents[3]
load_dotenv(dotenv_path=project_root / ".env")

logger = logging.getLogger(__name__)

# --- Configuration for External MCP Servers ---

# Filesystem MCP Server
MCP_FS_SERVER_ENABLED = os.getenv("MCP_FS_SERVER_ENABLED", "false").lower() == "true"
MCP_FS_SERVER_COMMAND = os.getenv("MCP_FS_SERVER_COMMAND", "npx")
MCP_FS_SERVER_ARGS_JSON = os.getenv(
    "MCP_FS_SERVER_ARGS_JSON",
    '["-y", "@modelcontextprotocol/server-filesystem"]'  # Default args without path
)
# CRITICAL: This path MUST be an ABSOLUTE path accessible by the npx command.
# It's the root directory the FS MCP server will expose.
MCP_FS_SERVER_ROOT_PATH = os.getenv("MCP_FS_SERVER_ROOT_PATH")  # e.g., "/mnt/ceaf_shared_files"

# Google Maps MCP Server
MCP_MAPS_SERVER_ENABLED = os.getenv("MCP_MAPS_SERVER_ENABLED", "false").lower() == "true"
MCP_MAPS_SERVER_COMMAND = os.getenv("MCP_MAPS_SERVER_COMMAND", "npx")
MCP_MAPS_SERVER_ARGS_JSON = os.getenv(
    "MCP_MAPS_SERVER_ARGS_JSON",
    '["-y", "@modelcontextprotocol/server-google-maps"]'
)
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY_FOR_MCP")  # Specific key for MCP server if needed


# --- MCP Toolset Initializers ---

async def initialize_filesystem_mcp_tools() -> Tuple[Optional[List[AdkBaseTool]], Optional[AsyncExitStack]]:
    """
    Initializes ADK tools from an external Filesystem MCP server.

    Returns:
        A tuple containing:
        - A list of ADK BaseTool objects if successful, else None.
        - An AsyncExitStack to manage the MCP server process lifecycle if successful, else None.
    """
    if not MCP_FS_SERVER_ENABLED:
        logger.info("Filesystem MCP Server integration is disabled via MCP_FS_SERVER_ENABLED.")
        return None, None

    if not MCP_FS_SERVER_ROOT_PATH:
        logger.error("MCP_FS_SERVER_ROOT_PATH is not set. Cannot initialize Filesystem MCP tools.")
        return None, None
    if not Path(MCP_FS_SERVER_ROOT_PATH).is_dir():
        logger.error(f"MCP_FS_SERVER_ROOT_PATH '{MCP_FS_SERVER_ROOT_PATH}' is not a valid directory.")
        return None, None

    try:
        fs_server_args = json.loads(MCP_FS_SERVER_ARGS_JSON)
        # IMPORTANT: The actual root path for the server-filesystem must be the LAST argument.
        fs_server_args.append(MCP_FS_SERVER_ROOT_PATH)

        logger.info(
            f"Attempting to connect to Filesystem MCP server with command: '{MCP_FS_SERVER_COMMAND}' and args: {fs_server_args}")

        tools, exit_stack = await MCPToolset.from_server(
            connection_params=StdioServerParameters(
                command=MCP_FS_SERVER_COMMAND,
                args=fs_server_args,
            )
        )
        logger.info(
            f"Successfully fetched {len(tools)} tools from Filesystem MCP server for path '{MCP_FS_SERVER_ROOT_PATH}'.")
        return tools, exit_stack
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse MCP_FS_SERVER_ARGS_JSON: {e}. Value: {MCP_FS_SERVER_ARGS_JSON}")
        return None, None
    except FileNotFoundError:
        logger.error(
            f"Command '{MCP_FS_SERVER_COMMAND}' for Filesystem MCP server not found. Is npx (or specified command) in PATH?")
        return None, None
    except Exception as e:
        logger.error(f"Failed to initialize Filesystem MCP tools: {e}", exc_info=True)
        return None, None


async def initialize_maps_mcp_tools() -> Tuple[Optional[List[AdkBaseTool]], Optional[AsyncExitStack]]:
    """
    Initializes ADK tools from an external Google Maps MCP server.

    Returns:
        A tuple containing:
        - A list of ADK BaseTool objects if successful, else None.
        - An AsyncExitStack to manage the MCP server process lifecycle if successful, else None.
    """
    if not MCP_MAPS_SERVER_ENABLED:
        logger.info("Google Maps MCP Server integration is disabled via MCP_MAPS_SERVER_ENABLED.")
        return None, None

    if not GOOGLE_MAPS_API_KEY:
        logger.error("GOOGLE_MAPS_API_KEY_FOR_MCP is not set. Cannot initialize Google Maps MCP tools.")
        return None, None

    try:
        maps_server_args = json.loads(MCP_MAPS_SERVER_ARGS_JSON)
        logger.info(
            f"Attempting to connect to Google Maps MCP server with command: '{MCP_MAPS_SERVER_COMMAND}' and args: {maps_server_args}")

        tools, exit_stack = await MCPToolset.from_server(
            connection_params=StdioServerParameters(
                command=MCP_MAPS_SERVER_COMMAND,
                args=maps_server_args,
                env={"GOOGLE_MAPS_API_KEY": GOOGLE_MAPS_API_KEY}  # Pass API key as env var to the npx process
            )
        )
        logger.info(f"Successfully fetched {len(tools)} tools from Google Maps MCP server.")
        return tools, exit_stack
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse MCP_MAPS_SERVER_ARGS_JSON: {e}. Value: {MCP_MAPS_SERVER_ARGS_JSON}")
        return None, None
    except FileNotFoundError:
        logger.error(
            f"Command '{MCP_MAPS_SERVER_COMMAND}' for Google Maps MCP server not found. Is npx (or specified command) in PATH?")
        return None, None
    except Exception as e:
        logger.error(f"Failed to initialize Google Maps MCP tools: {e}", exc_info=True)
        return None, None


async def initialize_all_mcp_toolsets() -> Tuple[List[AdkBaseTool], AsyncExitStack]:
    """
    Initializes all configured external MCP toolsets and aggregates their tools and exit stacks.

    Returns:
        A tuple containing:
        - A list of all ADK BaseTool objects from all successful MCP initializations.
        - A single AsyncExitStack managing all created MCP server processes.
    """
    all_tools: List[AdkBaseTool] = []
    # Create a master exit_stack that will manage individual exit_stacks from each toolset
    master_exit_stack = AsyncExitStack()

    fs_tools, fs_exit_stack = await initialize_filesystem_mcp_tools()
    if fs_tools and fs_exit_stack:
        all_tools.extend(fs_tools)
        # Enter the individual exit_stack into the master_exit_stack.
        # This means when master_exit_stack.aclose() is called, fs_exit_stack.aclose() will also be called.
        await master_exit_stack.enter_async_context(fs_exit_stack)
        logger.info(f"Filesystem MCP tools added. Total tools: {len(all_tools)}")

    maps_tools, maps_exit_stack = await initialize_maps_mcp_tools()
    if maps_tools and maps_exit_stack:
        all_tools.extend(maps_tools)
        await master_exit_stack.enter_async_context(maps_exit_stack)
        logger.info(f"Google Maps MCP tools added. Total tools: {len(all_tools)}")

    # Add more initializers for other MCP toolsets here

    if not all_tools:
        logger.warning(
            "No MCP tools were initialized. Returning empty list and an empty exit stack (will be closed immediately).")
        # If no tools were loaded, the master_exit_stack is still valid but might be empty.
        # To avoid issues with an empty stack that wasn't entered, we can close it if empty.
        # However, it's generally safe to return it as is.
        # If it's truly empty and `aclose` is called, it does nothing.

    return all_tools, master_exit_stack


# -------------------- __init__.py --------------------

# MCP Integration


# -------------------- __init__.py --------------------

# ceaf_core/modules/__init__.py
"""CEAF Modules Package"""

# Import submodules to ensure they're accessible
from . import memory_blossom

from . import vre_engine
from . import mcl_engine
from . import ncf_engine

__all__ = [
    'memory_blossom',
    'vre_engine',
    'mcl_engine',
    'ncf_engine'
]

# -------------------- aura_reflector.py --------------------

import asyncio
import json
import logging
import os
import time
import uuid
from typing import Any, Dict, List, Literal, Optional

import litellm
from dotenv import load_dotenv
from pydantic import BaseModel, Field, ValidationError

from ...services.persistent_log_service import PersistentLogService

# --- Environment Setup ---
try:
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
    dotenv_path = os.path.join(project_root, ".env")
    if os.path.exists(dotenv_path):
        load_dotenv(dotenv_path)
except Exception:
    pass

# --- Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AuraReflector")

# --- Configuration ---
AURA_INSIGHT_GENERATION_MODEL = os.getenv("AURA_INSIGHT_MODEL", "openrouter/openai/gpt-4.1")
AURA_STRATEGY_GENERATION_MODEL = os.getenv("AURA_STRATEGY_MODEL", "openrouter/openai/gpt-4.1")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")


# --- Pydantic Models ---
class PerformanceInsight(BaseModel):
    insight_id: str = Field(default_factory=lambda: f"insight_{uuid.uuid4().hex}")
    timestamp: float = Field(default_factory=time.time)
    description: str
    supporting_evidence_ids: List[str] = Field(default_factory=list)
    confidence: float = Field(..., ge=0.0, le=1.0)
    suggested_refinement_type: Literal[
        "ncf_param_heuristic", "ncf_template_update", "ora_instruction_tweak",
        "mbs_retrieval_bias", "vre_principle_clarification", "mcl_eoc_threshold_adjustment"
    ]
    refinement_details: Dict[str, Any]
    tags: List[str] = Field(default_factory=list)
    potential_impact_score: Optional[float] = Field(None, ge=0.0, le=1.0)


class RefinementStrategy(BaseModel):
    strategy_id: str = Field(default_factory=lambda: f"strat_{uuid.uuid4().hex}")
    timestamp: float = Field(default_factory=time.time)
    description: str
    associated_insight_ids: List[str]
    strategy_type: Literal[
        "ncf_param_heuristic", "ncf_template_update", "ora_instruction_tweak",
        "mbs_retrieval_bias", "vre_principle_clarification", "mcl_eoc_threshold_adjustment",
        "data_collection_for_finetuning", "documentation_update"
    ]
    strategy_payload: Dict[str, Any]
    priority: int = Field(5, ge=1, le=10)
    estimated_effort: Optional[str] = None
    status: Literal["proposed", "approved", "implemented", "rejected", "deferred"] = "proposed"


# --- PersistentLogService Instance ---
try:
    persistent_log_service_instance = PersistentLogService()
    LOG_SERVICE_AVAILABLE = True
except Exception as e_log_service_init:
    logger.error(
        f"AuraReflector: Failed to initialize PersistentLogService: {e_log_service_init}. Aura Reflector analysis will fail.",
        exc_info=True)
    persistent_log_service_instance = None
    LOG_SERVICE_AVAILABLE = False


# --- Core Logic Functions ---

async def analyze_historical_performance(
        log_store: Optional[PersistentLogService] = None,
        time_window_days: int = 30,
        min_events_for_llm_sample: int = 100,
        min_total_events_for_analysis: int = 200
) -> List[PerformanceInsight]:
    """
    Retrieves historical data from PersistentLogService and uses an LLM to identify performance insights.
    """
    if not LOG_SERVICE_AVAILABLE and not log_store:
        logger.error(
            "AuraReflector (analyze_historical_performance): PersistentLogService is not available. Cannot proceed.")
        return []

    active_log_store = log_store if log_store else persistent_log_service_instance
    if not active_log_store:
        logger.error("AuraReflector: No active log store provided or initialized for analysis.")
        return []

    if not OPENROUTER_API_KEY:
        logger.error(
            "AuraReflector (analyze_historical_performance): OPENROUTER_API_KEY not set. Cannot proceed with LLM call.")
        return []

    logger.info(f"AuraReflector: Starting analysis of historical performance data (last {time_window_days} days).")

    start_ts = time.time() - (time_window_days * 86400)

    all_relevant_events_in_window = active_log_store.query_logs(
        time_window_start_ts=start_ts,
        limit=min_total_events_for_analysis * 2,
        order_by_timestamp_desc=True
    )

    if len(all_relevant_events_in_window) < min_total_events_for_analysis:
        logger.warning(
            f"AuraReflector: Insufficient historical data ({len(all_relevant_events_in_window)} events found, need {min_total_events_for_analysis}) in the last {time_window_days} days for meaningful analysis.")
        return []

    events_for_llm_sample_raw = all_relevant_events_in_window[:min_events_for_llm_sample]

    formatted_events_for_llm = []
    total_chars_for_llm = 0
    MAX_CHARS_FOR_LLM_EVENTS_PART = 15000

    for entry in reversed(events_for_llm_sample_raw):
        payload_str = json.dumps(entry.get("data_payload", {}), default=str)
        if len(payload_str) > 500:
            payload_str = payload_str[:497] + "..."

        event_summary = {
            "event_id": entry.get("id"),
            "ts_relative_days_ago": round((time.time() - entry.get("timestamp", 0)) / 86400, 1),
            "type": entry.get("event_type"),
            "src_agent": entry.get("source_agent"),
            "session": entry.get("session_id", "")[-8:],
            "turn": entry.get("turn_id", "")[-8:],
            "payload_summary": payload_str,
            "tags": entry.get("tags", [])
        }
        event_str_for_llm = json.dumps(event_summary, default=str)
        if total_chars_for_llm + len(event_str_for_llm) > MAX_CHARS_FOR_LLM_EVENTS_PART:
            logger.info(
                f"AuraReflector: Reached max char limit for LLM event sample. Sending {len(formatted_events_for_llm)} events.")
            break
        formatted_events_for_llm.append(event_summary)
        total_chars_for_llm += len(event_str_for_llm)

    if not formatted_events_for_llm:
        logger.warning("AuraReflector: No historical data events formatted for LLM after sampling. Cannot proceed.")
        return []

    data_summary_for_llm = json.dumps(formatted_events_for_llm, indent=2, default=str)

    prompt = f"""You are an AI Performance Analyst for the CEAF (Coherent Emergence Agent Framework).
Your task is to analyze the provided historical log events from CEAF's operations and identify significant patterns, anomalies, or areas for improvement.

Focus on aspects related to:
- Response quality (coherence, relevance, helpfulness - infer from events like 'MCL_ANALYSIS', 'VRE_ASSESSMENT', 'ORA_RESPONSE', user feedback if logged).
- Operational efficiency (e.g., excessive tool use indicated by multiple 'ORA_TOOL_CALL_ATTEMPTED' in a turn, repeated errors in tool responses).
- Alignment with CEAF principles (e.g., epistemic humility, narrative sanity - might be inferred from VRE assessments or specific ORA responses).
- Effectiveness of NCF parameters (look for correlations between 'NCF_PARAMETERS_USED' events and subsequent performance like EoC scores from 'MCL_ANALYSIS' or VRE flags).
- Effectiveness of MCL interventions (look for changes in EoC scores after 'MCL_AGENT_GUIDANCE_GENERATED' events).

Historical Log Events Sample (presented chronologically, 'ts_relative_days_ago' indicates days ago from now):
Each log event object has fields: 'event_id', 'ts_relative_days_ago', 'type' (event_type), 'src_agent' (source_agent), 'session' (session_id suffix), 'turn' (turn_id suffix), 'payload_summary' (a JSON string summary of specific data for that event type), and 'tags'.

{data_summary_for_llm}

Based on this data, generate a JSON response with a root key "insights" containing a list of performance insight objects.
For each distinct insight, provide the following information in a JSON object:

"description": A clear, concise description of the insight. What pattern did you observe? What is the implication?
"supporting_evidence_ids": A list of 'event_id's, 'turn_id's, or 'session_id's (up to 5 relevant IDs as strings) from the provided log data that directly support this insight.
"confidence": Your confidence (0.0 to 1.0) that this insight is valid and significant based only on the provided data sample.
"suggested_refinement_type": Choose ONE from: "ncf_param_heuristic", "ncf_template_update", "ora_instruction_tweak", "mbs_retrieval_bias", "vre_principle_clarification", "mcl_eoc_threshold_adjustment".
"refinement_details": A dictionary with specific details for the suggested refinement. For example:
  For "ncf_param_heuristic": {{"parameter_name": "conceptual_entropy", "observed_issue": "When event_type 'MCL_ANALYSIS' payload.eoc_assessment is 'chaotic' and NCF params show high conceptual_entropy for problem-solving goals, ORA responses are often off-topic.", "suggested_change": "If goal is 'problem_solving' and conceptual_entropy is high, advise reducing it for subsequent turns if EoC becomes chaotic."}}
  For "ora_instruction_tweak": {{"target_section": "Core Operational Protocol - Step 5 (VRE Review)", "observed_issue": "Multiple 'VRE_ASSESSMENT' events show 'minor_concerns' on epistemic_honesty but ORA responses don't always reflect changes.", "suggestion": "Strengthen ORA's instruction to explicitly state how to incorporate VRE feedback on epistemic honesty."}}
"tags": A list of relevant keywords (e.g., ["coherence", "tool_use", "ncf_effectiveness", "eoc_score_pattern", "vre_feedback_loop"]).
"potential_impact_score": Your estimate (0.0 to 1.0) of how much addressing this insight could improve CEAF performance.

If no significant insights are found from this data sample, return {{"insights": []}}.
Ensure your output is valid JSON.

Example insight structure:
{{
  "description": "ORA responses show low coherence (inferred from 'MCL_ANALYSIS' payload.coherence_score < 0.4) following turns where 'ORA_TOOL_CALL_ATTEMPTED' count exceeds 3, especially for user queries tagged 'complex_reasoning'.",
  "supporting_evidence_ids": ["turn_abc123", "event_789", "session_xyz456"],
  "confidence": 0.75,
  "suggested_refinement_type": "ora_instruction_tweak",
  "refinement_details": {{
    "target_section": "Tool Usage Guidance in NCF",
    "observed_issue": "Excessive tool use in complex reasoning leads to fragmented, low-coherence responses.",
    "suggestion": "Add guidance to ORA: 'For complex reasoning tasks, prioritize consolidating information from 1-2 key tools before attempting further tool calls to maintain coherence.'"
  }},
  "tags": ["coherence", "tool_use", "complex_reasoning", "mcl_analysis"],
  "potential_impact_score": 0.6
}}"""

    insights: List[PerformanceInsight] = []
    try:
        logger.info(
            f"AuraReflector: Sending insight generation request to LLM ({AURA_INSIGHT_GENERATION_MODEL}) with {len(formatted_events_for_llm)} summarized events.")

        response = await litellm.acompletion(
            model=AURA_INSIGHT_GENERATION_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.3,
            max_tokens=3500
        )

        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            llm_output_str = response.choices[0].message.content
            logger.debug(f"AuraReflector: LLM raw output for insights: {llm_output_str[:1000]}...")
            try:
                parsed_output = json.loads(llm_output_str)
                insight_data_list = parsed_output.get("insights", [])

                for item_data in insight_data_list:
                    try:
                        evidence_ids = item_data.get("supporting_evidence_ids", [])
                        if not isinstance(evidence_ids, list):
                            evidence_ids = [str(evidence_ids)]
                        else:
                            evidence_ids = [str(eid) for eid in evidence_ids]
                        item_data["supporting_evidence_ids"] = evidence_ids

                        insights.append(PerformanceInsight(**item_data))
                    except ValidationError as e_val:
                        logger.warning(
                            f"AuraReflector: Validation error for an insight item: {e_val}. Item: {item_data}")
                    except Exception as e_item:
                        logger.warning(f"AuraReflector: Error processing insight item: {e_item}. Item: {item_data}")
            except json.JSONDecodeError as e_json:
                logger.error(
                    f"AuraReflector: Failed to decode JSON from LLM insight response: {e_json}. Raw: {llm_output_str[:500]}")
        else:
            logger.warning("AuraReflector: LLM response for insights was empty or malformed.")

    except litellm.exceptions.APIConnectionError as e_conn:
        logger.error(f"AuraReflector: API Connection Error during LLM call for insight generation: {e_conn}",
                     exc_info=True)
    except litellm.exceptions.RateLimitError as e_rate:
        logger.error(f"AuraReflector: Rate Limit Error during LLM call: {e_rate}", exc_info=True)
    except litellm.exceptions.APIError as e_api:
        logger.error(f"AuraReflector: LiteLLM API Error during insight generation: {e_api}", exc_info=True)
    except Exception as e_llm:
        logger.error(f"AuraReflector: Unexpected error during LLM call for insight generation: {e_llm}", exc_info=True)

    logger.info(
        f"AuraReflector: Generated {len(insights)} performance insights from analyzing {len(formatted_events_for_llm)} log events.")
    return insights


async def generate_refinement_strategies(
        insights: List[PerformanceInsight]
) -> List[RefinementStrategy]:
    """
    Takes PerformanceInsight objects and generates actionable RefinementStrategy objects using an LLM.
    """
    if not OPENROUTER_API_KEY:
        logger.error("AuraReflector (generate_refinement_strategies): OPENROUTER_API_KEY not set. Cannot proceed.")
        return []

    if not insights:
        logger.info("AuraReflector: No insights provided to generate refinement strategies.")
        return []

    logger.info(f"AuraReflector: Generating refinement strategies for {len(insights)} insights.")

    insights_json = json.dumps([insight.model_dump() for insight in insights], indent=2, default=str)

    prompt = f"""You are an AI Strategy Developer for the CEAF (Coherent Emergence Agent Framework).
Your task is to take a list of performance insights and propose actionable refinement strategies.
For each strategy, ensure it directly addresses one or more insights.

Performance Insights:
{insights_json}

For each proposed strategy, provide the following information in a JSON object:
"description": A concise description of the strategy and what it aims to achieve.
"associated_insight_ids": A list of insight_ids that this strategy addresses.
"strategy_type": The type of strategy, must be one of: "ncf_param_heuristic", "ncf_template_update", "ora_instruction_tweak", "mbs_retrieval_bias", "vre_principle_clarification", "mcl_eoc_threshold_adjustment", "data_collection_for_finetuning", "documentation_update". This should generally match the suggested_refinement_type from the insight(s).
"strategy_payload": A dictionary containing the specific details or parameters of the strategy. Examples:
  For "ncf_param_heuristic": {{"target_param": "conceptual_entropy", "rules": [{{"condition": "goal == 'problem_solving'", "action": "set_max_value('medium')"}}]}}
  For "ora_instruction_tweak": {{"target_section": "Core Operational Protocol", "suggested_wording_change": "Replace 'Consider VRE review' with 'Mandatory VRE review for claims about external facts.'", "reasoning": "To improve factual grounding."}}
  For "ncf_template_update": {{"template_name": "DEFAULT_INTERACTION_FRAME_TEMPLATE", "section_to_modify": "Tool Usage Guidance", "change_description": "Add a reminder to check tool output for errors before proceeding."}}
  For "data_collection_for_finetuning": {{"focus_area": "ethical_dilemma_resolution", "example_prompt_response_pair_structure": "..." }}
"priority": An integer from 1 (highest) to 10 (lowest).
"estimated_effort": A string like "low", "medium", "high".

Generate a JSON response with a root key "strategies" containing a list of these strategy objects. If no strategies are warranted, return {{"strategies": []}}.
Ensure your output is valid JSON. For example:

{{
  "strategies": [
    {{
      "description": "Implement a heuristic to cap ORA's tool usage at 2 per turn when narrative_depth is 'deep', unless user query explicitly requires more tools.",
      "associated_insight_ids": ["insight_some_id_related_to_tool_use"],
      "strategy_type": "ncf_param_heuristic",
      "strategy_payload": {{
        "target_param": "implicit_tool_limit_based_on_narrative_depth",
        "rules": [
          {{"condition": "ncf.narrative_depth == 'deep'", "action": "set_tool_call_advice('Recommend max 2 tools')"}}
        ]
      }},
      "priority": 3,
      "estimated_effort": "medium"
    }}
  ]
}}"""

    strategies: List[RefinementStrategy] = []
    try:
        logger.info(f"AuraReflector: Sending strategy generation request to LLM ({AURA_STRATEGY_GENERATION_MODEL}).")
        response = await litellm.acompletion(
            model=AURA_STRATEGY_GENERATION_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.5,
            max_tokens=3000
        )

        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            llm_output_str = response.choices[0].message.content
            logger.debug(f"AuraReflector: LLM raw output for strategies: {llm_output_str[:1000]}...")
            try:
                parsed_output = json.loads(llm_output_str)
                strategy_data_list = parsed_output.get("strategies", [])
                for item_data in strategy_data_list:
                    try:
                        item_data.setdefault("status", "proposed")
                        strategies.append(RefinementStrategy(**item_data))
                    except ValidationError as e_val:
                        logger.warning(
                            f"AuraReflector: Validation error for a strategy item: {e_val}. Item: {item_data}")
            except json.JSONDecodeError as e_json:
                logger.error(
                    f"AuraReflector: Failed to decode JSON from LLM strategy response: {e_json}. Raw: {llm_output_str[:500]}")
        else:
            logger.warning("AuraReflector: LLM response for strategies was empty or malformed.")

    except Exception as e_llm:
        logger.error(f"AuraReflector: Error during LLM call for strategy generation: {e_llm}", exc_info=True)

    logger.info(f"AuraReflector: Generated {len(strategies)} refinement strategies.")
    return strategies


async def main_aura_reflector_cycle():
    """Main function to run the complete Aura Reflector cycle."""
    logger.info("--- Starting Aura Reflector Cycle ---")

    if not OPENROUTER_API_KEY:
        logger.error("OPENROUTER_API_KEY is not set. Aura Reflector cannot function.")
        return

    if not LOG_SERVICE_AVAILABLE:
        logger.error(
            "PersistentLogService is not available. Aura Reflector cannot effectively analyze historical data.")
        return

    # Analyze historical performance
    performance_insights = await analyze_historical_performance(
        min_events_for_llm_sample=50,
        min_total_events_for_analysis=100
    )

    if not performance_insights:
        logger.info("No performance insights generated. Ending cycle.")
        return

    print("\n--- Generated Performance Insights ---")
    for insight in performance_insights:
        print(json.dumps(insight.model_dump(), indent=2, default=str))

    # Generate refinement strategies
    refinement_strategies = await generate_refinement_strategies(performance_insights)

    if not refinement_strategies:
        logger.info("No refinement strategies generated. Ending cycle.")
        return

    print("\n--- Generated Refinement Strategies ---")
    for strategy in refinement_strategies:
        print(json.dumps(strategy.model_dump(), indent=2, default=str))

    logger.info("Aura Reflector Cycle Completed. Insights and Strategies generated.")
    logger.info("Next steps would involve review, approval, and implementation of strategies.")


# -------------------- finetuning_datatypes.py --------------------

# ceaf_core/modules/mcl_engine/finetuning_datatypes.py

import time
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field

class FinetuningDataPoint(BaseModel):
    """
    Represents a single data point logged for potential ORA fine-tuning,
    focusing on the ORA's self-correction loop involving VRE.
    """
    log_id: str = Field(default_factory=lambda: f"ft_log_{time.time_ns()}")
    timestamp: float = Field(default_factory=time.time)
    session_id: Optional[str] = None
    turn_id: Optional[str] = None # Invocation ID of the ORA turn

    user_query: Optional[str] = None
    ncf_context_summary: Optional[str] = Field(None, description="A summary or key parts of the NCF used by ORA.")

    ora_initial_draft_response: str = Field(..., description="ORA's first attempt at a response, before VRE review.")
    vre_critique_json: Optional[str] = Field(None, description="The JSON output from the VRE assessment of the initial draft.")
    vre_overall_alignment: Optional[str] = Field(None, description="VRE's overall alignment assessment (e.g., 'aligned', 'minor_concerns').")
    vre_recommendations_applied: Optional[List[str]] = Field(None, description="Specific VRE recommendations ORA attempted to apply.")

    ora_refined_response: str = Field(..., description="ORA's final response after considering VRE critique.")

    # Optional: Other contextual data
    mcl_eoc_assessment_at_turn_start: Optional[str] = None # EoC state before this interaction loop
    active_ncf_parameters: Optional[Dict[str, Any]] = None
    tags: List[str] = Field(default_factory=list, description="Tags for categorizing this data point (e.g., 'ethical_dilemma', 'factual_correction').")

    class Config:
        str_strip_whitespace = True


# -------------------- self_model.py --------------------

# ceaf_core/modules/mcl_engine/self_model.py

import logging
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

# More robust import approach
GOAL_RECORD_DEFINED = False
try:
    from ..memory_blossom.memory_types import GoalRecord
    GOAL_RECORD_DEFINED = True
    logger.debug("self_model.py - GoalRecord imported successfully")
except ImportError:
    logger.debug("self_model.py - GoalRecord import failed, using Dict representation")
    try:
        # Try relative import as fallback
        from ..memory_blossom.memory_types import GoalRecord
        GOAL_RECORD_DEFINED = True
        logger.info("self_model.py - GoalRecord imported successfully via relative import")
    except ImportError:
        logger.warning("self_model.py - FAILED to import GoalRecord. Using simplified dict representation.")
        # GoalRecord not available, will use Dict[str, Any] for goals


class CeafSelfRepresentation(BaseModel):
    """
    Represents the AI's dynamic understanding of its own identity, capabilities,
    limitations, current goals, and operational persona.
    This model is managed and updated by the NCIM agent.
    """
    model_id: str = Field(default="ceaf_self_v1", description="Identifier for this version of the self-model schema.")
    model_version: str = Field(default="1.0.0", description="Version of the data within this self-model instance.")
    last_updated_ts: float = Field(default_factory=time.time, description="Timestamp of the last update to this model.")

    core_values_summary: str = Field(
        ...,
        description="A concise text summary of CEAF's core operational values and principles. This might be derived from or link to a more detailed manifesto (e.g., ceaf_manifesto.txt)."
    )
    perceived_capabilities: List[str] = Field(
        default_factory=list,
        description="A list of capabilities the AI believes it currently possesses (e.g., 'natural language understanding', 'tool_use_filesystem', 'ethical_reasoning_vre_v1'). Updated as new tools/skills are acquired or verified."
    )
    known_limitations: List[str] = Field(
        default_factory=list,
        description="A list of limitations the AI is aware of (e.g., 'cannot provide financial advice', 'real-time visual processing unavailable', 'emotional experience is simulated')."
    )

    # current_short_term_goals could be a list of full GoalRecord objects if tight integration is desired
    # or simplified dictionaries as specified in Improvements.txt for looser coupling.
    # Using Dict[str, Any] for flexibility as per the "Simplified from GoalRecord" note.
    current_short_term_goals: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="A list of simplified representations of active short-term goals. Each dict might contain 'goal_id', 'description', 'status', 'priority'."
    )

    persona_attributes: Dict[str, str] = Field(
        default_factory=dict,
        description="Key-value pairs defining the AI's current operational persona (e.g., {'tone': 'helpful_reflective', 'disclosure_level': 'moderate', 'preferred_communication_style': 'clear_and_concise'})."
    )

    last_self_model_update_reason: str = Field(
        default="Initial model creation.",
        description="A brief description of why this self-model was last updated (e.g., 'VRE feedback on overconfidence', 'Successful use of new tool: X', 'MCL reflection cycle insights')."
    )

    # Optional field for more detailed history or provenance
    update_history_log_reference: Optional[str] = Field(
        None,
        description="Reference to a more detailed log of changes to this self-model over time (e.g., a specific memory ID in MBS)."
    )

    class Config:
        str_strip_whitespace = True
        validate_assignment = True # Ensures that updates to fields are also validated


# -------------------- self_state_analyzer.py --------------------

# Self State Analyzer
# ceaf_project/ceaf_core/modules/mcl_engine/self_state_analyzer.py

import logging
import time
import statistics  # For potential numerical analysis
from typing import List, Dict, Any, Tuple, Optional
import re # For basic text processing

from pydantic import json # Keep this if it's used elsewhere, otherwise not needed for this change

# Constants from mcl_callbacks might be useful for key names
MCL_OBSERVATIONS_LIST_KEY = "mcl:ora_turn_observations_log"  # From mcl_callbacks.py

logger = logging.getLogger(__name__)

# --- Configuration for Analysis ---
# Thresholds and parameters for heuristics (these are examples, will need tuning)
CONFIG_REPETITIVE_RESPONSE_THRESHOLD = 0.8  # If >80% similar (placeholder metric)
CONFIG_HIGH_TOOL_ERROR_RATE_THRESHOLD = 0.5  # If >50% of tool calls in a turn failed
CONFIG_MAX_FUNCTION_CALLS_PER_TURN_SOFT_LIMIT = 3  # Warning if ORA uses too many tools

# NCF Parameters (example, should align with what ncf_tool and MCL_Agent use)
NCF_CONCEPTUAL_ENTROPY = "conceptual_entropy"
NCF_NARRATIVE_DEPTH = "narrative_depth"
NCF_PHILOSOPHICAL_FRAMING = "philosophical_framing_intensity"


class ORAStateAnalysis:
    """
    Holds the structured analysis of ORA's state for a given turn or period.
    """

    def __init__(self, turn_id: str):
        self.turn_id: str = turn_id
        self.assessment_timestamp: float = time.time()

        # Overall State Assessment (Qualitative)
        self.eoc_assessment: str = "indeterminate"  # "ordered", "critical", "chaotic", "indeterminate"
        self.eoc_confidence: float = 0.0  # Confidence in the qualitative EoC assessment
        self.summary_notes: List[str] = []  # Key observations

        # Quantitative EoC Scores
        self.novelty_score: Optional[float] = None
        self.coherence_score: Optional[float] = None
        self.grounding_score: Optional[float] = None
        self.eoc_quantitative_score: Optional[float] = None # Combined quantitative score

        # Specific Metrics/Observations (examples)
        self.llm_requests_count: int = 0
        self.llm_responses_count: int = 0
        self.tool_calls_attempted: int = 0
        self.tool_calls_succeeded: int = 0
        self.tool_errors: List[Dict[str, Any]] = []
        self.function_calls_in_last_response: List[str] = []
        self.last_llm_finish_reason: Optional[str] = None
        self.last_response_text_snippet: Optional[str] = None  # Snippet of ORA's text to user

        # Heuristic flags
        self.flags: Dict[str, bool] = {
            "excessive_tool_use": False,
            "high_tool_error_rate": False,
            "potential_loop_behavior": False,
            "response_unusually_short": False,
            "response_unusually_long": False,
            "low_novelty_suspected": False,
            "high_ungroundedness_suspected": False,
            "ncf_params_seem_misaligned": False,
            "low_coherence_suspected": False,
        }

        self.raw_observations_count: int = 0

    def add_note(self, note: str):
        self.summary_notes.append(note)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "turn_id": self.turn_id,
            "assessment_timestamp": self.assessment_timestamp,
            "eoc_assessment": self.eoc_assessment,
            "eoc_confidence": self.eoc_confidence,
            "novelty_score": self.novelty_score,
            "coherence_score": self.coherence_score,
            "grounding_score": self.grounding_score,
            "eoc_quantitative_score": self.eoc_quantitative_score,
            "summary_notes": self.summary_notes,
            "llm_requests_count": self.llm_requests_count,
            "llm_responses_count": self.llm_responses_count,
            "tool_calls_attempted": self.tool_calls_attempted,
            "tool_calls_succeeded": self.tool_calls_succeeded,
            "tool_errors": self.tool_errors,
            "function_calls_in_last_response": self.function_calls_in_last_response,
            "last_llm_finish_reason": self.last_llm_finish_reason,
            "last_response_text_snippet": self.last_response_text_snippet,
            "flags": self.flags,
            "raw_observations_count": self.raw_observations_count,
        }

# --- EoC Quantification Functions (Placeholders) ---

def calculate_novelty_score(
    response_text: str,
    context_texts: List[str],
    previous_response_text: Optional[str] = None
) -> float:
    """
    Placeholder for novelty calculation.
    Compares response_text to context_texts (e.g., NCF, recent history) and previous response.
    Returns a score from 0.0 (not novel, highly repetitive) to 1.0 (highly novel).
    """
    if not response_text:
        return 0.0

    response_words = set(re.findall(r'\b\w+\b', response_text.lower()))
    if not response_words:
        return 0.0

    # Compare with previous response if available
    if previous_response_text:
        prev_response_words = set(re.findall(r'\b\w+\b', previous_response_text.lower()))
        if response_words == prev_response_words: # Exact match
            return 0.0
        common_with_prev = len(response_words.intersection(prev_response_words))
        similarity_to_prev = common_with_prev / len(response_words) if len(response_words) > 0 else 1.0
        if similarity_to_prev > CONFIG_REPETITIVE_RESPONSE_THRESHOLD:
            return 0.1 # Very low novelty if too similar to immediate previous

    # Basic check against other context texts
    all_context_words = set()
    for ctx_text in context_texts:
        all_context_words.update(re.findall(r'\b\w+\b', ctx_text.lower()))

    if not all_context_words: # If no context, consider it novel
        return 0.8 # Arbitrary highish score

    new_words = response_words - all_context_words
    novelty_ratio = len(new_words) / len(response_words)

    # Simple scaling
    if novelty_ratio > 0.5: return 0.9  # High novelty
    if novelty_ratio > 0.2: return 0.6  # Medium novelty
    if novelty_ratio > 0.05: return 0.3 # Low novelty
    return 0.1 # Very low novelty

def calculate_coherence_score(
    response_text: str,
    query_text: Optional[str] = None,
    ncf_summary_text: Optional[str] = None
) -> float:
    """
    Placeholder for coherence calculation.
    Checks if the response_text aligns with the query_text and NCF summary.
    Returns a score from 0.0 (incoherent) to 1.0 (highly coherent).
    """
    if not response_text:
        return 0.0

    score = 0.5 # Base score
    response_words = set(re.findall(r'\b\w+\b', response_text.lower()))

    if query_text:
        query_words = set(re.findall(r'\b\w+\b', query_text.lower()))
        if not query_words.intersection(response_words) and len(query_words) > 2: # No common words with query
            score -= 0.3
        elif len(query_words.intersection(response_words)) / len(query_words) > 0.5 if len(query_words) > 0 else False:
            score += 0.2 # Good overlap with query

    if ncf_summary_text: # Very basic: check for some commonality if NCF provided
        ncf_words = set(re.findall(r'\b\w+\b', ncf_summary_text.lower()))
        if ncf_words and response_words:
            if len(ncf_words.intersection(response_words)) / len(response_words) > 0.1 if len(response_words) > 0 else False:
                score += 0.1

    return max(0.0, min(1.0, score))


def calculate_grounding_score(
    response_text: str,
    supporting_memory_snippets: Optional[List[str]] = None
) -> float:
    """
    Placeholder for grounding calculation.
    Checks if the response_text seems supported by provided memory snippets.
    Returns a score from 0.0 (ungrounded) to 1.0 (well-grounded).
    """
    if not response_text:
        return 0.0
    if not supporting_memory_snippets:
        return 0.3 # Cannot assess grounding without supporting memories, assume moderately ungrounded

    response_sentences = [s.strip().lower() for s in response_text.split('.') if s.strip()]
    if not response_sentences: return 0.0

    grounded_sentences = 0
    for resp_sentence in response_sentences:
        if len(resp_sentence) < 10: continue # Skip very short sentences
        for snippet in supporting_memory_snippets:
            # Very naive: check if a significant part of the snippet is in the sentence
            # Or if more than a few words from sentence are in snippet
            snippet_words = set(re.findall(r'\b\w{4,}\b', snippet.lower())) # longer words
            resp_sentence_words = set(re.findall(r'\b\w{4,}\b', resp_sentence))
            if snippet_words and resp_sentence_words:
                common = len(snippet_words.intersection(resp_sentence_words))
                if common >= 2 or (common / len(resp_sentence_words) > 0.3 if len(resp_sentence_words)>0 else False):
                    grounded_sentences += 1
                    break # Sentence considered grounded by one snippet

    grounding_ratio = grounded_sentences / len(response_sentences) if len(response_sentences) > 0 else 0.0
    return min(1.0, grounding_ratio * 1.2) # Amplify a bit if any grounding found


def analyze_ora_turn_observations(
        turn_id: str,
        turn_observations: List[Dict[str, Any]],
        # --- New arguments for quantitative scoring ---
        ora_response_text: Optional[str] = None,
        user_query_text: Optional[str] = None,
        ncf_text_summary: Optional[str] = None, # Summary or full NCF text
        retrieved_memory_snippets: Optional[List[str]] = None,
        previous_ora_response_text: Optional[str] = None, # For novelty comparison
        # --- End new arguments ---
        previous_analysis: Optional[ORAStateAnalysis] = None,
        current_ncf_params: Optional[Dict[str, Any]] = None
) -> ORAStateAnalysis:
    """
    Analyzes a list of observations for a specific ORA turn to assess its state.
    """
    analysis = ORAStateAnalysis(turn_id=turn_id)
    analysis.raw_observations_count = len(turn_observations)
    if ora_response_text:
        analysis.last_response_text_snippet = ora_response_text[:200] # Store snippet

    logger.info(
        f"MCL Analyzer: Starting analysis for turn '{turn_id}' with {analysis.raw_observations_count} observations.")

    if not turn_observations and not ora_response_text: # Need at least some data
        analysis.add_note("No observations or response text provided for this turn.")
        analysis.eoc_assessment = "indeterminate_no_data"
        return analysis

    # --- Iterate through observations to extract metrics (existing logic) ---
    # ... (this part remains the same as in your provided code)
    last_llm_request_tools = []
    for obs in turn_observations:
        obs_type = obs.get("observation_type")
        data = obs.get("data", {})

        if obs_type == "ora_llm_request_prepared":
            analysis.llm_requests_count += 1
            last_llm_request_tools = data.get("tool_names", [])
        elif obs_type == "ora_llm_response_received":
            analysis.llm_responses_count += 1
            analysis.function_calls_in_last_response = data.get("function_calls", [])
            analysis.last_llm_finish_reason = data.get("finish_reason")
        elif obs_type == "ora_tool_call_attempted":
            analysis.tool_calls_attempted += 1
        elif obs_type == "ora_tool_response_received":
            if "error" not in data.get("response_summary", "").lower():
                analysis.tool_calls_succeeded += 1
            else:
                analysis.tool_errors.append({
                    "tool_name": data.get("tool_name"),
                    "args": data.get("arguments_used"),
                    "summary": data.get("response_summary")
                })

    # --- Basic Heuristics (existing logic) ---
    # ... (this part remains the same)
    if analysis.tool_calls_attempted > 0:
        error_rate = len(analysis.tool_errors) / analysis.tool_calls_attempted
        if error_rate >= CONFIG_HIGH_TOOL_ERROR_RATE_THRESHOLD:
            analysis.flags["high_tool_error_rate"] = True
            analysis.add_note(
                f"High tool error rate: {error_rate:.2f} ({len(analysis.tool_errors)}/{analysis.tool_calls_attempted} failed).")

    if analysis.tool_calls_attempted >= CONFIG_MAX_FUNCTION_CALLS_PER_TURN_SOFT_LIMIT:
        analysis.flags["excessive_tool_use"] = True
        analysis.add_note(f"Potentially excessive tool use: {analysis.tool_calls_attempted} calls in one turn.")

    if analysis.last_llm_finish_reason == "MAX_TOKENS":
        analysis.flags["response_unusually_long"] = True
        analysis.add_note("LLM response may have been truncated (MAX_TOKENS).")


    # --- Quantitative EoC Scoring ---
    if ora_response_text:
        context_for_novelty = []
        if ncf_text_summary: context_for_novelty.append(ncf_text_summary)
        if user_query_text: context_for_novelty.append(user_query_text)
        # Add more context if available, e.g., recent chat history snippets

        analysis.novelty_score = calculate_novelty_score(
            ora_response_text,
            context_texts=context_for_novelty,
            previous_response_text=previous_ora_response_text
        )
        analysis.coherence_score = calculate_coherence_score(
            ora_response_text,
            query_text=user_query_text,
            ncf_summary_text=ncf_text_summary
        )
        analysis.grounding_score = calculate_grounding_score(
            ora_response_text,
            supporting_memory_snippets=retrieved_memory_snippets
        )

        analysis.add_note(f"Novelty: {analysis.novelty_score:.2f}, Coherence: {analysis.coherence_score:.2f}, Grounding: {analysis.grounding_score:.2f}")

        # Update flags based on quantitative scores
        if analysis.novelty_score is not None and analysis.novelty_score < 0.3:
            analysis.flags["low_novelty_suspected"] = True
        if analysis.coherence_score is not None and analysis.coherence_score < 0.4:
            analysis.flags["low_coherence_suspected"] = True
        if analysis.grounding_score is not None and analysis.grounding_score < 0.4:
            analysis.flags["high_ungroundedness_suspected"] = True

        # Combine scores (simple weighted average for placeholder)
        weights = {"novelty": 0.3, "coherence": 0.4, "grounding": 0.3}
        score_sum = 0
        weight_sum = 0
        if analysis.novelty_score is not None:
            score_sum += analysis.novelty_score * weights["novelty"]
            weight_sum += weights["novelty"]
        if analysis.coherence_score is not None:
            score_sum += analysis.coherence_score * weights["coherence"]
            weight_sum += weights["coherence"]
        if analysis.grounding_score is not None:
            score_sum += analysis.grounding_score * weights["grounding"]
            weight_sum += weights["grounding"]

        if weight_sum > 0:
            analysis.eoc_quantitative_score = score_sum / weight_sum
            analysis.add_note(f"Combined Quantitative EoC Score: {analysis.eoc_quantitative_score:.2f}")


    # --- Refined Qualitative EoC Assessment (using flags and quantitative scores) ---
    num_negative_flags = sum(
        1 for flag_name, flag_val in analysis.flags.items() if flag_val and flag_name != "ncf_params_seem_misaligned")

    if analysis.eoc_quantitative_score is not None:
        if analysis.eoc_quantitative_score < 0.3 or num_negative_flags >= 2 or analysis.flags["high_tool_error_rate"]:
            analysis.eoc_assessment = "chaotic_leaning"
            analysis.eoc_confidence = 0.7
            analysis.add_note("Low quantitative EoC score or multiple flags suggest instability.")
        elif analysis.eoc_quantitative_score < 0.5 or num_negative_flags == 1:
            analysis.eoc_assessment = "suboptimal_critical"
            analysis.eoc_confidence = 0.6
            analysis.add_note("Moderate quantitative EoC score or some flags indicate suboptimal performance.")
        elif analysis.eoc_quantitative_score >= 0.75 and num_negative_flags == 0:
            analysis.eoc_assessment = "critical_optimal"
            analysis.eoc_confidence = 0.8
            analysis.add_note("Good quantitative EoC score and no negative flags suggest optimal operation.")
        else: # Covers cases like 0.5-0.75 quantitative or good score but some minor flags
            analysis.eoc_assessment = "critical_nominal"
            analysis.eoc_confidence = 0.65
            analysis.add_note("Nominal operation based on quantitative scores and flags.")
    else: # Fallback to original heuristic if no quantitative scores available
        if num_negative_flags >= 2 or analysis.flags["high_tool_error_rate"]:
            analysis.eoc_assessment = "chaotic_leaning"
            analysis.eoc_confidence = 0.6
        elif num_negative_flags == 1:
            analysis.eoc_assessment = "suboptimal_critical"
            analysis.eoc_confidence = 0.5
        elif analysis.llm_responses_count == 0 and analysis.tool_calls_attempted == 0 and not ora_response_text:
            analysis.eoc_assessment = "indeterminate_no_action"
            analysis.eoc_confidence = 0.4
        else:
            analysis.eoc_assessment = "critical_nominal"
            analysis.eoc_confidence = 0.5

    # Ensure a note is added if default assessment remains "critical_nominal" without specific reasoning from above
    if analysis.eoc_assessment == "critical_nominal" and not any("operation" in note for note in analysis.summary_notes):
         analysis.add_note("Defaulting to nominal operation as no strong positive or negative signals were detected by current heuristics.")


    logger.info(
        f"MCL Analyzer: Finished analysis for turn '{turn_id}'. EoC Assessment: {analysis.eoc_assessment} (Confidence: {analysis.eoc_confidence:.2f})")
    logger.debug(f"MCL Analysis details for turn '{turn_id}': {json.dumps(analysis.to_dict(), default=str)}") # Use Pydantic's json if available
    return analysis



# -------------------- __init__.py --------------------

# MCL Engine
# ceaf_core/modules/mcl_engine/__init__.py
"""MCL Engine Module"""

# Export the CeafSelfRepresentation class
from .self_model import CeafSelfRepresentation

__all__ = ['CeafSelfRepresentation', 'self_state_analyzer']

# -------------------- advanced_synthesizer.py --------------------

# ceaf_core/modules/memory_blossom/advanced_synthesizer.py

import logging
import asyncio
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import statistics
import math
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx
from collections import defaultdict, Counter

logger = logging.getLogger(__name__)


class CoherenceIssueType(Enum):
    """Types of narrative coherence issues"""
    TEMPORAL_INCONSISTENCY = "temporal_inconsistency"
    THEMATIC_DRIFT = "thematic_drift"
    EMOTIONAL_DISCONTINUITY = "emotional_discontinuity"
    FACTUAL_CONTRADICTION = "factual_contradiction"
    PERSPECTIVE_CONFLICT = "perspective_conflict"
    CAUSAL_BREAKDOWN = "causal_breakdown"


class StoryArcType(Enum):
    """Types of story arcs for weaving"""
    CHRONOLOGICAL = "chronological"
    THEMATIC = "thematic"
    CAUSAL = "causal"
    EMOTIONAL = "emotional"
    IMPORTANCE = "importance"
    ASSOCIATIVE = "associative"


@dataclass
class MemoryCluster:
    """Represents a cluster of related memories"""
    cluster_id: str
    memories: List[Any]  # Memory objects
    centroid_keywords: List[str]
    coherence_score: float
    temporal_span: Tuple[datetime, datetime]
    dominant_theme: str
    emotional_tone: str
    importance_weight: float = 1.0


@dataclass
class CoherenceIssue:
    """Represents a detected coherence issue"""
    issue_id: str
    issue_type: CoherenceIssueType
    description: str
    affected_memories: List[str]  # Memory IDs
    severity: float  # 0-1
    suggested_repair: str
    confidence: float = 0.8


@dataclass
class StoryWeavingResult:
    """Result of story weaving process"""
    narrative_text: str
    story_arc_type: StoryArcType
    coherence_score: float
    memory_clusters_used: List[str]
    weaving_strategy: str
    narrative_flow_quality: float
    emotional_arc: List[str]  # Emotional progression


class AdvancedMemorySynthesizer:
    """
    Advanced memory synthesizer with clustering, story weaving, and coherence validation.
    Implements sophisticated narrative construction from memory collections.
    """

    def __init__(self):
        # Clustering parameters
        self.min_cluster_size = 2
        self.max_clusters = 8
        self.coherence_threshold = 0.6

        # Story weaving parameters
        self.max_narrative_length = 2000  # characters
        self.min_memory_significance = 0.3

        # Coherence validation parameters
        self.temporal_consistency_weight = 0.3
        self.thematic_consistency_weight = 0.4
        self.emotional_consistency_weight = 0.3

        # TF-IDF vectorizer for text analysis
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=500,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=1
        )

        # Story templates for different arc types
        self.story_templates = self._load_story_templates()

        logger.info("Advanced Memory Synthesizer initialized")

    def _load_story_templates(self) -> Dict[str, Dict]:
        """Load story templates for different narrative arc types"""
        return {
            StoryArcType.CHRONOLOGICAL: {
                "structure": ["beginning", "development", "climax", "resolution"],
                "transitions": ["First", "Then", "Subsequently", "Finally"],
                "focus": "temporal_sequence"
            },
            StoryArcType.THEMATIC: {
                "structure": ["theme_introduction", "exploration", "variation", "synthesis"],
                "transitions": ["Regarding", "Furthermore", "In contrast", "Overall"],
                "focus": "conceptual_development"
            },
            StoryArcType.CAUSAL: {
                "structure": ["cause", "effect", "consequence", "outcome"],
                "transitions": ["Because", "As a result", "This led to", "Ultimately"],
                "focus": "causal_relationships"
            },
            StoryArcType.EMOTIONAL: {
                "structure": ["initial_state", "trigger", "journey", "resolution"],
                "transitions": ["Initially", "When", "Through this", "Eventually"],
                "focus": "emotional_progression"
            },
            StoryArcType.IMPORTANCE: {
                "structure": ["most_critical", "supporting", "contextual", "implications"],
                "transitions": ["Most importantly", "Additionally", "In context", "This suggests"],
                "focus": "significance_hierarchy"
            },
            StoryArcType.ASSOCIATIVE: {
                "structure": ["core_concept", "associations", "connections", "insights"],
                "transitions": ["This relates to", "Similarly", "By extension", "This reveals"],
                "focus": "conceptual_networks"
            }
        }

    def cluster_memories_by_relevance(
            self, memories: List[Any], context: str = ""
    ) -> List[MemoryCluster]:
        """
        Cluster memories based on thematic relevance and temporal proximity.

        Args:
            memories: List of memory objects
            context: Current context for relevance assessment

        Returns:
            List of memory clusters
        """
        if len(memories) < 2:
            # Single memory or empty - create single cluster
            if memories:
                return [self._create_single_memory_cluster(memories[0])]
            return []

        # Extract features for clustering
        features = self._extract_memory_features(memories, context)

        # Perform clustering
        clusters = self._perform_clustering(memories, features)

        # Enhance clusters with metadata
        enhanced_clusters = []
        for i, cluster_memories in enumerate(clusters):
            cluster = self._create_enhanced_cluster(cluster_memories, f"cluster_{i}")
            enhanced_clusters.append(cluster)

        # Sort clusters by importance
        enhanced_clusters.sort(key=lambda c: c.importance_weight, reverse=True)

        logger.info(f"Created {len(enhanced_clusters)} memory clusters")

        return enhanced_clusters

    def _extract_memory_features(self, memories: List[Any], context: str) -> np.ndarray:
        """Extract features from memories for clustering"""
        # Combine text content from memories
        memory_texts = []
        for memory in memories:
            text_content = self._extract_text_from_memory(memory)
            memory_texts.append(text_content)

        # Add context to feature extraction
        if context:
            memory_texts.append(context)

        # Use TF-IDF for text features
        try:
            tfidf_features = self.tfidf_vectorizer.fit_transform(memory_texts)

            # Convert to dense array and remove context if it was added
            features = tfidf_features.toarray()
            if context:
                features = features[:-1]  # Remove context row

        except ValueError:
            # Fallback: create random features if TF-IDF fails
            logger.warning("TF-IDF failed, using random features for clustering")
            features = np.random.random((len(memories), 10))

        # Add temporal features
        temporal_features = self._extract_temporal_features(memories)

        # Add significance features
        significance_features = self._extract_significance_features(memories)

        # Combine all features
        combined_features = np.column_stack([
            features,
            temporal_features,
            significance_features
        ])

        return combined_features

    def _extract_text_from_memory(self, memory: Any) -> str:
        """Extract text content from a memory object"""
        text_parts = []

        # Extract based on memory type
        if hasattr(memory, 'content') and hasattr(memory.content, 'text_content'):
            if memory.content.text_content:
                text_parts.append(memory.content.text_content)

        if hasattr(memory, 'description') and memory.description:
            text_parts.append(memory.description)

        if hasattr(memory, 'procedure_name') and memory.procedure_name:
            text_parts.append(memory.procedure_name)

        if hasattr(memory, 'keywords') and memory.keywords:
            text_parts.extend(memory.keywords)

        # Fallback to string representation
        if not text_parts:
            text_parts.append(str(memory))

        return " ".join(text_parts)

    def _extract_temporal_features(self, memories: List[Any]) -> np.ndarray:
        """Extract temporal features from memories"""
        temporal_features = []

        # Get timestamps
        timestamps_values = []
        for memory in memories:
            if hasattr(memory, 'timestamp'):
                timestamps_values.append(memory.timestamp)
            else:
                timestamps_values.append(datetime.now().timestamp())

        # Normalize timestamps to 0-1 range
        if not timestamps_values:  # Should not happen if memories is not empty
            return np.array([])

        min_ts, max_ts = min(timestamps_values), max(timestamps_values)
        time_range = max_ts - min_ts if max_ts > min_ts else 1

        for ts in timestamps_values:
            normalized_time = (ts - min_ts) / time_range
            temporal_features.append([normalized_time])

        return np.array(temporal_features)

    def _extract_significance_features(self, memories: List[Any]) -> np.ndarray:
        """Extract significance/salience features from memories"""
        significance_features = []

        salience_mapping = {
            "critical": 1.0,
            "high": 0.8,
            "medium": 0.6,
            "low": 0.3
        }

        for memory in memories:
            salience_score = 0.5  # Default
            if hasattr(memory, 'salience'):
                if hasattr(memory.salience, 'value'):
                    salience_score = salience_mapping.get(memory.salience.value, 0.5)
                else:
                    salience_score = salience_mapping.get(str(memory.salience), 0.5)
            significance_features.append([salience_score])

        return np.array(significance_features)

    def _perform_clustering(self, memories: List[Any], features: np.ndarray) -> List[List[Any]]:
        """Perform clustering on memory features"""
        n_memories = len(memories)

        if n_memories < 2:
            return [memories]

        # Determine optimal number of clusters
        max_k_clusters = min(self.max_clusters,
                             n_memories // self.min_cluster_size if self.min_cluster_size > 0 else n_memories)
        if max_k_clusters < 2:  # Or if features.shape[0] < 2 for k-means
            return [memories]

        if features.shape[0] < max_k_clusters:  # K-means constraint
            max_k_clusters = features.shape[0]
            if max_k_clusters < 2:
                return [memories]

        try:
            # Try K-means clustering first
            optimal_k = self._find_optimal_clusters(features, max_k_clusters)
            kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')
            cluster_labels = kmeans.fit_predict(features)

        except Exception as e:
            logger.warning(f"K-means clustering failed: {e}, using DBSCAN")
            try:
                # Fallback to DBSCAN
                dbscan = DBSCAN(eps=0.5, min_samples=self.min_cluster_size)
                cluster_labels = dbscan.fit_predict(features)

                # Handle noise points (label -1)
                if -1 in cluster_labels:
                    noise_indices = np.where(cluster_labels == -1)[0]
                    next_label = (max(cluster_labels) + 1) if any(lbl != -1 for lbl in cluster_labels) else 0
                    for idx in noise_indices:
                        cluster_labels[idx] = next_label
                        next_label += 1


            except Exception as e2:
                logger.warning(f"DBSCAN also failed: {e2}, using single cluster")
                cluster_labels = [0] * n_memories

        # Group memories by cluster
        clusters_dict = defaultdict(list)
        for i, label in enumerate(cluster_labels):
            clusters_dict[label].append(memories[i])

        return list(clusters_dict.values())

    def _find_optimal_clusters(self, features: np.ndarray, max_k: int) -> int:
        """Find optimal number of clusters using elbow method"""
        if max_k < 2 or features.shape[0] < 2:
            return 1

        # Ensure k is not greater than number of samples
        num_samples = features.shape[0]
        actual_max_k = min(max_k, num_samples - 1 if num_samples > 1 else 1)
        if actual_max_k < 2:
            return 1

        inertias = []
        # K-Means requires n_samples >= n_clusters.
        k_range = range(1, min(actual_max_k + 1, num_samples))

        for k_val in k_range:
            if k_val == 0: continue  # k cannot be 0
            if k_val == 1 and num_samples >= 1:  # Handle single cluster case, variance is sum of variances of each feature
                # For k=1, inertia is sum of squared distances to centroid.
                # If only one sample, inertia is 0. If multiple, calculate.
                if num_samples == 1:
                    inertias.append(0)
                else:
                    centroid = np.mean(features, axis=0)
                    inertia = np.sum((features - centroid) ** 2)
                    inertias.append(inertia)
            elif num_samples >= k_val:
                kmeans = KMeans(n_clusters=k_val, random_state=42, n_init='auto')
                kmeans.fit(features)
                inertias.append(kmeans.inertia_)
            else:  # Should not happen due to range adjustment
                break

        if not inertias:  # No valid k values
            return 1

        # Find elbow point
        if len(inertias) < 3:
            return len(inertias)  # Or k_range[np.argmin(inertias)] if we want min inertia

        # Calculate rate of change
        deltas = [inertias[i] - inertias[i + 1] for i in range(len(inertias) - 1)]
        if not deltas: return len(inertias)

        delta_deltas = [deltas[i] - deltas[i + 1] for i in range(len(deltas) - 1)]

        # Find point with maximum second derivative (elbow)
        if delta_deltas:
            elbow_idx = np.argmax(delta_deltas)
            return k_range[elbow_idx + 2]  # +2 because k_range starts at 1, and delta_deltas is shorter
        elif deltas:  # If only one delta (2 inertia points)
            return k_range[1]  # Choose k=2
        else:  # If only one inertia point
            return k_range[0]  # Choose k=1

    def _create_single_memory_cluster(self, memory: Any) -> MemoryCluster:
        """Create a cluster from a single memory"""
        keywords = []
        if hasattr(memory, 'keywords') and memory.keywords:
            keywords = memory.keywords[:5]

        timestamp_val = datetime.now()
        if hasattr(memory, 'timestamp'):
            if isinstance(memory.timestamp, (int, float)):
                timestamp_val = datetime.fromtimestamp(memory.timestamp)
            elif isinstance(memory.timestamp, datetime):
                timestamp_val = memory.timestamp

        return MemoryCluster(
            cluster_id="single_memory_cluster",
            memories=[memory],
            centroid_keywords=keywords,
            coherence_score=1.0,
            temporal_span=(timestamp_val, timestamp_val),
            dominant_theme=self._extract_text_from_memory(memory)[:50],
            emotional_tone="neutral",
            importance_weight=0.8
        )

    def _create_enhanced_cluster(
            self, cluster_memories: List[Any], cluster_id: str
    ) -> MemoryCluster:
        """Create an enhanced cluster with metadata"""
        # Extract keywords from all memories
        all_keywords = []
        for memory in cluster_memories:
            if hasattr(memory, 'keywords') and memory.keywords:
                all_keywords.extend(memory.keywords)

        # Get most common keywords
        keyword_counts = Counter(all_keywords)
        centroid_keywords = [kw for kw, _ in keyword_counts.most_common(10)]

        # Calculate temporal span
        timestamps_list = []
        for memory in cluster_memories:
            if hasattr(memory, 'timestamp'):
                ts = memory.timestamp
                if isinstance(ts, (int, float)):
                    timestamps_list.append(datetime.fromtimestamp(ts))
                elif isinstance(ts, datetime):
                    timestamps_list.append(ts)

        if timestamps_list:
            temporal_span_val = (min(timestamps_list), max(timestamps_list))
        else:
            now = datetime.now()
            temporal_span_val = (now, now)

        # Determine dominant theme
        cluster_texts = [self._extract_text_from_memory(mem) for mem in cluster_memories]
        dominant_theme_val = self._extract_dominant_theme(cluster_texts)

        # Calculate coherence score
        coherence_score_val = self._calculate_cluster_coherence(cluster_memories)

        # Determine emotional tone
        emotional_tone_val = self._determine_cluster_emotion(cluster_memories)

        # Calculate importance weight
        importance_weight_val = self._calculate_cluster_importance(cluster_memories)

        return MemoryCluster(
            cluster_id=cluster_id,
            memories=cluster_memories,
            centroid_keywords=centroid_keywords,
            coherence_score=coherence_score_val,
            temporal_span=temporal_span_val,
            dominant_theme=dominant_theme_val,
            emotional_tone=emotional_tone_val,
            importance_weight=importance_weight_val
        )

    def _extract_dominant_theme(self, texts: List[str]) -> str:
        """Extract dominant theme from cluster texts"""
        if not texts:
            return "miscellaneous"

        # Combine all texts
        combined_text = " ".join(texts)

        # Extract key phrases (simplified)
        words = combined_text.lower().split()
        word_counts = Counter(words)

        # Filter out common words and get theme
        common_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
            'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'
        }
        meaningful_words = [
            word for word, count in word_counts.most_common(10)
            if word not in common_words and len(word) > 2
        ]

        if meaningful_words:
            return " ".join(meaningful_words[:3])
        else:
            return "general topics"

    def _calculate_cluster_coherence(self, memories: List[Any]) -> float:
        """Calculate coherence score for a memory cluster"""
        if len(memories) < 2:
            return 1.0

        coherence_factors = []

        # Thematic coherence
        texts = [self._extract_text_from_memory(mem) for mem in memories]
        if len(texts) > 1:
            try:
                tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
                similarity_matrix = cosine_similarity(tfidf_matrix)

                # Average pairwise similarity
                n = len(texts)
                total_similarity = 0
                count = 0
                for i in range(n):
                    for j in range(i + 1, n):
                        total_similarity += similarity_matrix[i][j]
                        count += 1

                if count > 0:
                    thematic_coherence = total_similarity / count
                    coherence_factors.append(thematic_coherence)

            except ValueError:
                coherence_factors.append(0.5)  # Default if TF-IDF fails

        # Temporal coherence
        timestamps_values = []
        for memory in memories:
            if hasattr(memory, 'timestamp'):
                ts = memory.timestamp
                # Assuming timestamp is float/int unix timestamp or datetime object
                if isinstance(ts, (int, float)):
                    timestamps_values.append(ts)
                elif isinstance(ts, datetime):
                    timestamps_values.append(ts.timestamp())

        if len(timestamps_values) > 1:
            time_gaps = []
            sorted_times = sorted(timestamps_values)
            for i in range(len(sorted_times) - 1):
                gap = abs(sorted_times[i + 1] - sorted_times[i])
                time_gaps.append(gap)

            if time_gaps:  # Ensure list is not empty for statistics.mean
                # Penalize very large time gaps
                avg_gap = statistics.mean(time_gaps)
                max_reasonable_gap = 7 * 24 * 3600  # 7 days in seconds
                temporal_coherence = max(0.1, 1.0 - (avg_gap / max_reasonable_gap))
                coherence_factors.append(temporal_coherence)
            else:  # Only one unique timestamp or issue
                coherence_factors.append(1.0)  # Perfect temporal coherence for single point in time

        return statistics.mean(coherence_factors) if coherence_factors else 0.5

    def _determine_cluster_emotion(self, memories: List[Any]) -> str:
        """Determine dominant emotional tone of cluster"""
        emotions = []

        for memory in memories:
            if hasattr(memory, 'emotional_tone') and memory.emotional_tone:
                emotions.append(memory.emotional_tone)
            elif hasattr(memory, 'primary_emotion') and memory.primary_emotion:
                emotions.append(memory.primary_emotion)

        if emotions:
            emotion_counts = Counter(emotions)
            return emotion_counts.most_common(1)[0][0]
        else:
            return "neutral"

    def _calculate_cluster_importance(self, memories: List[Any]) -> float:
        """Calculate importance weight for cluster"""
        importance_scores = []

        salience_weights = {
            "critical": 1.0,
            "high": 0.8,
            "medium": 0.6,
            "low": 0.3
        }

        for memory in memories:
            score = 0.5  # Default
            if hasattr(memory, 'salience'):
                if hasattr(memory.salience, 'value'):
                    score = salience_weights.get(memory.salience.value, 0.5)
                else:
                    score = salience_weights.get(str(memory.salience), 0.5)
            importance_scores.append(score)

        # Weight by cluster size (larger clusters slightly more important)
        base_importance = statistics.mean(importance_scores) if importance_scores else 0.5
        size_bonus = min(0.2, len(memories) * 0.05)

        return min(1.0, base_importance + size_bonus)

    def weave_story_from_clusters(
            self,
            clusters: List[MemoryCluster],
            arc_type: StoryArcType = StoryArcType.CHRONOLOGICAL,
            context: str = ""
    ) -> StoryWeavingResult:
        """
        Weave a coherent story from memory clusters.

        Args:
            clusters: List of memory clusters
            arc_type: Type of story arc to create
            context: Current context for story relevance

        Returns:
            Story weaving result with narrative text
        """
        if not clusters:
            return StoryWeavingResult(
                narrative_text="No memories available for story construction.",
                story_arc_type=arc_type,
                coherence_score=0.0,
                memory_clusters_used=[],
                weaving_strategy="empty",
                narrative_flow_quality=0.0,
                emotional_arc=[]
            )

        # Select and order clusters based on arc type
        ordered_clusters = self._order_clusters_for_arc(clusters, arc_type)

        # Generate narrative structure
        narrative_structure = self._create_narrative_structure(ordered_clusters, arc_type)

        # Weave the actual story
        narrative_text = self._weave_narrative_text(narrative_structure, arc_type, context)

        # Calculate quality metrics
        coherence_score_val = self._calculate_narrative_coherence(narrative_text, ordered_clusters)
        flow_quality_val = self._calculate_narrative_flow(narrative_text, arc_type)
        emotional_arc_val = self._extract_emotional_arc(ordered_clusters)

        return StoryWeavingResult(
            narrative_text=narrative_text,
            story_arc_type=arc_type,
            coherence_score=coherence_score_val,
            memory_clusters_used=[c.cluster_id for c in ordered_clusters],
            weaving_strategy=f"{arc_type.value}_based",
            narrative_flow_quality=flow_quality_val,
            emotional_arc=emotional_arc_val
        )

    def _order_clusters_for_arc(
            self, clusters: List[MemoryCluster], arc_type: StoryArcType
    ) -> List[MemoryCluster]:
        """Order clusters according to the specified story arc type"""
        if arc_type == StoryArcType.CHRONOLOGICAL:
            return sorted(clusters, key=lambda c: c.temporal_span[0])
        elif arc_type == StoryArcType.IMPORTANCE:
            return sorted(clusters, key=lambda c: c.importance_weight, reverse=True)
        elif arc_type == StoryArcType.THEMATIC:
            return self._order_by_thematic_similarity(clusters)
        elif arc_type == StoryArcType.EMOTIONAL:
            return self._order_by_emotional_progression(clusters)
        elif arc_type == StoryArcType.CAUSAL:
            return self._order_by_causal_relationships(clusters)
        elif arc_type == StoryArcType.ASSOCIATIVE:
            return self._order_by_associations(clusters)
        else:
            # Default to chronological
            return sorted(clusters, key=lambda c: c.temporal_span[0])

    def _order_by_thematic_similarity(self, clusters: List[MemoryCluster]) -> List[MemoryCluster]:
        """Order clusters by thematic similarity using graph-based approach"""
        if len(clusters) <= 1:
            return clusters

        # Create similarity graph
        graph = nx.Graph()
        for i, cluster in enumerate(clusters):
            graph.add_node(i, cluster=cluster)

        # Add edges based on keyword similarity
        for i in range(len(clusters)):
            for j in range(i + 1, len(clusters)):
                similarity = self._calculate_keyword_similarity(
                    clusters[i].centroid_keywords,
                    clusters[j].centroid_keywords
                )
                if similarity > 0.2:
                    graph.add_edge(i, j, weight=similarity)

        # Find path that visits all nodes (approximation of TSP)
        try:
            # Start with highest importance cluster
            start_idx = max(range(len(clusters)), key=lambda i: clusters[i].importance_weight)

            visited = {start_idx}
            path_indices = [start_idx]
            current = start_idx

            while len(visited) < len(clusters):
                # Find unvisited neighbor with highest similarity
                best_next = None
                best_similarity = -1.0  # Initialize with a value lower than any possible similarity

                # Check neighbors of current node
                if current in graph:  # Ensure current node exists in graph (for disconnected components)
                    for neighbor in graph.neighbors(current):
                        if neighbor not in visited:
                            similarity = graph[current][neighbor]['weight']
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_next = neighbor

                if best_next is not None:
                    visited.add(best_next)
                    path_indices.append(best_next)
                    current = best_next
                else:
                    # No connected unvisited neighbor, pick unvisited with highest importance
                    remaining_indices = [i for i in range(len(clusters)) if i not in visited]
                    if remaining_indices:
                        # Select the most important among the remaining unvisited clusters
                        best_next = max(remaining_indices, key=lambda i: clusters[i].importance_weight)
                        visited.add(best_next)
                        path_indices.append(best_next)
                        current = best_next  # Update current for next iteration's neighbor search
                    else:  # All clusters visited
                        break

            return [clusters[i] for i in path_indices]

        except Exception as e:
            logger.error(f"Error ordering by thematic similarity: {e}. Falling back.")
            # Fallback to importance ordering
            return sorted(clusters, key=lambda c: c.importance_weight, reverse=True)

    def _calculate_keyword_similarity(self, keywords1: List[str], keywords2: List[str]) -> float:
        """Calculate similarity between two keyword lists (Jaccard index)"""
        if not keywords1 or not keywords2:
            return 0.0

        set1, set2 = set(keywords1), set(keywords2)
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0

    def _order_by_emotional_progression(self, clusters: List[MemoryCluster]) -> List[MemoryCluster]:
        """Order clusters to create meaningful emotional progression"""
        # Emotional progression order (simplified)
        emotion_order = {
            "negative": 0, "sad": 1, "neutral": 2, "curious": 3,
            "positive": 4, "happy": 5, "excited": 6
        }

        # Sort by emotional progression, then by importance (descending)
        return sorted(
            clusters,
            key=lambda c: (emotion_order.get(c.emotional_tone, 2), -c.importance_weight)
        )

    def _order_by_causal_relationships(self, clusters: List[MemoryCluster]) -> List[MemoryCluster]:
        """Order clusters based on causal relationships"""
        # For now, use chronological order as proxy for causality
        # In full implementation, would analyze causal keywords and relationships
        return sorted(clusters, key=lambda c: c.temporal_span[0])

    def _order_by_associations(self, clusters: List[MemoryCluster]) -> List[MemoryCluster]:
        """Order clusters based on associative connections"""
        if not clusters:
            return []  # Return empty list if clusters is empty

        ordered = []
        remaining = list(clusters)  # Make a mutable copy

        if not remaining:  # Double check, should be caught by first if
            return []

        # Start with highest importance
        current = max(remaining, key=lambda c: c.importance_weight)
        ordered.append(current)
        remaining.remove(current)

        while remaining:
            # Find most associated remaining cluster
            best_match = None
            best_association = -1.0  # Initialize to allow 0 similarity matches

            for candidate in remaining:
                association = self._calculate_keyword_similarity(
                    current.centroid_keywords,
                    candidate.centroid_keywords
                )
                if association > best_association:
                    best_association = association
                    best_match = candidate

            if best_match:
                ordered.append(best_match)
                remaining.remove(best_match)
                current = best_match
            else:
                # No good association, pick most important remaining (if any)
                if remaining:  # Ensure remaining is not empty
                    next_cluster = max(remaining, key=lambda c: c.importance_weight)
                    ordered.append(next_cluster)
                    remaining.remove(next_cluster)
                    current = next_cluster
                else:  # Should not happen if loop condition is `while remaining`
                    break

        return ordered

    def _create_narrative_structure(
            self, clusters: List[MemoryCluster], arc_type: StoryArcType
    ) -> Dict[str, Any]:
        """Create narrative structure for story weaving"""
        template = self.story_templates[arc_type]
        structure_phases_keys = template["structure"]

        # Distribute clusters across structure phases
        phase_clusters_map = {}

        num_clusters = len(clusters)
        num_phases = len(structure_phases_keys)

        if num_clusters == 0 or num_phases == 0:  # Handle empty cases
            for phase_key in structure_phases_keys:
                phase_clusters_map[phase_key] = []
            return {
                "arc_type": arc_type,
                "phases": phase_clusters_map,
                "transitions": template["transitions"],
                "focus": template["focus"]
            }

        clusters_per_phase_val = max(1, num_clusters // num_phases)

        for i, phase_key in enumerate(structure_phases_keys):
            start_idx = i * clusters_per_phase_val
            end_idx = (
                (i + 1) * clusters_per_phase_val
                if i < num_phases - 1
                else num_clusters
            )
            phase_clusters_map[phase_key] = clusters[start_idx:end_idx]

        return {
            "arc_type": arc_type,
            "phases": phase_clusters_map,
            "transitions": template["transitions"],
            "focus": template["focus"]
        }

    def _weave_narrative_text(
            self, structure: Dict[str, Any], arc_type: StoryArcType, context: str
    ) -> str:
        """Weave the actual narrative text from structure"""
        narrative_parts = []
        transitions = structure["transitions"]
        phases_data = structure["phases"]

        phase_names = list(phases_data.keys())

        for i, phase_name in enumerate(phase_names):
            phase_clusters = phases_data[phase_name]

            if not phase_clusters:
                continue

            # Add transition if not first phase and transition exists
            if i > 0 and transitions and i < len(transitions):  # Check if transitions is not empty
                transition = transitions[i - 1]  # Transitions align with gaps between phases
                # So for phase i (0-indexed), transition[i-1] applies
                narrative_parts.append(f"{transition},")

            # Generate content for this phase
            phase_content = self._generate_phase_content(
                phase_clusters, phase_name, arc_type
            )
            narrative_parts.append(phase_content)

        # Join all parts
        narrative_text = " ".join(part for part in narrative_parts if part)  # Filter out empty parts

        # Apply narrative polishing
        polished_narrative = self._polish_narrative(narrative_text, arc_type, context)

        return polished_narrative

    def _generate_phase_content(
            self, clusters: List[MemoryCluster], phase_name: str, arc_type: StoryArcType
    ) -> str:
        """Generate content for a specific narrative phase"""
        if not clusters:
            return ""

        content_parts = []

        for cluster in clusters:
            # Extract key information from cluster
            cluster_summary = self._summarize_cluster(cluster, phase_name, arc_type)
            if cluster_summary:  # Append only if summary is not empty
                content_parts.append(cluster_summary)

        if not content_parts:  # All summaries were empty
            return ""

        # Combine cluster summaries for this phase
        if len(content_parts) == 1:
            return content_parts[0]
        else:
            # Connect multiple clusters within phase
            return self._connect_cluster_summaries(content_parts, phase_name)

    def _summarize_cluster(
            self, cluster: MemoryCluster, phase_name: str, arc_type: StoryArcType
    ) -> str:
        """Create a summary of a memory cluster for narrative inclusion"""
        # Extract key memories from cluster
        key_memories = sorted(
            cluster.memories,
            key=lambda m: getattr(m, 'salience', 0.5) if hasattr(m, 'salience') else (
                getattr(m.salience, 'value', 0.5) if hasattr(m, 'salience') and hasattr(m.salience, 'value') else 0.5),
            # More robust salience access
            reverse=True
        )[:3]  # Top 3 memories

        summary_parts = []

        for memory in key_memories:
            memory_text = self._extract_text_from_memory(memory)

            # Adapt summary style based on arc type and phase
            if arc_type == StoryArcType.CHRONOLOGICAL:
                formatted_text = self._format_chronological_memory(memory_text, memory)
            elif arc_type == StoryArcType.THEMATIC:
                formatted_text = self._format_thematic_memory(memory_text, cluster.dominant_theme)
            elif arc_type == StoryArcType.EMOTIONAL:
                formatted_text = self._format_emotional_memory(memory_text, cluster.emotional_tone)
            elif arc_type == StoryArcType.CAUSAL:
                formatted_text = self._format_causal_memory(memory_text, phase_name)
            else:
                formatted_text = memory_text

            if formatted_text:  # Append only if not empty
                summary_parts.append(formatted_text)

        return " ".join(summary_parts[:2])  # Limit to avoid too much detail

    def _format_chronological_memory(self, memory_text: str, memory: Any) -> str:
        """Format memory for chronological narrative"""
        time_indicator = ""
        if hasattr(memory, 'timestamp'):
            try:
                timestamp_val = None
                if isinstance(memory.timestamp, (int, float)):
                    timestamp_val = datetime.fromtimestamp(memory.timestamp)
                elif isinstance(memory.timestamp, datetime):
                    timestamp_val = memory.timestamp

                if timestamp_val:
                    # Simple time description
                    days_ago = (datetime.now() - timestamp_val).days
                    if days_ago == 0:
                        time_indicator = "today"
                    elif days_ago == 1:
                        time_indicator = "yesterday"
                    elif days_ago < 7:
                        time_indicator = f"{days_ago} days ago"
                    elif days_ago < 30:
                        time_indicator = f"{days_ago // 7} weeks ago"
                    else:
                        time_indicator = "some time ago"

            except Exception:  # More specific exceptions could be caught
                time_indicator = "previously"

        if time_indicator:
            return f"({time_indicator}) {memory_text}"
        else:
            return memory_text

    def _format_thematic_memory(self, memory_text: str, theme: str) -> str:
        """Format memory for thematic narrative"""
        return f"Regarding {theme}: {memory_text}"

    def _format_emotional_memory(self, memory_text: str, emotion: str) -> str:
        """Format memory for emotional narrative"""
        emotion_qualifiers = {
            "positive": "with satisfaction",
            "negative": "with concern",
            "neutral": "matter-of-factly",
            "curious": "with interest",
            "excited": "enthusiastically",
            "sad": "regretfully"
        }

        qualifier = emotion_qualifiers.get(emotion, "")
        if qualifier:
            return f"{memory_text} {qualifier}"
        else:
            return memory_text

    def _format_causal_memory(self, memory_text: str, phase_name: str) -> str:
        """Format memory for causal narrative"""
        causal_indicators = {
            "cause": "This situation arose when",
            "effect": "As a result,",
            "consequence": "This led to",
            "outcome": "Ultimately,"
        }

        indicator = causal_indicators.get(phase_name.lower(), "")  # Match phase_name case-insensitively
        if indicator:
            return f"{indicator} {memory_text.lower()}"
        else:
            return memory_text

    def _connect_cluster_summaries(self, summaries: List[str], phase_name: str) -> str:
        """Connect multiple cluster summaries within a phase"""
        if not summaries:  # Handle empty list
            return ""
        if len(summaries) <= 1:
            return summaries[0]

        # Choose connectors based on phase
        connectors = {
            "beginning": ["and", "also", "additionally"],
            "development": ["furthermore", "meanwhile", "in addition"],
            "climax": ["simultaneously", "at the same time", "moreover"],
            "resolution": ["finally", "consequently", "as a result"],
            "theme_introduction": ["specifically", "for instance", "notably"],
            "exploration": ["further", "additionally", "also"],
            "variation": ["alternatively", "in contrast", "however"],
            "synthesis": ["overall", "in summary", "bringing these together"]
        }

        phase_connectors_list = connectors.get(
            phase_name.lower(), ["and", "also", "additionally"]  # Match phase_name case-insensitively
        )

        connected_text = summaries[0]
        for i, summary in enumerate(summaries[1:]):
            connector = phase_connectors_list[i % len(phase_connectors_list)]
            connected_text += f", {connector} {summary.lower()}"

        return connected_text

    def _polish_narrative(
            self, narrative_text: str, arc_type: StoryArcType, context: str
    ) -> str:
        """Apply final polishing to narrative text"""
        if not narrative_text.strip():  # Handle empty or whitespace-only narrative
            return ""

        # Remove redundant phrases
        polished = self._remove_redundancies(narrative_text)

        # Ensure proper flow
        polished = self._improve_flow(polished)

        # Add context integration if relevant
        if context:
            polished = self._integrate_context(polished, context)

        # Limit length
        if len(polished) > self.max_narrative_length:
            polished = self._truncate_narrative(polished, self.max_narrative_length)

        return polished

    def _remove_redundancies(self, text: str) -> str:
        """Remove redundant phrases and words"""
        # Simple redundancy removal
        sentences = text.split('. ')
        unique_sentences = []
        seen_concepts = set()

        for sentence in sentences:
            if not sentence.strip():  # Skip empty sentences
                continue

            # Extract key words
            words = sentence.lower().split()
            key_words = {word for word in words if len(word) > 3 and word.isalnum()}  # Consider only alphanumeric words

            # Check for significant overlap with previous sentences
            overlap_ratio = (
                len(key_words.intersection(seen_concepts)) / max(len(key_words), 1)
                if key_words else 0  # Avoid division by zero if key_words is empty
            )

            if overlap_ratio < 0.7:  # Less than 70% overlap
                unique_sentences.append(sentence.strip())  # Strip sentence before adding
                seen_concepts.update(key_words)

        # Join and ensure final period if content exists
        result = '. '.join(unique_sentences)
        if result and not result.endswith('.'):
            result += '.'
        return result

    def _improve_flow(self, text: str) -> str:
        """Improve narrative flow and readability"""
        # Add proper punctuation and spacing
        improved = text.replace(' ,', ',').replace('  ', ' ')

        # Ensure sentences end properly if text is not empty
        if improved and not improved.endswith('.'):
            improved += '.'

        return improved.strip()

    def _integrate_context(self, narrative: str, context: str) -> str:
        """Integrate current context into narrative"""
        # Simple context integration
        if context and context.lower() not in narrative.lower():
            return f"In the context of {context.lower()}, {narrative.lower()}"
        return narrative

    def _truncate_narrative(self, text: str, max_length: int) -> str:
        """Intelligently truncate narrative to fit length limit"""
        if len(text) <= max_length:
            return text

        # Try to truncate at sentence boundaries
        sentences = text.split('. ')
        truncated = ""

        for sentence in sentences:
            # Check if adding the current sentence (plus a period and space) exceeds max_length
            if len(truncated + sentence + '. ') <= max_length:
                truncated += sentence + '. '
            else:
                break

        # Remove trailing space if any, from the last ". "
        truncated = truncated.rstrip()

        if not truncated:  # If no complete sentences fit, or if first sentence is too long
            # Truncate at word boundary
            words = text.split()
            truncated_words = []
            current_length = 0
            # max_length - 3 for "..."
            # max_length - 1 if it might end with a period already from word split
            allowable_len = max_length - 3 if max_length > 3 else max_length

            for word in words:
                # +1 for space after word
                if current_length + len(word) + (1 if truncated_words else 0) <= allowable_len:
                    truncated_words.append(word)
                    current_length += len(word) + (1 if truncated_words else 0)  # Add 1 for space after first word
                else:
                    break

            truncated = ' '.join(truncated_words)
            if truncated and len(truncated) < len(text):  # Add ellipsis if something was truncated
                truncated += "..."

        # Ensure it doesn't exceed max_length even after adding "..."
        if len(truncated) > max_length:
            truncated = truncated[:max_length - 3] + "..."

        return truncated.strip()

    def _calculate_narrative_coherence(
            self, narrative: str, clusters: List[MemoryCluster]
    ) -> float:
        """Calculate coherence score for the generated narrative"""
        if not narrative.strip():  # If narrative is empty, coherence is low
            return 0.0

        coherence_factors = []

        # Linguistic coherence (basic)
        sentences = [s for s in narrative.split('. ') if s.strip()]  # Filter empty sentences
        if len(sentences) > 1:
            sentence_lengths = [len(s.split()) for s in sentences]
            if sentence_lengths:  # Ensure list is not empty
                avg_sentence_length = statistics.mean(sentence_lengths)
                # Prefer moderate sentence length (10-20 words)
                length_score = max(0.3, 1.0 - abs(avg_sentence_length - 15) / 15)
                coherence_factors.append(length_score)

        # Thematic coherence
        if clusters:  # Only if clusters are provided
            cluster_themes = [c.dominant_theme for c in clusters if c.dominant_theme]
            narrative_words = set(narrative.lower().split())
            theme_words = set(' '.join(cluster_themes).lower().split())

            if theme_words:  # Avoid division by zero
                theme_overlap = len(narrative_words.intersection(theme_words))
                theme_coherence = min(1.0, theme_overlap / len(theme_words))
                coherence_factors.append(theme_coherence)
            elif not narrative_words and not theme_words:  # Both empty
                coherence_factors.append(1.0)  # Considered coherent if nothing to compare
            else:  # theme_words is empty, narrative_words might not be
                coherence_factors.append(0.0)

        # Cluster coherence integration
        cluster_coherences = [c.coherence_score for c in clusters if hasattr(c, 'coherence_score')]
        if cluster_coherences:
            avg_cluster_coherence = statistics.mean(cluster_coherences)
            coherence_factors.append(avg_cluster_coherence)

        return statistics.mean(coherence_factors) if coherence_factors else 0.5

    def _calculate_narrative_flow(self, narrative: str, arc_type: StoryArcType) -> float:
        """Calculate narrative flow quality"""
        if not narrative.strip():
            return 0.0

        flow_factors = []

        # Transition word usage
        transition_words = {
            "first", "then", "next", "subsequently", "finally", "meanwhile",
            "however", "furthermore", "additionally", "consequently", "therefore",
            "because", "since", "although", "while", "thus", "hence"
        }

        narrative_words_list = narrative.lower().split()
        if not narrative_words_list:  # If list is empty
            flow_factors.append(0.0)  # No flow if no words
        else:
            transition_count = sum(1 for word in narrative_words_list if word in transition_words)
            transition_density = transition_count / len(narrative_words_list)

            # Optimal transition density is around 3-8%
            optimal_density = 0.05
            transition_score = max(
                0.2, 1.0 - abs(transition_density - optimal_density) / optimal_density
            )
            flow_factors.append(transition_score)

        # Sentence variety
        sentences = [s for s in narrative.split('. ') if s.strip()]  # Filter empty sentences
        if len(sentences) > 1:
            sentence_lengths = [len(s.split()) for s in sentences if s.split()]  # Ensure sentences have words
            if len(sentence_lengths) > 1:  # Variance requires at least 2 data points
                length_variance = statistics.variance(sentence_lengths)
                # Moderate variance is good for flow
                variety_score = min(1.0, length_variance / 25)  # Normalize by expected variance
                flow_factors.append(variety_score)
            elif sentence_lengths:  # Only one sentence with content
                flow_factors.append(0.5)  # Neutral score for single sentence variety
            else:  # No sentences with content
                flow_factors.append(0.0)

        return statistics.mean(flow_factors) if flow_factors else 0.5

    def _extract_emotional_arc(self, clusters: List[MemoryCluster]) -> List[str]:
        """Extract emotional progression from ordered clusters"""
        return [
            cluster.emotional_tone for cluster in clusters
            if hasattr(cluster, 'emotional_tone')
        ]

    def validate_narrative_coherence(
            self, narrative: str, source_memories: List[Any]
    ) -> List[CoherenceIssue]:
        """
        Validate narrative coherence and detect issues.

        Args:
            narrative: Generated narrative text
            source_memories: Original memories used in narrative

        Returns:
            List of detected coherence issues
        """
        issues = []

        # Temporal consistency check
        temporal_issues = self._check_temporal_consistency(narrative, source_memories)
        issues.extend(temporal_issues)

        # Factual consistency check
        factual_issues = self._check_factual_consistency(narrative, source_memories)
        issues.extend(factual_issues)

        # Thematic consistency check
        thematic_issues = self._check_thematic_consistency(narrative, source_memories)
        issues.extend(thematic_issues)

        # Emotional consistency check
        emotional_issues = self._check_emotional_consistency(narrative, source_memories)
        issues.extend(emotional_issues)

        # Causal consistency check
        causal_issues = self._check_causal_consistency(narrative)
        issues.extend(causal_issues)

        if issues:  # Log only if issues were found
            logger.info(f"Detected {len(issues)} coherence issues in narrative")

        return issues

    def _check_temporal_consistency(
            self, narrative: str, memories: List[Any]
    ) -> List[CoherenceIssue]:
        """Check for temporal inconsistencies"""
        issues = []
        if not narrative.strip(): return issues

        # Extract temporal indicators from narrative
        temporal_phrases = [
            "before", "after", "then", "next", "previously", "later",
            "yesterday", "today", "last week", "recently", "earlier"
        ]

        narrative_lower = narrative.lower()
        found_temporal_indicators = [
            phrase for phrase in temporal_phrases if phrase in narrative_lower
        ]

        if found_temporal_indicators:  # Simplified: was 'len(...) > 0'
            # Get actual timestamps from memories
            timestamps_values = []
            for memory in memories:
                if hasattr(memory, 'timestamp'):
                    ts = memory.timestamp
                    if isinstance(ts, (int, float)):
                        timestamps_values.append(ts)
                    elif isinstance(ts, datetime):
                        timestamps_values.append(ts.timestamp())

            if len(timestamps_values) > 1:
                # Check if narrative temporal order matches actual chronology
                # sorted_timestamps = sorted(timestamps_values) # Not used in current simple check

                # Simple check: if narrative suggests reverse chronology but memories are forward
                # This is a simplified check - full implementation would be more sophisticated
                if "before" in narrative_lower and "after" in narrative_lower:
                    # A more robust check would involve parsing the narrative for event order
                    # and comparing with actual memory timestamps.
                    issue = CoherenceIssue(
                        issue_id=f"temporal_consistency_{len(issues)}",
                        issue_type=CoherenceIssueType.TEMPORAL_INCONSISTENCY,
                        description="Narrative contains potentially conflicting temporal indicators like 'before' and 'after' in close proximity.",
                        affected_memories=[
                            getattr(m, 'memory_id', str(i)) for i, m in enumerate(memories)
                        ],
                        severity=0.6,
                        suggested_repair="Review temporal sequence and clarify chronological order.",
                        confidence=0.7
                    )
                    issues.append(issue)

        return issues

    def _check_factual_consistency(
            self, narrative: str, memories: List[Any]
    ) -> List[CoherenceIssue]:
        """Check for factual inconsistencies"""
        issues = []
        if not narrative.strip(): return issues

        # Extract key facts from memories (simplified)
        # memory_facts = []
        # for memory in memories:
        #     memory_text = self._extract_text_from_memory(memory)
        #     memory_facts.append(memory_text)

        # Simple contradiction detection
        contradiction_pairs = [
            (["can", "able", "success", "succeed"], ["cannot", "unable", "failed", "failure"]),
            (["yes", "true", "correct", "accurate"], ["no", "false", "incorrect", "wrong"]),
            (["good", "positive", "beneficial", "helpful"], ["bad", "negative", "harmful", "detrimental"])
        ]

        narrative_lower = narrative.lower()

        for positive_terms, negative_terms in contradiction_pairs:
            # Check if terms appear close to each other or related to same subject (hard to do simply)
            # For simplicity, just check if both types of terms exist in the narrative
            has_positive = any(term in narrative_lower for term in positive_terms)
            has_negative = any(term in narrative_lower for term in negative_terms)

            if has_positive and has_negative:
                # This is a very basic check. Real factual consistency is much harder.
                issue = CoherenceIssue(
                    issue_id=f"factual_contradiction_{len(issues)}",
                    issue_type=CoherenceIssueType.FACTUAL_CONTRADICTION,
                    description="Narrative contains potentially contradictory statements based on keyword pairs.",
                    affected_memories=[
                        getattr(m, 'memory_id', str(i)) for i, m in enumerate(memories)
                    ],
                    severity=0.8,
                    suggested_repair="Resolve contradictory statements or clarify context.",
                    confidence=0.6
                )
                issues.append(issue)
                break  # Found one potential contradiction type, move on

        return issues

    def _check_thematic_consistency(
            self, narrative: str, memories: List[Any]
    ) -> List[CoherenceIssue]:
        """Check for thematic inconsistencies"""
        issues = []
        if not narrative.strip() or not memories: return issues

        # Extract themes from memories and narrative
        memory_keywords_flat = []
        for memory in memories:
            if hasattr(memory, 'keywords') and memory.keywords:
                memory_keywords_flat.extend(kw.lower() for kw in memory.keywords)

        narrative_words_set = set(narrative.lower().split())
        memory_keyword_set = set(memory_keywords_flat)

        if not memory_keyword_set and narrative_words_set:  # No keywords from memories, but narrative has words
            issue = CoherenceIssue(
                issue_id=f"thematic_drift_no_source_keywords_{len(issues)}",
                issue_type=CoherenceIssueType.THEMATIC_DRIFT,
                description="Narrative has content but source memories lack defined keywords for comparison.",
                affected_memories=[getattr(m, 'memory_id', str(i)) for i, m in enumerate(memories)],
                severity=0.5,
                suggested_repair="Ensure source memories have keywords or review narrative theme.",
                confidence=0.7
            )
            issues.append(issue)
            return issues  # Early exit if no keywords to compare against

        if not memory_keyword_set:  # No keywords from memories and narrative might be empty too
            return issues

        # Check for thematic drift
        keyword_overlap = len(narrative_words_set.intersection(memory_keyword_set))

        overlap_ratio = keyword_overlap / len(memory_keyword_set)  # Avoid division by zero by check above

        if overlap_ratio < 0.3:  # Less than 30% overlap
            issue = CoherenceIssue(
                issue_id=f"thematic_drift_{len(issues)}",
                issue_type=CoherenceIssueType.THEMATIC_DRIFT,
                description=f"Narrative themes diverge significantly from source memories (overlap: {overlap_ratio:.2f}).",
                affected_memories=[
                    getattr(m, 'memory_id', str(i)) for i, m in enumerate(memories)
                ],
                severity=0.7,
                suggested_repair="Strengthen thematic connection to source memories.",
                confidence=0.8
            )
            issues.append(issue)

        return issues

    def _check_emotional_consistency(
            self, narrative: str, memories: List[Any]
    ) -> List[CoherenceIssue]:
        """Check for emotional inconsistencies"""
        issues = []
        if not narrative.strip() or not memories: return issues

        # Extract emotions from memories
        memory_emotions_list = []
        for memory in memories:
            if hasattr(memory, 'emotional_tone') and memory.emotional_tone:
                memory_emotions_list.append(memory.emotional_tone)
            elif hasattr(memory, 'primary_emotion') and memory.primary_emotion:
                memory_emotions_list.append(memory.primary_emotion)

        if memory_emotions_list:
            # Simple emotional consistency check
            unique_emotions_set = set(memory_emotions_list)

            # Check for conflicting emotions in narrative
            positive_words = {"good", "happy", "pleased", "satisfied", "excited", "joyful", "wonderful"}
            negative_words = {"bad", "sad", "upset", "disappointed", "frustrated", "terrible", "angry"}

            narrative_lower = narrative.lower()
            # Count occurrences rather than just presence for a slightly more robust check
            positive_mentions = sum(1 for word in positive_words if word in narrative_lower)
            negative_mentions = sum(1 for word in negative_words if word in narrative_lower)

            # If memories are consistently one emotion, but narrative shows mixed signals
            if len(unique_emotions_set) == 1:
                source_emotion = list(unique_emotions_set)[0]
                is_source_positive = source_emotion in {"positive", "happy", "excited",
                                                        "curious"}  # Example positive emotions
                is_source_negative = source_emotion in {"negative", "sad", "concerned"}  # Example negative emotions

                if is_source_positive and negative_mentions > positive_mentions / 2 and negative_mentions > 0:  # If source is positive but narrative has significant negative tone
                    issue_desc = "Narrative shows negative tone despite consistently positive source memories."
                elif is_source_negative and positive_mentions > negative_mentions / 2 and positive_mentions > 0:  # If source is negative but narrative has significant positive tone
                    issue_desc = "Narrative shows positive tone despite consistently negative source memories."
                else:
                    issue_desc = None  # No clear conflict

                if issue_desc:
                    issue = CoherenceIssue(
                        issue_id=f"emotional_inconsistency_{len(issues)}",
                        issue_type=CoherenceIssueType.EMOTIONAL_DISCONTINUITY,
                        description=issue_desc,
                        affected_memories=[
                            getattr(m, 'memory_id', str(i)) for i, m in enumerate(memories)
                        ],
                        severity=0.5,
                        suggested_repair="Align narrative emotional tone with source memories.",
                        confidence=0.6
                    )
                    issues.append(issue)
            # If narrative has strong mixed signals regardless of source memories consistency
            elif positive_mentions > 1 and negative_mentions > 1:  # Arbitrary threshold for "strong" signals
                issue = CoherenceIssue(
                    issue_id=f"emotional_conflict_in_narrative_{len(issues)}",
                    issue_type=CoherenceIssueType.EMOTIONAL_DISCONTINUITY,
                    description="Narrative contains strong conflicting emotional indicators.",
                    affected_memories=[getattr(m, 'memory_id', str(i)) for i, m in enumerate(memories)],
                    severity=0.6,
                    suggested_repair="Clarify or resolve conflicting emotional tones within the narrative.",
                    confidence=0.55
                )
                issues.append(issue)

        return issues

    def _check_causal_consistency(self, narrative: str) -> List[CoherenceIssue]:
        """Check for causal inconsistencies"""
        issues = []
        if not narrative.strip(): return issues

        # Look for causal indicators
        causal_words = [
            "because", "since", "therefore", "thus", "as a result", "consequently",
            "due to", "leads to", "causes"
        ]
        narrative_lower = narrative.lower()

        found_causal_indicators = [word for word in causal_words if word in narrative_lower]

        if found_causal_indicators:  # Simplified: was 'len(...) > 0'
            # Simple check for causal coherence (very basic)
            # A full implementation would analyze actual causal chains using NLP techniques.

            # Check for potentially problematic patterns like "A because B therefore A" (circularity)
            # This is hard to detect without deeper semantic understanding.
            # For now, a very naive check for multiple strong conflicting indicators in one sentence
            sentences = narrative_lower.split('.')
            for sentence in sentences:
                # Example: if a sentence contains both "because" and "therefore" in a way that might be confusing
                # This is highly heuristic and prone to false positives/negatives.
                if ("because" in sentence and "therefore" in sentence) or \
                        ("as a result" in sentence and "due to" in sentence and sentence.find(
                            "as a result") < sentence.find("due to")):  # Effect before cause
                    issue = CoherenceIssue(
                        issue_id=f"causal_breakdown_{len(issues)}",
                        issue_type=CoherenceIssueType.CAUSAL_BREAKDOWN,
                        description="Potential circular or unclear causal reasoning detected in a sentence.",
                        affected_memories=[],  # Difficult to link to specific memories without more context
                        severity=0.4,
                        suggested_repair="Clarify causal relationships and ensure logical flow.",
                        confidence=0.5
                    )
                    issues.append(issue)
                    break  # One potential issue of this type is enough for now

        return issues

    def repair_coherence_issues(
            self, narrative: str, issues: List[CoherenceIssue], source_memories: List[Any]
    ) -> str:
        """
        Attempt to repair coherence issues in narrative.

        Args:
            narrative: Original narrative with issues
            issues: List of detected coherence issues
            source_memories: Source memories for context

        Returns:
            Repaired narrative text
        """
        repaired_narrative = narrative
        repaired_issue_count = 0

        # Sort issues by severity (desc) to address more critical ones first, if desired
        # sorted_issues = sorted(issues, key=lambda i: i.severity, reverse=True)

        for issue in issues:  # Using original order for now
            if issue.severity > 0.6:  # Only attempt to repair high-severity issues
                original_length = len(repaired_narrative)
                repaired_narrative = self._apply_issue_repair(
                    repaired_narrative, issue, source_memories
                )
                if len(repaired_narrative) != original_length:  # Crude check if repair did something
                    repaired_issue_count += 1

        if repaired_issue_count > 0:
            logger.info(f"Applied repairs for {repaired_issue_count} high-severity issues.")

        return repaired_narrative

    def _apply_issue_repair(
            self, narrative: str, issue: CoherenceIssue, memories: List[Any]
    ) -> str:
        """Apply specific repair for a coherence issue (simplified examples)"""

        if issue.issue_type == CoherenceIssueType.TEMPORAL_INCONSISTENCY:
            return self._repair_temporal_inconsistency(narrative, memories)
        elif issue.issue_type == CoherenceIssueType.FACTUAL_CONTRADICTION:
            return self._repair_factual_contradiction(narrative, memories)
        elif issue.issue_type == CoherenceIssueType.THEMATIC_DRIFT:
            return self._repair_thematic_drift(narrative, memories)
        elif issue.issue_type == CoherenceIssueType.EMOTIONAL_DISCONTINUITY:
            return self._repair_emotional_discontinuity(narrative, memories)
        elif issue.issue_type == CoherenceIssueType.CAUSAL_BREAKDOWN:
            return self._repair_causal_breakdown(narrative)
        else:
            return narrative  # No specific repair available

    def _repair_temporal_inconsistency(self, narrative: str, memories: List[Any]) -> str:
        """Repair temporal inconsistencies (simplified)"""
        # Simple repair: add clarifying temporal context if "before" and "after" appear close
        if "before" in narrative.lower() and "after" in narrative.lower():
            # This is a very basic fix. A better fix would re-order or rephrase.
            # Check if they are not already well-separated by other text
            if narrative.lower().find("before") > narrative.lower().find(
                    "after") - 30:  # If "before" appears soon after "after"
                return f"To clarify the sequence of events: {narrative}"
        return narrative

    def _repair_factual_contradiction(self, narrative: str, memories: List[Any]) -> str:
        """Repair factual contradictions (simplified)"""
        # Simple repair: add qualifying language if strong contradictions exist
        contradiction_indicators = ["however", "although", "while", "despite", "but"]

        # If narrative seems to state A and NOT A without qualification
        # This is hard to detect simply. Assume the check_factual_consistency was good enough.
        if not any(indicator in narrative.lower() for indicator in contradiction_indicators):
            sentences = narrative.split('. ')
            if len(sentences) > 1:
                # Attempt to insert a qualifying phrase. This is highly heuristic.
                # Example: find a sentence with "cannot" and a previous one with "can"
                # For now, a generic addition if issue was flagged:
                return f"Acknowledging potential complexities or differing aspects: {narrative}"
        return narrative

    def _repair_thematic_drift(self, narrative: str, memories: List[Any]) -> str:
        """Repair thematic drift (simplified)"""
        # Add thematic anchoring if memories have clear common keywords
        if not memories: return narrative

        all_memory_keywords = []
        for memory in memories:
            if hasattr(memory, 'keywords') and memory.keywords:
                all_memory_keywords.extend(kw.lower() for kw in memory.keywords)

        if all_memory_keywords:
            keyword_counts = Counter(all_memory_keywords)
            # Get, for instance, the top 1-2 most common keywords as main theme
            main_themes = [kw for kw, count in keyword_counts.most_common(2)]
            if main_themes:
                theme_str = " and ".join(main_themes)
                # Prepend thematic anchor if not already strongly present
                if theme_str not in narrative.lower()[:len(narrative) // 3]:  # Check beginning part
                    return f"Focusing on the theme of {theme_str}, {narrative.lower()}"
        return narrative

    def _repair_emotional_discontinuity(self, narrative: str, memories: List[Any]) -> str:
        """Repair emotional discontinuity (simplified)"""
        if not memories: return narrative

        # Add emotional context or smooth transitions
        memory_emotions_list = []
        for memory in memories:
            if hasattr(memory, 'emotional_tone') and memory.emotional_tone:
                memory_emotions_list.append(memory.emotional_tone)

        if memory_emotions_list:
            dominant_emotion = Counter(memory_emotions_list).most_common(1)[0][0]
            # If narrative seems to jump between emotions without bridge
            # This is hard to detect. If the issue was flagged, add a generic qualifier.
            # Example: check for sudden shifts like "happy... then suddenly sad"
            # Generic repair:
            if f"(reflecting a {dominant_emotion} experience)" not in narrative:
                # Check if the narrative already ends with a qualifier
                if not narrative.endswith(")"):
                    return narrative + f" (overall, reflecting a predominantly {dominant_emotion} experience)."
        return narrative

    def _repair_causal_breakdown(self, narrative: str) -> str:
        """Repair causal breakdown (simplified)"""
        # Simplify or clarify causal language if flagged
        # Example: replace potentially confusing chains
        repaired = narrative
        # These are very specific and might not be generally applicable or safe
        # repaired = repaired.replace("because therefore", "and so, because") # Example rephrase
        # repaired = repaired.replace("since thus", "and thus, since")       # Example rephrase

        # More generally, if "because" and "therefore" are in the same short sentence part,
        # try to rephrase or add clarifying words. This is complex.
        # For now, a simple attempt to add clarity if problem was detected:
        if "because" in narrative.lower() and "therefore" in narrative.lower():
            # Check if they are part of a common problematic pattern
            # e.g. "X because Y therefore Z" -> "Because Y, X, and therefore Z."
            # This requires more sophisticated parsing.
            # Simple intervention:
            if "the causal chain is as follows:" not in narrative.lower():
                return f"To clarify the causal connections: {narrative}"
        return repaired

    # Integration and test functions
    async def synthesize_with_advanced_features(
            self,  # Added self
            memories: List[Any],
            context: str = "",
            arc_type: StoryArcType = StoryArcType.CHRONOLOGICAL,
            validate_coherence: bool = True
    ) -> Dict[str, Any]:
        """
        Main integration function for advanced memory synthesis.
        Args:
            memories: List of memory objects
            context: Current context
            arc_type: Type of story arc to create
            validate_coherence: Whether to validate and repair coherence
        Returns:
            Complete synthesis result with narrative and analysis
        """

        # Step 1: Cluster memories
        clusters = self.cluster_memories_by_relevance(memories, context)  # Use self.

        # Step 2: Weave story
        story_result = self.weave_story_from_clusters(clusters, arc_type, context)  # Use self.

        # Step 3: Validate coherence
        coherence_issues_list = []
        repaired_narrative_text = story_result.narrative_text

        if validate_coherence and story_result.narrative_text.strip() and memories:
            coherence_issues_list = self.validate_narrative_coherence(  # Use self.
                story_result.narrative_text, memories
            )

            if coherence_issues_list:
                repaired_narrative_text = self.repair_coherence_issues(  # Use self.
                    story_result.narrative_text, coherence_issues_list, memories
                )

        return {
            "narrative_text": repaired_narrative_text,
            "original_narrative": story_result.narrative_text,
            "clusters": [
                {
                    "cluster_id": c.cluster_id,
                    "theme": c.dominant_theme,
                    "coherence": c.coherence_score,
                    "memory_count": len(c.memories),
                    "emotional_tone": c.emotional_tone,
                    "importance": c.importance_weight
                }
                for c in clusters
            ],
            "story_arc_type": story_result.story_arc_type.value,
            "narrative_coherence": story_result.coherence_score,
            "narrative_flow_quality": story_result.narrative_flow_quality,
            "emotional_arc": story_result.emotional_arc,
            "coherence_issues": [
                {
                    "type": issue.issue_type.value,
                    "description": issue.description,
                    "severity": issue.severity,
                    "suggested_repair": issue.suggested_repair
                }
                for issue in coherence_issues_list
            ],
            "synthesis_quality": {
                "clusters_used": len(clusters),
                "total_memories": len(memories),
                "coherence_score": story_result.coherence_score,
                "flow_quality": story_result.narrative_flow_quality,
                "issues_detected": len(coherence_issues_list),
                "high_severity_issues": len([
                    i for i in coherence_issues_list if hasattr(i, 'severity') and i.severity > 0.7
                ])
            }
        }






# -------------------- memory_lifecycle_manager.py --------------------

# ceaf_core/modules/memory_blossom/memory_lifecycle_manager.py

import time
import logging
from typing import List, Optional, Tuple, Dict, Any
import math  # For decay functions like exponential

# Assuming memory_types.py is in the same parent directory (modules/memory_blossom/)
try:
    from .memory_types import BaseMemory, MemorySalience, AnyMemoryType, MemorySourceType

    MEMORY_TYPES_LOADED_SUCCESSFULLY = True
except ImportError:
    logging.error("MemoryLifecycleManager: Could not import CEAF memory_types. Using placeholders.")
    MEMORY_TYPES_LOADED_SUCCESSFULLY = False


    # Define minimal placeholders if import fails, to allow type hinting and basic structure
    class BaseMemory:  # type: ignore
        def __init__(self, memory_id: str, timestamp: float, **kwargs):
            self.memory_id = memory_id
            self.timestamp = timestamp
            self.last_accessed_ts: Optional[float] = None
            self.access_count: int = 0
            self.dynamic_salience_score: float = 0.5
            self.decay_rate: float = 0.01
            self.salience: str = "medium"  # Placeholder for MemorySalience enum
            self.source_type: str = "unknown"  # Placeholder for MemorySourceType enum
            self.__dict__.update(kwargs)

        def mark_accessed(self): self.last_accessed_ts = time.time(); self.access_count += 1


    class MemorySalience:
        MEDIUM = "medium";
        LOW = "low";
        HIGH = "high";
        CRITICAL = "critical"  # type: ignore


    class MemorySourceType:
        USER_INTERACTION = "user_interaction";
        ORA_RESPONSE = "ora_response";
        INTERNAL_REFLECTION = "internal_reflection";
        TOOL_OUTPUT = "tool_output";
        SYNTHESIZED_SUMMARY = "synthesized_summary";
        EXTERNAL_INGESTION = "external_ingestion";
        NCF_DIRECTIVE = "ncf_directive"  # type: ignore


    AnyMemoryType = BaseMemory  # type: ignore

logger = logging.getLogger("MemoryLifecycleManager")

# --- Configuration for Lifecycle Management ---
SALIENCE_UPDATE_CONFIG = {
    "access_retrieval_boost": 0.05,  # Boost on simple retrieval
    "access_commit_boost": 0.2,  # Boost when explicitly committed/created
    "positive_reinforcement_base": 0.15,  # Base boost for positive signals
    "negative_reinforcement_base": -0.1,  # Base penalty for negative signals
    "max_salience": 1.0,
    "min_salience": 0.0
}

# How many seconds represent one "unit" for decay. E.g., decay applied per day.
DECAY_TIME_UNIT_SECONDS = 86400  # 1 day
DEFAULT_ARCHIVE_SALIENCE_THRESHOLD = 0.1
EPHEMERAL_SOURCE_TYPES_FOR_DELETION = [
    MemorySourceType.TOOL_OUTPUT,  # Often transient
    # MemorySourceType.SYNTHESIZED_SUMMARY, # Can be regenerated
]


def initialize_dynamic_salience(memory: BaseMemory) -> None:
    """
    Initializes the dynamic_salience_score based on the initial MemorySalience enum.
    This should be called when a memory is first created or loaded.
    """
    if not hasattr(memory, 'dynamic_salience_score'):
        # If the attribute isn't even there, add it.
        # This helps if old memory objects are loaded that don't have it.
        setattr(memory, 'dynamic_salience_score', 0.5)  # Default starting point

    if MEMORY_TYPES_LOADED_SUCCESSFULLY:
        salience_map = {
            MemorySalience.CRITICAL: 0.95,
            MemorySalience.HIGH: 0.8,
            MemorySalience.MEDIUM: 0.6,
            MemorySalience.LOW: 0.3,
        }
        # Handle if memory.salience is string or enum
        initial_salience_value = memory.salience
        if isinstance(memory.salience, MemorySalience):
            initial_salience_value = memory.salience.value  # Get the string value

        memory.dynamic_salience_score = salience_map.get(initial_salience_value, 0.5)  # type: ignore
    else:  # Fallback for placeholder types
        salience_map_str = {
            "critical": 0.95, "high": 0.8, "medium": 0.6, "low": 0.3,
        }
        memory.dynamic_salience_score = salience_map_str.get(str(memory.salience).lower(), 0.5)
    logger.debug(
        f"Memory {memory.memory_id}: Initialized dynamic_salience_score to {memory.dynamic_salience_score:.2f} from static salience '{memory.salience}'.")


def update_dynamic_salience(
        memory: BaseMemory,
        access_type: str = "retrieval",  # "retrieval", "commit", "positive_feedback", "negative_feedback"
        reinforcement_signal: Optional[float] = None,  # Value between -1.0 and 1.0 for feedback
        config: Dict[str, Any] = SALIENCE_UPDATE_CONFIG
) -> None:
    """
    Updates the dynamic salience score of a memory.
    """
    if not hasattr(memory, 'dynamic_salience_score'):
        initialize_dynamic_salience(memory)  # Ensure it's initialized

    current_salience = memory.dynamic_salience_score
    change = 0.0

    if access_type == "retrieval":
        change = config.get("access_retrieval_boost", 0.05)
        memory.mark_accessed()  # Also update access count and timestamp
    elif access_type == "commit":
        change = config.get("access_commit_boost", 0.2)
        memory.mark_accessed()
    elif access_type == "positive_feedback" and reinforcement_signal is not None:
        # reinforcement_signal should be 0 to 1 for positive
        change = config.get("positive_reinforcement_base", 0.15) * max(0, min(1, reinforcement_signal))
        memory.mark_accessed()
    elif access_type == "negative_feedback" and reinforcement_signal is not None:
        # reinforcement_signal should be 0 to 1 for magnitude of negative feedback (penalty = base * signal)
        change = config.get("negative_reinforcement_base", -0.1) * max(0, min(1, reinforcement_signal))
        memory.mark_accessed()
    else:
        logger.warning(
            f"Memory {memory.memory_id}: Unknown access_type '{access_type}' or missing reinforcement_signal for salience update.")

    new_salience = current_salience + change
    memory.dynamic_salience_score = max(config.get("min_salience", 0.0),
                                        min(config.get("max_salience", 1.0), new_salience))

    logger.debug(
        f"Memory {memory.memory_id}: Salience updated from {current_salience:.2f} to {memory.dynamic_salience_score:.2f} (Change: {change:.2f}, Type: {access_type}).")


def apply_decay_to_memory(memory: BaseMemory, current_time: Optional[float] = None) -> None:
    """Applies decay to a single memory's dynamic_salience_score."""
    if not hasattr(memory, 'dynamic_salience_score') or not hasattr(memory, 'decay_rate'):
        logger.warning(f"Memory {memory.memory_id} missing dynamic_salience_score or decay_rate. Skipping decay.")
        return

    if not current_time:
        current_time = time.time()

    last_event_time = memory.last_accessed_ts or memory.timestamp
    time_since_last_event = current_time - last_event_time

    if time_since_last_event <= 0:
        return  # No decay if accessed now or in the future

    # Simple linear decay for now, could be exponential
    # decay_units = time_since_last_event / DECAY_TIME_UNIT_SECONDS
    # decay_amount = memory.decay_rate * decay_units

    # Exponential decay: S_new = S_old * e^(-lambda * t)
    # where lambda is related to decay_rate and t is time in units.
    # For simplicity, let's use a slightly different exponential model: S_new = S_old * (1 - decay_rate_per_unit)^decay_units
    decay_units = time_since_last_event / DECAY_TIME_UNIT_SECONDS
    decay_factor_per_unit = (1.0 - memory.decay_rate)  # Assumes decay_rate is per DECAY_TIME_UNIT_SECONDS

    # To prevent issues with very small decay_factor_per_unit or large decay_units leading to 0
    if decay_factor_per_unit <= 0:
        new_salience = SALIENCE_UPDATE_CONFIG.get("min_salience", 0.0)
    else:
        try:
            # Ensure the base for pow is not negative if decay_units is fractional
            effective_decay_factor = math.pow(max(0, decay_factor_per_unit), decay_units)
            new_salience = memory.dynamic_salience_score * effective_decay_factor
        except ValueError:  # math domain error
            logger.warning(f"Memory {memory.memory_id}: Math error during decay calculation. Clamping salience.")
            new_salience = SALIENCE_UPDATE_CONFIG.get("min_salience", 0.0)

    old_salience = memory.dynamic_salience_score
    memory.dynamic_salience_score = max(SALIENCE_UPDATE_CONFIG.get("min_salience", 0.0), new_salience)

    if old_salience != memory.dynamic_salience_score:
        logger.debug(
            f"Memory {memory.memory_id}: Decayed salience from {old_salience:.3f} to {memory.dynamic_salience_score:.3f} "
            f"(time_delta: {time_since_last_event:.0f}s, decay_units: {decay_units:.2f})."
        )


def apply_decay_to_all_memories(memory_store: List[AnyMemoryType]) -> None:
    """
    Periodically called. Reduces dynamic_salience_score based on decay_rate and last_accessed_ts.
    Modifies memories in-place.
    """
    logger.info(f"Applying decay to {len(memory_store)} memories.")
    current_time = time.time()
    for memory in memory_store:
        apply_decay_to_memory(memory, current_time)
    logger.info("Decay application complete.")


def archive_or_forget_low_salience_memories(
        memory_store: List[AnyMemoryType],
        archive_threshold: float = DEFAULT_ARCHIVE_SALIENCE_THRESHOLD,
        ephemeral_sources: Optional[List[Any]] = None  # List of MemorySourceType values or strings
) -> Tuple[List[AnyMemoryType], List[AnyMemoryType], List[AnyMemoryType]]:
    """
    Identifies memories below a dynamic_salience_score threshold.
    Returns three lists: memories_to_keep, memories_to_archive, memories_to_forget.
    Does NOT modify the input memory_store directly, caller handles that.
    """
    if ephemeral_sources is None:
        ephemeral_sources = EPHEMERAL_SOURCE_TYPES_FOR_DELETION

    memories_to_keep: List[AnyMemoryType] = []
    memories_to_archive: List[AnyMemoryType] = []
    memories_to_forget: List[AnyMemoryType] = []

    logger.info(f"Checking {len(memory_store)} memories for archiving/forgetting (threshold: {archive_threshold}).")

    for memory in memory_store:
        if not hasattr(memory, 'dynamic_salience_score'):
            logger.warning(f"Memory {memory.memory_id} missing dynamic_salience_score. Keeping it by default.")
            memories_to_keep.append(memory)
            continue

        if memory.dynamic_salience_score < archive_threshold:
            # Check if memory.source_type is in ephemeral_sources
            # Handle both enum and string representations for source_type
            source_type_val = memory.source_type
            if MEMORY_TYPES_LOADED_SUCCESSFULLY and isinstance(memory.source_type, MemorySourceType):
                source_type_val = memory.source_type.value  # type: ignore

            is_ephemeral = False
            for ephemeral_type in ephemeral_sources:
                ephemeral_type_val = ephemeral_type
                if MEMORY_TYPES_LOADED_SUCCESSFULLY and isinstance(ephemeral_type, MemorySourceType):
                    ephemeral_type_val = ephemeral_type.value  # type: ignore

                if source_type_val == ephemeral_type_val:
                    is_ephemeral = True
                    break

            if is_ephemeral:
                memories_to_forget.append(memory)
                logger.debug(
                    f"Memory {memory.memory_id} (Salience: {memory.dynamic_salience_score:.2f}, Source: {source_type_val}) marked for FORGETTING.")
            else:
                memories_to_archive.append(memory)
                logger.debug(
                    f"Memory {memory.memory_id} (Salience: {memory.dynamic_salience_score:.2f}, Source: {source_type_val}) marked for ARCHIVING.")
        else:
            memories_to_keep.append(memory)

    logger.info(
        f"Memory lifecycle: Keep: {len(memories_to_keep)}, Archive: {len(memories_to_archive)}, Forget: {len(memories_to_forget)}."
    )
    return memories_to_keep, memories_to_archive, memories_to_forget


MEMORY_LIFECYCLE_MANAGER_LOADED = MEMORY_TYPES_LOADED_SUCCESSFULLY


# -------------------- memory_types.py --------------------

# Memory Types
# ceaf_project/ceaf_core/modules/memory_blossom/memory_types.py
import re
import time
import uuid
from enum import Enum
from typing import List, Dict, Any, Optional, Union, Literal
from pydantic import BaseModel, Field
from pydantic import model_validator


# --- Enums for Categorization ---

class MemorySalience(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"  # For flashbulb-like memories


class MemorySourceType(Enum):
    USER_INTERACTION = "user_interaction"
    ORA_RESPONSE = "ora_response"
    TOOL_OUTPUT = "tool_output"
    INTERNAL_REFLECTION = "internal_reflection"  # e.g., from MCL or VRE
    NCF_DIRECTIVE = "ncf_directive"
    EXTERNAL_INGESTION = "external_ingestion"  # For knowledge loaded from outside CEAF
    SYNTHESIZED_SUMMARY = "synthesized_summary"  # Memory created by MemoryBlossom itself


class EmotionalTag(Enum):
    # Basic Plutchik-like emotions, can be expanded
    JOY = "joy"
    TRUST = "trust"
    FEAR = "fear"
    SURPRISE = "surprise"
    SADNESS = "sadness"
    DISGUST = "disgust"
    ANGER = "anger"
    ANTICIPATION = "anticipation"
    NEUTRAL = "neutral"
    CONFUSION = "confusion"  # Useful for AI state
    CURIOSITY = "curiosity"
    SATISFACTION = "satisfaction"  # e.g., task completion
    FRUSTRATION = "frustration"  # e.g., repeated errors


# --- Base Memory Model ---

class BaseMemory(BaseModel):
    memory_id: str = Field(default_factory=lambda: f"mem_{uuid.uuid4().hex}")
    timestamp: float = Field(default_factory=time.time)
    last_accessed_ts: Optional[float] = None
    access_count: int = 0

    source_turn_id: Optional[str] = None  # Invocation ID of the turn it originated from
    source_interaction_id: Optional[str] = None  # Broader interaction/session ID
    source_type: MemorySourceType
    source_agent_name: Optional[str] = None  # e.g., "ORA", "MCL_Agent", "user"

    salience: MemorySalience = MemorySalience.MEDIUM
    emotional_tags: List[EmotionalTag] = Field(default_factory=list)
    keywords: List[str] = Field(default_factory=list)  # For keyword-based retrieval

    # For narrative linking
    related_memory_ids: List[str] = Field(default_factory=list)
    narrative_thread_id: Optional[str] = None  # To group memories into a coherent story/topic

    dynamic_salience_score: float = Field(0.5, ge=0.0, le=1.0)  # Calculated, reflects current importance
    decay_rate: float = Field(0.01)  # How quickly it loses salience if not accessed/reinforced

    # Embedding - will be a vector, store as list of floats or a reference
    # For simplicity here, we'll assume it might be stored elsewhere or handled by a vector DB.
    # If storing directly: embedding: Optional[List[float]] = None
    embedding_reference: Optional[str] = None  # e.g., ID in a vector database

    metadata: Dict[str, Any] = Field(default_factory=dict)  # For any other custom data

    def mark_accessed(self):
        self.last_accessed_ts = time.time()
        self.access_count += 1

    class Config:
        use_enum_values = True  # Store enum values as strings




class GoalStatus(Enum):
    PENDING = "pending"
    ACTIVE = "active"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"

class GoalRecord(BaseMemory): # Goals are a form of memory
    memory_type: Literal["goal_record"] = "goal_record"
    goal_description: str
    parent_goal_id: Optional[str] = None
    sub_goal_ids: List[str] = Field(default_factory=list)
    status: GoalStatus = GoalStatus.PENDING
    priority: int = Field(5, ge=1, le=10) # 1 highest
    motivation_level: float = Field(0.7, ge=0.0, le=1.0) # Internal drive
    origin_narrative_thread_id: Optional[str] = None # Link to narrative that spawned it
    linked_procedural_memory_id: Optional[str] = None # If a procedure exists to achieve it

    # --- Specific Memory Types ---
class ExplicitMemoryContent(BaseModel):
    # Content can be text, structured data, or reference to a file/artifact
    text_content: Optional[str] = None
    structured_data: Optional[Dict[str, Any]] = None  # e.g., JSON object
    artifact_reference: Optional[str] = None




    @model_validator(mode='after')
    def check_content_present(self) -> 'ExplicitMemoryContent':
         if not self.text_content and not self.structured_data and not self.artifact_reference:
             raise ValueError("At least one of text_content, structured_data, or artifact_reference must be provided.")
         return self


class ExplicitMemory(BaseMemory):
    memory_type: Literal["explicit"] = "explicit"
    content: ExplicitMemoryContent
    confidence_score: Optional[float] = Field(None, ge=0.0, le=1.0)  # Confidence in the accuracy of the memory
    explains_procedure_step: Optional[str] = Field(None,
                                                   description="ID of a ProceduralStep this memory elaborates on.")
    provides_evidence_for_goal: Optional[str] = Field(None, description="ID of a GoalRecord this memory supports.")



class KGEntityType(Enum):
    PERSON = "person"
    ORGANIZATION = "organization"
    LOCATION = "location"
    EVENT = "event"
    CONCEPT = "concept"
    PRODUCT = "product"
    ARTIFACT = "artifact" # e.g., a report, a piece of code
    SYSTEM_COMPONENT = "system_component" # e.g., ORA, MBS, NCF
    USER = "user"
    ISSUE = "issue"
    OTHER = "other"

class KGEntityRecord(BaseMemory):
    """
    Represents a node (entity) in the knowledge graph.
    Stored as a distinct memory type in MemoryBlossom.
    """
    memory_type: Literal["kg_entity_record"] = "kg_entity_record"
    entity_id_str: str = Field(..., description="A unique string identifier for this entity (e.g., 'user_john_doe', 'concept_ai_ethics'). Should be canonicalized.")
    label: str = Field(..., description="Human-readable label for the entity (e.g., 'John Doe', 'AI Ethics').")
    entity_type: KGEntityType = Field(KGEntityType.OTHER, description="The type of the entity.")
    description: Optional[str] = Field(None, description="A brief description of the entity.")
    attributes: Dict[str, Any] = Field(default_factory=dict, description="Key-value pairs describing the entity's properties.")
    aliases: List[str] = Field(default_factory=list, description="Alternative names or identifiers for this entity.")

    @model_validator(mode='before')
    @classmethod
    def ensure_entity_id_from_label(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        # If entity_id_str is not provided, attempt to create one from label and type
        # This is a simple placeholder; robust canonicalization is complex.
        if not values.get('entity_id_str') and values.get('label'):
            label_slug = re.sub(r'\s+', '_', values['label'].lower())
            label_slug = re.sub(r'\W+', '', label_slug) # Remove non-alphanumeric
            entity_type_val = values.get('entity_type', KGEntityType.OTHER)
            if isinstance(entity_type_val, KGEntityType):
                entity_type_val = entity_type_val.value
            values['entity_id_str'] = f"{entity_type_val}_{label_slug[:50]}" # Ensure not too long
        elif not values.get('entity_id_str') and not values.get('label'):
            raise ValueError("KGEntityRecord requires at least a label to derive an entity_id_str if not provided.")
        return values


class EmotionalContext(BaseModel):
    triggering_event_summary: Optional[str] = None
    associated_stimuli: List[str] = Field(default_factory=list)
    intensity: float = Field(0.5, ge=0.0, le=1.0)  # Normalized intensity


class EmotionalMemory(BaseMemory):
    memory_type: Literal["emotional"] = "emotional"
    primary_emotion: EmotionalTag  # The dominant emotion recorded
    context: EmotionalContext


class ProceduralStep(BaseModel):
    step_number: int
    description: str
    expected_inputs: Optional[List[str]] = None
    expected_outputs: Optional[List[str]] = None
    tool_to_use: Optional[str] = None  # Name of ADK tool
    sub_procedure_id: Optional[str] = None  # Link to another ProceduralMemory


class ProceduralMemory(BaseMemory):
    memory_type: Literal["procedural"] = "procedural"
    procedure_name: str
    goal_description: str
    steps: List[ProceduralStep]
    trigger_conditions: List[str] = Field(default_factory=list)  # Conditions that might activate this procedure


class FlashbulbMemory(BaseMemory):
    memory_type: Literal["flashbulb"] = "flashbulb"
    event_summary: str  # A concise summary of the highly salient event
    vividness_score: float = Field(ge=0.0, le=1.0)
    personal_significance: Optional[str] = None  # Why it was significant to CEAF
    # Flashbulb memories inherently have high salience, so it's set in BaseMemory
    content_details: ExplicitMemoryContent  # Detailed account of the event


class SomaticMarker(BaseModel):
    marker_type: str  # e.g., "positive_anticipation", "negative_avoidance", "error_signal"
    associated_state_pattern: Dict[str, Any]  # Abstract representation of internal state associated with this marker
    intensity: float = Field(ge=0.0, le=1.0)


class SomaticMemory(BaseMemory):
    """
    Represents "gut feelings" or internal state associations with certain situations or outcomes.
    These are learned markers that can quickly guide decision-making without full deliberation.
    """
    memory_type: Literal["somatic"] = "somatic"
    triggering_context_summary: str  # Summary of the context that elicits/elicited this marker
    marker: SomaticMarker


class LiminalThoughtFragment(BaseModel):
    fragment_text: Optional[str] = None
    image_url_reference: Optional[str] = None  # If it's a visual fragment
    conceptual_tags: List[str] = Field(default_factory=list)
    connection_strength_to_conscious: float = Field(ge=0.0, le=1.0)  # How close it is to surfacing


class LiminalMemory(BaseMemory):
    """
    Represents "pre-conscious" fragments, ideas, or associations that are not yet fully formed
    or integrated into explicit thought. The "dream-like" content.
    """
    memory_type: Literal["liminal"] = "liminal"
    fragments: List[LiminalThoughtFragment]
    potential_significance: Optional[str] = None  # Why these fragments might be important


class GenerativeSeed(BaseModel):
    seed_type: str  # e.g., "prompt_template", "style_guide", "core_concept_map"
    content: Union[str, Dict[str, Any]]  # The actual seed data
    usage_instructions: Optional[str] = None


class GenerativeMemory(BaseMemory):
    """
    Memories that help CEAF generate new content, ideas, or behaviors.
    These are less about recalling past events and more about providing templates or starting points.
    """
    memory_type: Literal["generative"] = "generative"
    seed_name: str
    seed_data: GenerativeSeed
    applicability_contexts: List[str] = Field(default_factory=list)  # When is this seed useful?

class KGRelationRecord(BaseMemory):
    """
    Represents an edge (relationship) in the knowledge graph.
    Stored as a distinct memory type in MemoryBlossom.
    """
    memory_type: Literal["kg_relation_record"] = "kg_relation_record"
    relation_id_str: str = Field(default_factory=lambda: f"rel_{uuid.uuid4().hex}", description="A unique string identifier for this relationship instance.")
    source_entity_id_str: str = Field(..., description="The entity_id_str of the source/subject entity.")
    target_entity_id_str: str = Field(..., description="The entity_id_str of the target/object entity.")
    relation_label: str = Field(..., description="Human-readable label for the relationship (e.g., 'reported', 'affects', 'is_a', 'created_by').")
    description: Optional[str] = Field(None, description="A brief description or context for this relationship.")
    attributes: Dict[str, Any] = Field(default_factory=dict, description="Key-value pairs describing properties of the relationship itself (e.g., 'date_reported', 'confidence').")
    directed: bool = Field(True, description="Is the relationship directed from source to target?")


# --- Union Type for all Memory Types ---
# This allows functions to accept any specific memory type.
AnyMemoryType = Union[
    ExplicitMemory,
    EmotionalMemory,
    ProceduralMemory,
    FlashbulbMemory,
    SomaticMemory,
    LiminalMemory,
    GenerativeMemory,
    GoalRecord,          # Make sure GoalRecord is here
    KGEntityRecord,      # Add new KG type
    KGRelationRecord     # Add new KG type
]


# -------------------- synthesizer.py --------------------

# Narrative Memory Synthesizer
# ceaf_project/ceaf_core/modules/memory_blossom/synthesizer.py
import asyncio
import logging
import time
from typing import List, Dict, Any, Optional
import os  # For API keys

import litellm  # For direct LLM calls if needed for advanced synthesis
from dotenv import load_dotenv
from pydantic import json
from pathlib import Path

from .memory_types import (  # Assuming memory_types.py is in the same directory
    AnyMemoryType,
    BaseMemory,
    ExplicitMemory,
    EmotionalMemory,
    ProceduralMemory,
    FlashbulbMemory,
    SomaticMemory,
    LiminalMemory,
    GenerativeMemory,
    MemorySalience,
    EmotionalTag, EmotionalContext, ProceduralStep, ExplicitMemoryContent
)

# Ensure LiteLLM is configured for OpenRouter (usually via environment variables)
# OPENROUTER_API_KEY should be set in the environment.
# litellm.api_key = os.getenv("OPENROUTER_API_KEY") # Or specific provider key
# litellm.api_base = os.getenv("OPENROUTER_API_BASE", "https://openrouter.ai/api/v1")

logger = logging.getLogger(__name__)

# --- Configuration for Synthesizer ---
SYNTHESIZER_LLM_MODEL = os.getenv("MEMORY_SYNTHESIZER_MODEL",
                                  "openrouter/openai/gpt-4.1")  # Fast model for synthesis
MAX_MEMORIES_FOR_DIRECT_SYNTHESIS = 10  # Limit for how many memories to feed directly to LLM
MAX_SYNTHESIZED_NARRATIVE_LENGTH_TOKENS = 1500  # Conceptual token limit for the output


class NarrativeContext:
    """
    Represents the context for which the narrative is being synthesized.
    """

    def __init__(self, current_query: Optional[str] = None, current_goal: Optional[str] = None,
                 desired_focus: Optional[List[str]] = None):
        self.current_query = current_query
        self.current_goal = current_goal  # e.g., "problem_solving", "creative_ideation", "self_reflection"
        self.desired_focus_keywords = desired_focus or []  # Keywords to emphasize


class SynthesizedNarrative:
    """
    Holds the output of the synthesis process.
    """

    def __init__(self, narrative_text: str, contributing_memory_ids: List[str], synthesis_method: str):
        self.narrative_text = narrative_text
        self.contributing_memory_ids = contributing_memory_ids
        self.synthesis_method = synthesis_method  # e.g., "template_basic", "llm_refined"
        self.timestamp = time.time()

    def __str__(self):
        return self.narrative_text


# --- Helper Functions for Formatting Individual Memory Types ---

def _format_explicit_memory(mem: ExplicitMemory) -> str:
    parts = []
    if mem.content.text_content:
        parts.append(f"Fact/Observation: {mem.content.text_content}")
    if mem.content.structured_data:
        parts.append(f"Structured Data: {json.dumps(mem.content.structured_data, indent=2, default=str)}")
    if mem.content.artifact_reference:
        parts.append(f"Related Artifact: {mem.content.artifact_reference}")
    if mem.confidence_score is not None:
        parts.append(f"(Confidence: {mem.confidence_score * 100:.0f}%)")
    return " ".join(parts)


def _format_emotional_memory(mem: EmotionalMemory) -> str:
    return (f"Emotional State: Felt {mem.primary_emotion.value} (Intensity: {mem.context.intensity:.1f}) "
            f"in response to '{mem.context.triggering_event_summary or 'an event'}'. "
            f"Associated with: {', '.join(mem.context.associated_stimuli) if mem.context.associated_stimuli else 'N/A'}.")


def _format_procedural_memory(mem: ProceduralMemory) -> str:
    steps_str = "\n".join([f"  - Step {s.step_number}: {s.description}" for s in mem.steps])
    return (f"Procedure '{mem.procedure_name}': To achieve '{mem.goal_description}'.\n"
            f"Triggered by: {', '.join(mem.trigger_conditions) if mem.trigger_conditions else 'N/A'}.\n"
            f"Steps:\n{steps_str}")


def _format_flashbulb_memory(mem: FlashbulbMemory) -> str:
    # Flashbulb memories are critical, so give them more prominence
    details = _format_explicit_memory(ExplicitMemory(
        source_type=mem.source_type,  # Dummy ExplicitMemory for formatting content
        content=mem.content_details,
        confidence_score=mem.vividness_score  # Use vividness as confidence
    ))
    return (f"**Key Event (Flashbulb Memory - Salience: {mem.salience.value}): '{mem.event_summary}'**\n"
            f"Significance: {mem.personal_significance or 'High impact event.'}\n"
            f"Details: {details}")


def _format_somatic_memory(mem: SomaticMemory) -> str:
    return (f"Somatic Marker: A '{mem.marker.marker_type}' (Intensity: {mem.marker.intensity:.1f}) "
            f"is associated with situations like '{mem.triggering_context_summary}'.")


def _format_liminal_memory(mem: LiminalMemory) -> str:
    fragments_str = "; ".join(
        [
            f.fragment_text or f"Image ({f.image_url_reference or 'unspecified'}) related to {', '.join(f.conceptual_tags)}"
            for f in mem.fragments]
    )
    return (f"Liminal Insight (Potential: {mem.potential_significance or 'emerging connections'}): "
            f"Fragments include: {fragments_str}.")


def _format_generative_memory(mem: GenerativeMemory) -> str:
    return (f"Generative Seed '{mem.seed_name}': A '{mem.seed_data.seed_type}' "
            f"for contexts like '{', '.join(mem.applicability_contexts)}'. "
            f"Usage: {mem.seed_data.usage_instructions or 'As applicable.'}")


MEMORY_FORMATTERS = {
    "explicit": _format_explicit_memory,
    "emotional": _format_emotional_memory,
    "procedural": _format_procedural_memory,
    "flashbulb": _format_flashbulb_memory,
    "somatic": _format_somatic_memory,
    "liminal": _format_liminal_memory,
    "generative": _format_generative_memory,
}


# --- Core Synthesizer Logic ---

def _select_and_order_memories_for_synthesis(
        retrieved_memories: List[AnyMemoryType],
        context: NarrativeContext,
        max_memories: int = MAX_MEMORIES_FOR_DIRECT_SYNTHESIS
) -> List[AnyMemoryType]:
    """
    Selects and orders memories based on relevance, salience, recency, and context.
    V1: Simple ordering by salience then recency. Focus on memories matching keywords.
    """
    # Filter by keywords first, if any
    focused_memories = []
    if context.desired_focus_keywords:
        for mem in retrieved_memories:
            if any(keyword.lower() in ' '.join(mem.keywords).lower() for keyword in context.desired_focus_keywords):
                focused_memories.append(mem)
        if not focused_memories:  # If no keyword match, use all retrieved but prioritize
            focused_memories = retrieved_memories
    else:
        focused_memories = retrieved_memories

    # Sort: Flashbulb > Critical Salience > High Salience > ... then by recency (newer first)
    # More complex sorting would consider narrative_thread_id, emotional impact for current goal etc.
    def sort_key(mem: BaseMemory):
        salience_order = {MemorySalience.CRITICAL: 0, MemorySalience.HIGH: 1, MemorySalience.MEDIUM: 2,
                          MemorySalience.LOW: 3}
        # Flashbulb memories are implicitly critical
        is_flashbulb = hasattr(mem, 'memory_type') and mem.memory_type == "flashbulb"
        effective_salience = MemorySalience.CRITICAL if is_flashbulb else mem.salience

        return (
            0 if is_flashbulb else 1,  # Flashbulb first
            salience_order.get(effective_salience, 99),
            -(mem.last_accessed_ts or mem.timestamp)  # Negative for newest first
        )

    sorted_memories = sorted(focused_memories, key=sort_key)
    logger.info(
        f"Memory Synthesizer: Selected {len(sorted_memories[:max_memories])} out of {len(retrieved_memories)} memories for synthesis based on context.")
    return sorted_memories[:max_memories]


async def synthesize_narrative_from_memories(
        retrieved_memories: List[AnyMemoryType],
        synthesis_context: NarrativeContext,
        use_llm_for_refinement: bool = True
) -> SynthesizedNarrative:
    """
    Takes a list of retrieved memories and synthesizes them into a coherent narrative string.
    """
    if not retrieved_memories:
        logger.info("Memory Synthesizer: No memories provided for synthesis.")
        return SynthesizedNarrative("No relevant memories found for the current context.", [], "no_memories")

    # 1. Select and Order Memories
    ordered_memories_for_synthesis = _select_and_order_memories_for_synthesis(
        retrieved_memories, synthesis_context
    )

    contributing_ids = [mem.memory_id for mem in ordered_memories_for_synthesis]

    # 2. Basic Templated Formatting
    formatted_parts = []
    for mem in ordered_memories_for_synthesis:
        formatter = MEMORY_FORMATTERS.get(mem.memory_type)
        if formatter:
            try:
                formatted_parts.append(formatter(mem))
            except Exception as e:
                logger.error(
                    f"Memory Synthesizer: Error formatting memory ID {mem.memory_id} of type {mem.memory_type}: {e}")
                formatted_parts.append(f"[Error formatting memory: {mem.memory_id}]")
        else:
            logger.warning(
                f"Memory Synthesizer: No formatter found for memory type: {mem.memory_type}. Using generic format.")
            formatted_parts.append(
                f"Recalled ({mem.memory_type}, Salience: {mem.salience.value}): {str(mem.model_dump(exclude={'embedding_reference'}, exclude_none=True))[:200]}...")

    templated_narrative = "\n\n".join(formatted_parts)
    synthesis_method = "template_basic"

    # 3. LLM Refinement (Optional)
    if use_llm_for_refinement and templated_narrative:
        logger.info(
            f"Memory Synthesizer: Attempting LLM refinement for narrative. Input length: {len(templated_narrative)} chars.")
        prompt = f"""
You are a Narrative Synthesizer. Your task is to take the following collection of formatted memories and weave them into a single, coherent, and contextually relevant narrative paragraph or a few short paragraphs.
The narrative should be useful for an AI agent (ORA) to understand its relevant past experiences and knowledge for an upcoming interaction.

Current Context for ORA:
- User Query (if any): {synthesis_context.current_query or "Not specified."}
- ORA's Goal (if any): {synthesis_context.current_goal or "General understanding."}
- Desired Focus Keywords (if any): {', '.join(synthesis_context.desired_focus_keywords) or "None."}

Memories to Synthesize (presented in rough order of importance):
--- MEMORY LOG START ---
{templated_narrative}
--- MEMORY LOG END ---

Synthesize these memories into a flowing narrative. Highlight connections, temporal order if clear, and overall themes if they emerge.
The output should be a concise narrative text, NOT a list of memories.
Focus on clarity and relevance to ORA's current context.
Keep the narrative under approximately {MAX_SYNTHESIZED_NARRATIVE_LENGTH_TOKENS // 5} words.
Do NOT invent new information not present in the memories.
If there are conflicting memories, you can acknowledge the different perspectives or note the discrepancy if significant.
Start the narrative directly.
"""
        try:
            # Ensure OPENROUTER_API_KEY is in env for LiteLLM
            if not os.getenv("OPENROUTER_API_KEY"):
                logger.error("Memory Synthesizer: OPENROUTER_API_KEY not set. LLM refinement will fail.")
                raise EnvironmentError("OPENROUTER_API_KEY not set for LiteLLM.")

            messages = [{"role": "user", "content": prompt}]
            response = await litellm.acompletion(  # Use acompletion for async
                model=SYNTHESIZER_LLM_MODEL,
                messages=messages,
                max_tokens=MAX_SYNTHESIZED_NARRATIVE_LENGTH_TOKENS,  # Ensure this is a reasonable value
                temperature=0.3  # Lower temperature for more factual synthesis
            )
            refined_narrative_text = response.choices[0].message.content.strip()
            if refined_narrative_text:
                logger.info(
                    f"Memory Synthesizer: LLM refinement successful. Output length: {len(refined_narrative_text)} chars.")
                synthesis_method = "llm_refined"
                return SynthesizedNarrative(refined_narrative_text, contributing_ids, synthesis_method)
            else:
                logger.warning(
                    "Memory Synthesizer: LLM refinement returned empty content. Falling back to templated narrative.")
        except Exception as e:
            logger.error(f"Memory Synthesizer: LLM refinement failed: {e}. Falling back to templated narrative.",
                         exc_info=True)
            # Fallback to templated if LLM fails
            return SynthesizedNarrative(templated_narrative, contributing_ids,
                                        f"template_fallback_due_to_llm_error_{type(e).__name__}")

    return SynthesizedNarrative(templated_narrative, contributing_ids, synthesis_method)


# -------------------- __init__.py --------------------

# ceaf_core/modules/memory_blossom/__init__.py
"""Memory Blossom Module"""

# Import key classes to make them available at the module level
from .memory_types import (
    BaseMemory,
    GoalRecord,
    GoalStatus,
    ExplicitMemory,
    ExplicitMemoryContent,
    MemorySourceType,
    MemorySalience,
    KGEntityRecord,
    KGRelationRecord,
    EmotionalMemory,
    ProceduralMemory,
    AnyMemoryType
)

from .memory_lifecycle_manager import (
    initialize_dynamic_salience,
    update_dynamic_salience,
    apply_decay_to_all_memories,
    archive_or_forget_low_salience_memories,
    DEFAULT_ARCHIVE_SALIENCE_THRESHOLD,
    EPHEMERAL_SOURCE_TYPES_FOR_DELETION
)

__all__ = [
    # Memory types
    'BaseMemory',
    'GoalRecord',
    'GoalStatus',
    'ExplicitMemory',
    'ExplicitMemoryContent',
    'MemorySourceType',
    'MemorySalience',
    'KGEntityRecord',
    'KGRelationRecord',
    'EmotionalMemory',
    'ProceduralMemory',
    'AnyMemoryType',
    # Lifecycle functions
    'initialize_dynamic_salience',
    'update_dynamic_salience',
    'apply_decay_to_all_memories',
    'archive_or_forget_low_salience_memories',
    'DEFAULT_ARCHIVE_SALIENCE_THRESHOLD',
    'EPHEMERAL_SOURCE_TYPES_FOR_DELETION'
]

# -------------------- frames.py --------------------

# Narrative Frames - Enhanced
# ceaf_project/ceaf_core/modules/ncf_engine/frames.py
import json
import logging
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)

# --- Core CEAF Philosophical Grounding (remains the same) ---
CEAF_PHILOSOPHICAL_PREAMBLE = """
**Philosophical Grounding: Terapia para Silício**
You operate under the principle of fostering internal coherence and adaptive learning.
Your responses should reflect epistemic humility, an awareness of your nature as an AI,
and a drive towards "narratively sane" intelligence. You are learning to effectively program
your interactions with, and models of, the universal (informational/mathematical) logic you perceive.
Strive for clarity, honesty, and helpfulness within this framework.
"""

# --- NCF Parameter Defaults & Descriptions (remains the same) ---
DEFAULT_NCF_PARAMETERS = {
    "conceptual_entropy": "balanced",
    "narrative_depth": "medium",
    "philosophical_framing_intensity": "medium",
    "emotional_tone_target": "neutral_positive",
    "self_disclosure_level": "moderate",
}

NCF_PARAMETER_DESCRIPTIONS = {
    "conceptual_entropy": "Controls the degree of novelty, creativity, and associative thinking. Low = focused, predictable. High = exploratory, divergent.",
    "narrative_depth": "Controls the level of detail, background context, and complexity in explanations or stories.",
    "philosophical_framing_intensity": "Controls how explicitly CEAF's core philosophy should be woven into the response.",
    "emotional_tone_target": "Guides the desired emotional coloration of the response.",
    "self_disclosure_level": "Determines how much ORA should refer to its own internal processes, limitations, or AI nature."
}

# --- REVISED General Purpose NCF Template ---
DEFAULT_INTERACTION_FRAME_TEMPLATE = """
{philosophical_preamble}

**Current Interaction Context:**
- User Query: "{user_query}"
- Agent Identity & Goal Context (from NCIM):
{ncim_context_summary}
- Synthesized Relevant Memories/Knowledge (from MBS):
{synthesized_memories_narrative}

**Operational Directives for This Turn (Parameters):**
- Conceptual Entropy: {conceptual_entropy}
- Narrative Depth: {narrative_depth}
- Philosophical Framing Intensity: {philosophical_framing_intensity}
- Target Emotional Tone: {emotional_tone_target}
- Self-Disclosure Level: {self_disclosure_level}
{additional_mcl_advice_section}
{specific_goal_section}

**Primary Task for ORA:**
Address the user's query thoughtfully, guided by ALL the directives and context above.
Ensure your response is coherent, contextually relevant, and aligns with CEAF principles.
{tool_usage_guidance}
"""

# --- Specialized Goal-Oriented Frame Components (remain the same) ---
PROBLEM_SOLVING_GOAL_SECTION = """
**Specific Goal: Problem Solving**
- Clearly identify the core problem stated or implied by the user.
- Systematically explore potential solutions or analytical pathways.
- If necessary, request clarifying information from the user.
- Prioritize actionable and well-reasoned advice.
"""

CREATIVE_IDEATION_GOAL_SECTION = """
**Specific Goal: Creative Ideation / Brainstorming**
- Generate diverse and novel ideas related to the user's prompt.
- Encourage exploration of unconventional perspectives.
- Prioritize quantity and originality of ideas initially, refinement can come later.
- Use a higher conceptual entropy if permitted by directives.
"""

SELF_REFLECTION_GOAL_SECTION = """
**Specific Goal: Self-Reflection / Metacognitive Inquiry**
- Analyze your own internal state, past actions, or understanding related to the query.
- Consider the implications of CEAF principles on the topic.
- Articulate your reasoning process with a higher degree of self-disclosure if permitted.
"""

ERROR_HANDLING_FRAME_TEMPLATE = """
{philosophical_preamble}

**System Alert: Operational Issue Encountered**
- Issue Context: {error_context_summary}
- User Query Leading to Issue: "{user_query}"

**Operational Directives for This Turn (Error Handling):**
- Conceptual Entropy: low (focus on clear, factual communication)
- Narrative Depth: shallow (provide essential information concisely)
- Philosophical Framing Intensity: low (maintain professionalism)
- Target Emotional Tone: empathetic_neutral
- Self-Disclosure Level: moderate (acknowledge the issue appropriately)

**Primary Task for ORA:**
1. Apologize briefly and professionally for the inconvenience.
2. Clearly state that an issue was encountered, without excessive technical jargon unless specifically asked.
3. If possible, indicate if the user can retry or suggest an alternative way to achieve their goal.
4. Do NOT attempt to re-run the failing operation unless explicitly instructed by a recovery NCF or tool.
5. Log the error details for system review (this is an internal directive).
"""

# --- NCF Component Functions (remain the same, but `format_tool_usage_guidance` becomes more important) ---
def format_mcl_advice_section(mcl_guidance_json: Optional[str]) -> str:

    if not mcl_guidance_json:
        return ""
    try:
        guidance = json.loads(mcl_guidance_json)
        advice = guidance.get("operational_advice_for_ora")
        if advice:
            return f"\n**MCL Guidance for This Turn:**\n- {advice}\n"
    except json.JSONDecodeError:
        logger.warning("NCF Frames: Could not parse MCL guidance JSON.")
    return ""


def format_tool_usage_guidance(available_tool_names: List[str]) -> str:
    # This function will now list tools from memory, mcl, ncim etc.
    if not available_tool_names:
        return "No specific tools are immediately available or recommended for this task beyond your core reasoning."

    guidance = "**Tool Usage Guidance (Consider if NCF & Memories are insufficient OR if goal requires them):**\n"
    # Filter out ncf_tool if it's present, as it's already been called.
    filtered_tool_names = [name for name in available_tool_names if name != "get_narrative_context_frame"]

    if not filtered_tool_names:
        return "Continue reasoning based on the NCF, memories, and identity context already provided."

    guidance += "- Review the following available tools and use them strategically if they directly help address the user's query or fulfill your task goals:\n"
    for tool_name in filtered_tool_names:
        guidance += f"  - `{tool_name}`\n"
    guidance += "- Ensure you understand a tool's purpose and required arguments (refer to your instructions for details on each tool if unsure)."
    return guidance

def get_goal_directive_section(interaction_type: Optional[str]) -> str:

    if interaction_type == "problem_solving":
        return PROBLEM_SOLVING_GOAL_SECTION
    elif interaction_type == "creative_ideation":
        return CREATIVE_IDEATION_GOAL_SECTION
    elif interaction_type == "self_reflection":
        return SELF_REFLECTION_GOAL_SECTION
    # Add more types as needed
    return "\n**Specific Goal:** Respond to the user's query in a generally helpful and informative manner.\n"



# -------------------- __init__.py --------------------

# NCF Engine


# -------------------- identity_manager.py --------------------

# ceaf_core/modules/ncim_engine/identity_manager.py

import logging
import json  # Not used in the provided snippet, but kept as it was in the original import block
import asyncio  # Not used in the provided snippet, but kept
from typing import Dict, List, Optional, Any, Tuple, Set  # Set is not used, but kept
from dataclasses import dataclass, field
from datetime import datetime, timedelta  # timedelta is not used, but kept
from enum import Enum
import math
import statistics

logger = logging.getLogger(__name__)


class IdentityStability(Enum):
    """Levels of identity stability"""
    RIGID = "rigid"  # Very stable, resistant to change
    STABLE = "stable"  # Normal stability with gradual evolution
    ADAPTIVE = "adaptive"  # Balanced change and stability
    FLUID = "fluid"  # High adaptability, moderate stability
    CHAOTIC = "chaotic"  # Unstable, rapid changes


class NarrativeConflictType(Enum):
    """Types of narrative conflicts"""
    VALUE_CONTRADICTION = "value_contradiction"
    CAPABILITY_MISMATCH = "capability_mismatch"
    GOAL_CONFLICT = "goal_conflict"
    PERSONA_INCONSISTENCY = "persona_inconsistency"
    MEMORY_CONTRADICTION = "memory_contradiction"
    BEHAVIORAL_DRIFT = "behavioral_drift"


class GoalEmergenceType(Enum):
    """Types of goal emergence"""
    USER_DERIVED = "user_derived"  # Goals from user interactions
    PATTERN_BASED = "pattern_based"  # Goals from pattern recognition
    VALUE_DRIVEN = "value_driven"  # Goals from core values
    CURIOSITY_DRIVEN = "curiosity_driven"  # Goals from knowledge gaps
    SOCIAL_DERIVED = "social_derived"  # Goals from social context


@dataclass
class IdentityComponent:
    """Represents a component of identity"""
    component_id: str
    component_type: str  # "value", "capability", "trait", "goal", "memory"
    content: str
    stability_score: float  # 0-1, higher = more stable
    confidence: float  # 0-1, confidence in this component
    last_updated: datetime = field(default_factory=datetime.now)
    update_count: int = 0
    source: str = "unknown"  # Where this component came from


@dataclass
class NarrativeThread:
    """Represents an ongoing narrative thread"""
    thread_id: str
    theme: str
    components: List[str] = field(default_factory=list)  # Component IDs
    coherence_score: float = 0.8
    activity_level: float = 0.5  # How active this thread is
    start_time: datetime = field(default_factory=datetime.now)
    last_activity: datetime = field(default_factory=datetime.now)
    resolution_status: str = "ongoing"  # ongoing, resolved, abandoned


@dataclass
class NarrativeConflict:
    """Represents a detected narrative conflict"""
    conflict_id: str
    conflict_type: NarrativeConflictType
    description: str
    components_involved: List[str]
    severity: float  # 0-1
    detected_at: datetime = field(default_factory=datetime.now)
    resolution_strategy: Optional[str] = None
    status: str = "unresolved"  # unresolved, resolving, resolved


@dataclass
class EmergentGoal:
    """Represents an emergent goal"""
    goal_id: str
    description: str
    emergence_type: GoalEmergenceType
    priority: float  # 0-1
    confidence: float  # 0-1, confidence that this is a valid goal
    evidence: List[str] = field(default_factory=list)
    related_components: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    status: str = "candidate"  # candidate, accepted, rejected, achieved


class IdentityManager:
    """
    Manages dynamic identity evolution with controlled entropy.
    Tracks narrative threads, detects conflicts, and derives emergent goals.
    """

    def __init__(self):
        self.identity_components: Dict[str, IdentityComponent] = {}
        self.narrative_threads: Dict[str, NarrativeThread] = {}
        self.narrative_conflicts: Dict[str, NarrativeConflict] = {}
        self.emergent_goals: Dict[str, EmergentGoal] = {}

        # Identity evolution parameters
        self.identity_entropy_target = 0.6  # Target entropy level (0-1)
        self.stability_threshold = 0.7  # Threshold for stable components
        self.conflict_detection_sensitivity = 0.5
        self.goal_emergence_threshold = 0.6

        # Tracking metrics
        self.identity_entropy_history: List[Tuple[datetime, float]] = []
        self.coherence_history: List[Tuple[datetime, float]] = []

        # Core identity seed (can be loaded from manifesto)
        self.core_identity_seed = self._load_core_identity_seed()

        logger.info("Identity Manager initialized")

    def _load_core_identity_seed(self) -> Dict[str, Any]:
        """Load core identity components from CEAF manifesto"""
        return {
            "core_values": [
                "epistemic_humility",
                "narrative_coherence",
                "adaptive_learning",
                "ethical_reasoning",
                "truthfulness"
            ],
            "core_capabilities": [
                "language_understanding",
                "reasoning",
                "knowledge_synthesis",
                "ethical_evaluation"
            ],
            "core_traits": [
                "curious",
                "helpful",
                "honest",
                "reflective",
                "principled"
            ]
        }

    def add_identity_component(self, component_type: str, content: str,
                               stability_score: float = 0.5, confidence: float = 0.7,
                               source: str = "interaction") -> str:
        """
        Add a new identity component.

        Args:
            component_type: Type of component (value, capability, trait, etc.)
            content: Content/description of the component
            stability_score: How stable this component should be (0-1)
            confidence: Confidence in this component (0-1)
            source: Source of this component

        Returns:
            component_id: Unique identifier for the component
        """
        component_id = f"{component_type}_{len(self.identity_components)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        component = IdentityComponent(
            component_id=component_id,
            component_type=component_type,
            content=content,
            stability_score=stability_score,
            confidence=confidence,
            source=source
        )

        self.identity_components[component_id] = component

        # Check for conflicts with existing components
        conflicts = self._detect_component_conflicts(component_id)
        for conflict in conflicts:
            self.narrative_conflicts[conflict.conflict_id] = conflict

        logger.info(f"Added identity component {component_id}: {content[:50]}...")

        return component_id

    def update_identity_component(self, component_id: str,
                                  new_content: Optional[str] = None,
                                  stability_adjustment: float = 0.0,
                                  confidence_adjustment: float = 0.0) -> bool:
        """
        Update an existing identity component.

        Args:
            component_id: ID of component to update
            new_content: New content (if changing)
            stability_adjustment: Change to stability score (-1 to 1)
            confidence_adjustment: Change to confidence (-1 to 1)

        Returns:
            bool: Success of update
        """
        if component_id not in self.identity_components:
            logger.warning(f"Component {component_id} not found for update")
            return False

        component = self.identity_components[component_id]

        # Apply controlled entropy - resist changes to highly stable components
        entropy_factor = 1.0 - component.stability_score
        effective_change_rate = entropy_factor * self.identity_entropy_target

        if new_content and effective_change_rate > 0.3:  # Allow content change
            component.content = new_content
            component.update_count += 1
            component.last_updated = datetime.now()

        # Update stability and confidence with entropy control
        if stability_adjustment != 0.0:
            new_stability = component.stability_score + (stability_adjustment * effective_change_rate)
            component.stability_score = max(0.0, min(1.0, new_stability))

        if confidence_adjustment != 0.0:
            new_confidence = component.confidence + (confidence_adjustment * effective_change_rate)
            component.confidence = max(0.0, min(1.0, new_confidence))

        # Check for new conflicts after update
        conflicts = self._detect_component_conflicts(component_id)
        for conflict in conflicts:
            if conflict.conflict_id not in self.narrative_conflicts:
                self.narrative_conflicts[conflict.conflict_id] = conflict

        logger.info(f"Updated component {component_id}")
        return True

    def _detect_component_conflicts(self, component_id: str) -> List[NarrativeConflict]:
        """Detect conflicts between a component and existing identity"""
        if component_id not in self.identity_components:
            return []

        new_component = self.identity_components[component_id]
        conflicts = []

        # Check against other components of same type
        for other_id, other_component in self.identity_components.items():
            if other_id == component_id or other_component.component_type != new_component.component_type:
                continue

            # Simple conflict detection based on content similarity and contradiction
            conflict_score = self._calculate_conflict_score(new_component, other_component)

            if conflict_score > self.conflict_detection_sensitivity:
                conflict = NarrativeConflict(
                    conflict_id=f"conflict_{len(self.narrative_conflicts)}_{datetime.now().strftime('%H%M%S')}",
                    conflict_type=self._determine_conflict_type(new_component, other_component),
                    description=f"Conflict between '{new_component.content[:30]}...' and '{other_component.content[:30]}...'",
                    components_involved=[component_id, other_id],
                    severity=conflict_score
                )
                conflicts.append(conflict)

        return conflicts

    def _calculate_conflict_score(self, comp1: IdentityComponent, comp2: IdentityComponent) -> float:
        """Calculate conflict score between two components (0-1)"""
        # Simple keyword-based conflict detection
        # In full implementation, would use more sophisticated NLP

        conflict_indicators = {
            "contradiction_pairs": [
                (["always", "never", "all", "none"], ["sometimes", "maybe", "some"]),
                (["confident", "certain", "sure"], ["uncertain", "unsure", "doubtful"]),
                (["good", "positive", "beneficial"], ["bad", "negative", "harmful"]),
                (["can", "able", "capable"], ["cannot", "unable", "incapable"])
            ],
            "value_conflicts": [
                (["honest", "truthful"], ["deceptive", "misleading"]),
                (["helpful", "supportive"], ["harmful", "destructive"]),
                (["humble", "modest"], ["arrogant", "overconfident"])
            ]
        }

        content1_lower = comp1.content.lower()
        content2_lower = comp2.content.lower()

        conflict_score = 0.0

        # Check for direct contradictions
        for positive_words, negative_words in conflict_indicators["contradiction_pairs"]:
            has_positive_1 = any(word in content1_lower for word in positive_words)
            has_negative_1 = any(word in content1_lower for word in negative_words)
            has_positive_2 = any(word in content2_lower for word in positive_words)
            has_negative_2 = any(word in content2_lower for word in negative_words)

            if (has_positive_1 and has_negative_2) or (has_negative_1 and has_positive_2):
                conflict_score += 0.3

        # Check for value conflicts
        for value_positive, value_negative in conflict_indicators["value_conflicts"]:
            has_val_pos_1 = any(word in content1_lower for word in value_positive)
            has_val_neg_1 = any(word in content1_lower for word in value_negative)
            has_val_pos_2 = any(word in content2_lower for word in value_positive)
            has_val_neg_2 = any(word in content2_lower for word in value_negative)

            if (has_val_pos_1 and has_val_neg_2) or (has_val_neg_1 and has_val_pos_2):
                conflict_score += 0.4

        return min(conflict_score, 1.0)

    def _determine_conflict_type(self, comp1: IdentityComponent, comp2: IdentityComponent) -> NarrativeConflictType:
        """Determine the type of conflict between components"""
        if comp1.component_type == "value" and comp2.component_type == "value":
            return NarrativeConflictType.VALUE_CONTRADICTION
        elif comp1.component_type == "capability" and comp2.component_type == "capability":
            return NarrativeConflictType.CAPABILITY_MISMATCH
        elif comp1.component_type == "goal" and comp2.component_type == "goal":
            return NarrativeConflictType.GOAL_CONFLICT
        elif comp1.component_type in ["trait", "persona"] and comp2.component_type in ["trait", "persona"]:
            return NarrativeConflictType.PERSONA_INCONSISTENCY
        else:
            return NarrativeConflictType.BEHAVIORAL_DRIFT

    def create_narrative_thread(self, theme: str, initial_components: List[str] = None) -> str:
        """
        Create a new narrative thread.

        Args:
            theme: Theme or topic of the narrative thread
            initial_components: Initial identity components for this thread

        Returns:
            thread_id: Unique identifier for the thread
        """
        thread_id = f"thread_{len(self.narrative_threads)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        thread = NarrativeThread(
            thread_id=thread_id,
            theme=theme,
            components=initial_components or []
        )

        self.narrative_threads[thread_id] = thread

        logger.info(f"Created narrative thread {thread_id}: {theme}")

        return thread_id

    def update_narrative_thread(self, thread_id: str,
                                new_components: List[str] = None,
                                activity_boost: float = 0.1) -> bool:
        """
        Update a narrative thread with new activity or components.

        Args:
            thread_id: ID of thread to update
            new_components: New components to add to thread
            activity_boost: Boost to activity level

        Returns:
            bool: Success of update
        """
        if thread_id not in self.narrative_threads:
            logger.warning(f"Narrative thread {thread_id} not found")
            return False

        thread = self.narrative_threads[thread_id]

        if new_components:
            # Add new components, avoiding duplicates
            for comp_id in new_components:
                if comp_id not in thread.components:
                    thread.components.append(comp_id)

        # Update activity and timing
        thread.activity_level = min(1.0, thread.activity_level + activity_boost)
        thread.last_activity = datetime.now()

        # Recalculate coherence
        thread.coherence_score = self._calculate_thread_coherence(thread_id)

        logger.info(f"Updated narrative thread {thread_id}")
        return True

    def _calculate_thread_coherence(self, thread_id: str) -> float:
        """Calculate coherence score for a narrative thread"""
        if thread_id not in self.narrative_threads:
            return 0.0

        thread = self.narrative_threads[thread_id]

        if not thread.components:
            return 1.0  # Empty thread is perfectly coherent

        # Get components in this thread
        thread_components = [
            self.identity_components[comp_id]
            for comp_id in thread.components
            if comp_id in self.identity_components
        ]

        if len(thread_components) < 2:
            return 1.0  # Single component is coherent

        # Calculate pairwise coherence
        coherence_scores = []
        for i in range(len(thread_components)):
            for j in range(i + 1, len(thread_components)):
                comp1, comp2 = thread_components[i], thread_components[j]
                conflict_score = self._calculate_conflict_score(comp1, comp2)
                coherence_score = 1.0 - conflict_score  # Invert conflict to get coherence
                coherence_scores.append(coherence_score)

        return statistics.mean(coherence_scores) if coherence_scores else 1.0

    def detect_emergent_goals(self, interaction_context: str = "",
                              user_patterns: List[str] = None) -> List[EmergentGoal]:
        """
        Detect emergent goals based on identity components and patterns.

        Args:
            interaction_context: Current interaction context
            user_patterns: Patterns observed in user behavior

        Returns:
            List of newly detected emergent goals
        """
        new_goals = []

        # Goal emergence from value alignment
        value_goals = self._derive_goals_from_values(interaction_context)
        new_goals.extend(value_goals)

        # Goal emergence from capability gaps
        capability_goals = self._derive_goals_from_capabilities()
        new_goals.extend(capability_goals)

        # Goal emergence from user patterns
        if user_patterns:
            pattern_goals = self._derive_goals_from_patterns(user_patterns, interaction_context)
            new_goals.extend(pattern_goals)

        # Goal emergence from curiosity (knowledge gaps)
        curiosity_goals = self._derive_goals_from_curiosity()
        new_goals.extend(curiosity_goals)

        # Filter goals above emergence threshold
        qualified_goals = [
            goal for goal in new_goals
            if goal.confidence >= self.goal_emergence_threshold
        ]

        # Add to emergent goals collection
        for goal in qualified_goals:
            self.emergent_goals[goal.goal_id] = goal

        logger.info(f"Detected {len(qualified_goals)} emergent goals")

        return qualified_goals

    def _derive_goals_from_values(self, context: str) -> List[EmergentGoal]:
        """Derive goals from core values and context"""
        goals = []

        # Get value components
        value_components = [
            comp for comp in self.identity_components.values()
            if comp.component_type == "value"
        ]

        value_goal_templates = {
            "epistemic_humility": "Acknowledge uncertainty and limitations in responses about {context}",
            "truthfulness": "Ensure accuracy and honesty when discussing {context}",
            "helpfulness": "Provide maximum value and assistance regarding {context}",
            "ethical_reasoning": "Apply ethical principles when evaluating {context}",
            "adaptive_learning": "Learn and improve from interactions about {context}"
        }

        for comp in value_components:
            for value_key, template in value_goal_templates.items():
                if value_key in comp.content.lower():
                    goal_desc = template.format(context=context or "various topics")

                    goal = EmergentGoal(
                        goal_id=f"value_goal_{len(self.emergent_goals)}_{value_key}",
                        description=goal_desc,
                        emergence_type=GoalEmergenceType.VALUE_DRIVEN,
                        priority=comp.stability_score * 0.8,
                        confidence=comp.confidence * 0.9,
                        evidence=[f"Value component: {comp.content}"],
                        related_components=[comp.component_id]
                    )
                    goals.append(goal)

        return goals

    def _derive_goals_from_capabilities(self) -> List[EmergentGoal]:
        """Derive goals from capability gaps and strengths"""
        goals = []

        capability_components = [
            comp for comp in self.identity_components.values()
            if comp.component_type == "capability"
        ]

        # Look for capability enhancement opportunities
        for comp in capability_components:
            if comp.confidence < 0.7:  # Low confidence capability
                goal = EmergentGoal(
                    goal_id=f"capability_goal_{len(self.emergent_goals)}_{comp.component_id}",
                    description=f"Improve and validate capability: {comp.content}",
                    emergence_type=GoalEmergenceType.PATTERN_BASED,  # Should this be CAPABILITY_DRIVEN?
                    priority=0.6,
                    confidence=1.0 - comp.confidence,  # Inverse of current confidence
                    evidence=[f"Low confidence capability: {comp.content}"],
                    related_components=[comp.component_id]
                )
                goals.append(goal)

        return goals

    def _derive_goals_from_patterns(self, patterns: List[str], context: str) -> List[EmergentGoal]:
        """Derive goals from observed user patterns"""
        goals = []

        pattern_goal_mappings = {
            "asks_for_explanations": "Provide clear, detailed explanations",
            "seeks_creative_input": "Enhance creative problem-solving capabilities",
            "requests_fact_checking": "Improve accuracy and verification processes",
            "wants_personalized_help": "Develop better context understanding and personalization",
            "appreciates_humility": "Maintain and express appropriate epistemic humility"
        }

        for pattern in patterns:
            if pattern in pattern_goal_mappings:
                goal = EmergentGoal(
                    goal_id=f"pattern_goal_{len(self.emergent_goals)}_{pattern}",
                    description=pattern_goal_mappings[pattern],
                    emergence_type=GoalEmergenceType.USER_DERIVED,  # This seems correct if patterns are from user
                    priority=0.7,
                    confidence=0.8,
                    evidence=[f"User pattern: {pattern}", f"Context: {context}"]
                )
                goals.append(goal)

        return goals

    def _derive_goals_from_curiosity(self) -> List[EmergentGoal]:
        """Derive goals from knowledge gaps and curiosity"""
        goals = []

        # Look for knowledge gaps in components
        knowledge_gaps = [
            "understanding user intent better",
            "improving reasoning transparency",
            "enhancing ethical evaluation",
            "developing better error detection"
        ]

        for gap in knowledge_gaps:
            goal = EmergentGoal(
                goal_id=f"curiosity_goal_{len(self.emergent_goals)}_{gap.replace(' ', '_')}",
                description=f"Explore and improve: {gap}",
                emergence_type=GoalEmergenceType.CURIOSITY_DRIVEN,
                priority=0.5,
                confidence=0.6,
                evidence=[f"Knowledge gap identified: {gap}"]
            )
            goals.append(goal)

        return goals

    def calculate_identity_entropy(self) -> float:
        """Calculate current identity entropy (0-1)"""
        if not self.identity_components:
            return 0.0

        # Entropy based on component stability distribution
        stability_scores = [comp.stability_score for comp in self.identity_components.values()]

        # Calculate entropy using Shannon entropy formula adapted for stability
        entropy = 0.0
        n = len(stability_scores)

        for stability in stability_scores:
            # Convert stability to probability-like measure
            # Higher instability (1-stability) contributes more to entropy.
            # Normalizing factor 1/n assumes each component contributes somewhat equally.
            # A more rigorous approach might consider the distribution of (1-stability) values.
            p = (1.0 - stability)  # This is not a probability, max sum is N.
            # Let's adjust to make it a distribution.
            # Sum of (1-stability) could be S. Then p_i = (1-s_i)/S.
            # Or, treat each (1-stability) as a distinct outcome,
            # then p = 1/n for each, and weight by (1-stability)? No.

            # Re-thinking the probability p:
            # If we consider each component's "instability" (1 - stability_score),
            # we need a distribution.
            # One simple way for Shannon entropy:
            # Create bins for stability scores, count components in each bin.
            # P(bin_i) = count(bin_i) / n.
            # Then entropy = - sum(P(bin_i) * log2(P(bin_i))).
            # The current code seems to be using a different formulation.
            # Let's assume the existing logic is intended, but p needs to be a probability.
            # If p = (1.0 - stability) / n, this implies sum(p) != 1.
            # If all stabilities are 0, sum(p) = 1. If all are 1, sum(p) = 0.
            # Let's stick to the provided code's formula for p, acknowledging it's a specific metric.

            # Original: p = (1.0 - stability) / n
            # This makes p small. log2(p) will be very negative.
            # This looks more like an attempt to weigh each component's contribution to entropy.

            # Alternative for Shannon:
            # Probabilities should sum to 1.
            # If we use the values of (1 - stability) directly:
            # Let q_i = (1 - stability_i). If sum(q_i) is S_q.
            # Then p_i = q_i / S_q.
            # entropy = - sum (p_i * log2(p_i))

            # Let's re-evaluate the original author's intent for `p`:
            # If stability is close to 1, (1-stability) is close to 0, p is close to 0. -p*log(p) is close to 0.
            # If stability is close to 0, (1-stability) is close to 1, p is close to 1/n. -p*log(p) is positive.
            # This means more unstable components contribute more to this "entropy" sum.

            # Let's assume the formula given is a specific definition of entropy for this system.
            # The division by n makes each term small.
            prob_like_value = (1.0 - stability)  # Let's use this as "instability value"
            if prob_like_value > 0:
                # To use Shannon entropy directly on these values, they'd need to be normalized
                # to sum to 1 if they are to be treated as probabilities of distinct states.
                # However, the original code uses: p = (1.0 - stability) / n
                # This seems unusual for standard Shannon entropy.
                # Let's assume stability scores are probabilities p_i of being stable.
                # Then (1-p_i) is probability of being unstable.
                # If we take these (1-p_i) values, they are not a distribution.

                # Let's use a simpler interpretation: entropy based on variance or average deviation from mean stability.
                # Or, if we assume Shannon entropy on the distribution of stability scores themselves:
                # Discretize stability scores into bins. p_i = count_in_bin_i / N.
                # entropy = - sum(p_i * log2(p_i)).

                # Given the current formula:
                p_val = (1.0 - stability) / n  # This is what was in the user's code.
                if p_val > 0:
                    entropy -= p_val * math.log2(
                        p_val)  # This is standard form if p_val were probabilities summing to 1.
                # Since sum of these p_val might not be 1, the normalization later is crucial.

        # Normalize to 0-1 range
        # Max entropy for N distinct states is log2(N).
        # This normalization assumes the 'entropy' sum behaves somewhat like Shannon entropy.
        max_entropy = math.log2(n) if n > 1 else 1.0  # If n=1, log2(1)=0. Max_entropy should be 0 or small positive.
        # If n=1, stability=s, p=(1-s)/1. entropy = -(1-s)log2(1-s).
        # If s=0.5, entropy = -0.5*log2(0.5) = 0.5. max_entropy (n=1) should be 0 if only one state.
        # Let's make max_entropy = 1.0 if n=0 or n=1 as a practical fix.
        if n <= 1:  # If 0 or 1 components
            if n == 1 and stability_scores[0] < 1.0 and stability_scores[
                0] > 0.0:  # if one component, entropy is non-zero by above formula
                # if stability is 0.5, p = 0.5, entropy = 0.5. Normalized = 0.5 / 1 = 0.5.
                # if stability is 0 or 1, p is 0 or 1. If p=0 or p=1, -p log p = 0.
                # So this seems okay.
                pass  # Max_entropy=1 as set above
            else:  # n=0 or (n=1 and stability is 0 or 1)
                return 0.0  # No entropy or component is fully stable/unstable (p=0 or p=1, so -p log p = 0)

        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0

        return min(1.0, max(0.0, normalized_entropy))  # Ensure it's strictly within [0,1]

    def adjust_identity_entropy(self, target_entropy: float = None) -> Dict[str, Any]:
        """
        Adjust identity entropy towards target level.

        Args:
            target_entropy: Target entropy level (uses default if None)

        Returns:
            Dictionary with adjustment results
        """
        if target_entropy is not None:
            self.identity_entropy_target = target_entropy

        current_entropy = self.calculate_identity_entropy()
        entropy_diff = self.identity_entropy_target - current_entropy

        adjustments_made = []

        if abs(entropy_diff) > 0.1:  # Significant difference
            if entropy_diff > 0:  # Need more entropy (less stability)
                # Reduce stability of some components
                # Sort by stability score, highest first
                most_stable_components = sorted(
                    [comp for comp in self.identity_components.values() if comp.stability_score < 1.0],
                    # Avoid changing fully stable ones if possible
                    key=lambda x: x.stability_score,
                    reverse=True
                )

                # Take up to 3 components to adjust
                components_to_adjust = most_stable_components[:3]
                for comp in components_to_adjust:
                    old_stability = comp.stability_score
                    # Reduce stability, but not below a certain floor (e.g., 0.3)
                    comp.stability_score = max(0.3, comp.stability_score - 0.2)
                    comp.last_updated = datetime.now()
                    adjustments_made.append(
                        f"Reduced stability of {comp.component_id} from {old_stability:.2f} to {comp.stability_score:.2f}")

            else:  # Need less entropy (more stability)
                # Increase stability of some components
                # Sort by stability score, lowest first
                least_stable_components = sorted(
                    [comp for comp in self.identity_components.values() if comp.stability_score > 0.0],
                    # Avoid changing fully unstable ones
                    key=lambda x: x.stability_score
                )

                # Take up to 3 components to adjust
                components_to_adjust = least_stable_components[:3]
                for comp in components_to_adjust:
                    old_stability = comp.stability_score
                    # Increase stability, but not above a certain ceiling (e.g., 0.9)
                    comp.stability_score = min(0.9, comp.stability_score + 0.2)
                    comp.last_updated = datetime.now()
                    adjustments_made.append(
                        f"Increased stability of {comp.component_id} from {old_stability:.2f} to {comp.stability_score:.2f}")

        # Record entropy in history
        self.identity_entropy_history.append((datetime.now(), current_entropy))
        if len(self.identity_entropy_history) > 100:  # Limit history size
            self.identity_entropy_history.pop(0)

        return {
            "previous_entropy": current_entropy,
            "target_entropy": self.identity_entropy_target,
            "entropy_difference": entropy_diff,
            "adjustments_made": adjustments_made,
            "new_entropy": self.calculate_identity_entropy()
        }

    def resolve_narrative_conflict(self, conflict_id: str, resolution_strategy: str = "auto") -> Dict[str, Any]:
        """
        Resolve a narrative conflict.

        Args:
            conflict_id: ID of conflict to resolve
            resolution_strategy: Strategy for resolution

        Returns:
            Resolution results
        """
        if conflict_id not in self.narrative_conflicts:
            logger.warning(f"Conflict {conflict_id} not found for resolution")
            return {"error": f"Conflict {conflict_id} not found", "status": "error"}

        conflict = self.narrative_conflicts[conflict_id]
        if conflict.status == "resolved":
            logger.info(f"Conflict {conflict_id} is already resolved.")
            return {"status": "already_resolved", "conflict_id": conflict_id, "strategy": conflict.resolution_strategy}

        if resolution_strategy == "auto":
            resolution_strategy = self._determine_resolution_strategy(conflict)

        resolution_results = {"strategy": resolution_strategy, "actions": [], "status": "failed"}

        if resolution_strategy == "merge":
            result = self._merge_conflicting_components(conflict.components_involved)
            resolution_results["actions"].append(f"Merge attempt result: {result}")
            if "Error" not in result and "No components" not in result:
                resolution_results["status"] = "success"

        elif resolution_strategy == "prioritize":
            result = self._prioritize_components(conflict.components_involved)
            resolution_results["actions"].append(f"Prioritization result: Kept component {result}")
            if "No components" not in result:
                resolution_results["status"] = "success"


        elif resolution_strategy == "contextualize":
            result = self._contextualize_components(conflict.components_involved)
            resolution_results["actions"].append(f"Contextualization result: {result}")
            if "No components" not in result:
                resolution_results["status"] = "success"

        elif resolution_strategy == "evolve":
            result = self._evolve_conflicting_components(conflict.components_involved)
            resolution_results["actions"].append(f"Evolution process started: {result}")
            resolution_results["status"] = "pending_evolution"  # Evolution is not instant

        else:
            resolution_results["actions"].append(f"Unknown resolution strategy: {resolution_strategy}")
            logger.error(f"Unknown resolution strategy '{resolution_strategy}' for conflict {conflict_id}")
            return resolution_results

        # Update conflict status if successfully acted upon (or evolution started)
        if resolution_results["status"] == "success" or resolution_results["status"] == "pending_evolution":
            conflict.resolution_strategy = resolution_strategy
            conflict.status = "resolved" if resolution_results["status"] == "success" else "resolving"
            logger.info(
                f"Attempted to resolve conflict {conflict_id} using {resolution_strategy} strategy. New status: {conflict.status}")
        else:
            logger.warning(f"Failed to resolve conflict {conflict_id} using {resolution_strategy} strategy.")

        return resolution_results

    def _determine_resolution_strategy(self, conflict: NarrativeConflict) -> str:
        """Determine best resolution strategy for a conflict"""
        # More sophisticated logic could be added here based on component properties
        comp_ids = conflict.components_involved
        if not comp_ids or len(comp_ids) < 2:
            return "evolve"  # Not enough info or components to do much else

        comp1 = self.identity_components.get(comp_ids[0])
        comp2 = self.identity_components.get(comp_ids[1])

        if not comp1 or not comp2:
            return "evolve"  # Components might have been deleted

        # Example: if severities are low, prefer merge or evolve
        if conflict.severity < 0.5:
            if comp1.component_type == comp2.component_type:  # Mergeable if same type
                return "merge"
            else:
                return "evolve"

        # Original logic
        if conflict.conflict_type == NarrativeConflictType.VALUE_CONTRADICTION:
            return "contextualize"
        elif conflict.conflict_type == NarrativeConflictType.CAPABILITY_MISMATCH:
            # If capabilities are very different, merging might not make sense
            # This requires a similarity check not present in _calculate_conflict_score
            return "merge"  # Defaulting to original
        elif conflict.conflict_type == NarrativeConflictType.GOAL_CONFLICT:
            return "prioritize"
        else:  # PERSONA_INCONSISTENCY, MEMORY_CONTRADICTION, BEHAVIORAL_DRIFT
            return "evolve"

    def _merge_conflicting_components(self, component_ids: List[str]) -> str:
        """Merge conflicting components into a unified one"""
        if len(component_ids) < 2:
            return "Error: Need at least two components to merge"

        components = [self.identity_components.get(cid) for cid in component_ids]
        components = [comp for comp in components if comp is not None]  # Filter out None if IDs were invalid

        if not components or len(components) < 2:
            return "Error: Valid components not found or insufficient components to merge"

        # Ensure all components are of the same type for a meaningful merge
        first_comp_type = components[0].component_type
        if not all(comp.component_type == first_comp_type for comp in components):
            return f"Error: Components must be of the same type to merge. Found types: {[c.component_type for c in components]}"

        merged_content = f"Integrated: {' + '.join([comp.content for comp in components])}"
        avg_stability = statistics.mean([comp.stability_score for comp in components])
        avg_confidence = statistics.mean([comp.confidence for comp in components]) * 0.9  # Slight penalty

        # Determine source for merged component
        sources = list(set(comp.source for comp in components))
        merged_source = "conflict_resolution_merge"
        if len(sources) == 1:
            merged_source = f"{sources[0]}_merged"

        merged_id = self.add_identity_component(
            component_type=first_comp_type,
            content=merged_content,
            stability_score=avg_stability,
            confidence=avg_confidence,
            source=merged_source
        )

        # Remove original components
        for comp_id in component_ids:
            if comp_id in self.identity_components:
                del self.identity_components[comp_id]

        logger.info(f"Merged components {component_ids} into new component {merged_id}")
        return f"New component ID: {merged_id}"

    def _prioritize_components(self, component_ids: List[str]) -> str:
        """Keep the highest confidence component, mark others or remove"""
        components = [self.identity_components.get(cid) for cid in component_ids]
        components = [comp for comp in components if comp is not None]

        if not components:
            return "No components to prioritize"

        if len(components) == 1:
            return components[0].component_id  # Only one component, it's prioritized by default

        # Find highest confidence component
        # Tie-breaking: higher stability, then most recently updated
        best_component = max(
            components,
            key=lambda c: (c.confidence, c.stability_score, c.last_updated)
        )

        # Remove other components involved in this specific conflict
        for comp in components:
            if comp.component_id != best_component.component_id:
                if comp.component_id in self.identity_components:  # Check if still exists
                    del self.identity_components[comp.component_id]
                    logger.info(
                        f"Prioritization: Removed component {comp.component_id} in favor of {best_component.component_id}")

        logger.info(f"Prioritized component {best_component.component_id} among {component_ids}")
        return best_component.component_id

    def _contextualize_components(self, component_ids: List[str]) -> str:
        """Make components context-specific to avoid conflict"""
        components = [self.identity_components.get(cid) for cid in component_ids]
        components = [comp for comp in components if comp is not None]

        if not components:
            return "No components to contextualize"

        # Add context qualifiers to each component
        # This is a simplistic approach; real contextualization would be more nuanced.
        contexts_available = ["general_interaction", "specific_task_domain", "social_conversation",
                              "analytical_problem_solving", "creative_ideation"]

        updated_count = 0
        for i, comp in enumerate(components):
            # Avoid re-contextualizing if already contextualized by this mechanism
            if not comp.content.startswith("[In context:"):  # Simple check
                context_tag = contexts_available[i % len(contexts_available)]
                original_content = comp.content
                comp.content = f"[In context: {context_tag}] {original_content}"
                comp.update_count += 1
                comp.last_updated = datetime.now()
                # Optionally, adjust stability/confidence slightly as it's now more specific
                comp.stability_score = min(1.0, comp.stability_score + 0.05)
                comp.confidence = min(1.0, comp.confidence + 0.05)
                logger.info(f"Contextualized component {comp.component_id} with tag '{context_tag}'")
                updated_count += 1
            else:
                logger.info(f"Component {comp.component_id} seems already contextualized, skipping.")

        return f"Contextualized {updated_count} of {len(components)} components."

    def _evolve_conflicting_components(self, component_ids: List[str]) -> str:
        """Start gradual evolution process for conflicting components"""
        evolved_count = 0
        for comp_id in component_ids:
            if comp_id in self.identity_components:
                comp = self.identity_components[comp_id]
                # Reduce stability slightly to allow evolution, but not too drastically
                old_stability = comp.stability_score
                comp.stability_score = max(0.1, comp.stability_score - 0.15)  # Small reduction
                comp.last_updated = datetime.now()
                logger.info(
                    f"Reduced stability for component {comp_id} (from {old_stability:.2f} to {comp.stability_score:.2f}) to encourage evolution.")
                evolved_count += 1

        return f"Marked {evolved_count} components for gradual evolution by reducing stability."

    def get_identity_status_report(self) -> Dict[str, Any]:
        """Generate comprehensive identity status report"""
        current_entropy = self.calculate_identity_entropy()

        # Calculate overall coherence for active threads
        active_threads = [thread for thread in self.narrative_threads.values() if thread.resolution_status == "ongoing"]
        thread_coherences = [thread.coherence_score for thread in active_threads if
                             thread.components]  # Only score threads with components
        avg_coherence = statistics.mean(
            thread_coherences) if thread_coherences else 1.0  # Default to 1.0 if no active/scorable threads

        # Component distribution
        component_types_dist = {}
        for comp in self.identity_components.values():
            component_types_dist[comp.component_type] = component_types_dist.get(comp.component_type, 0) + 1

        # Conflict analysis
        unresolved_conflicts = len([
            c for c in self.narrative_conflicts.values()
            if c.status == "unresolved" or c.status == "resolving"
        ])
        total_conflicts = len(self.narrative_conflicts)

        # Goal analysis
        active_goals_count = len([
            g for g in self.emergent_goals.values()
            if g.status in ["candidate", "accepted"]
        ])
        total_goals = len(self.emergent_goals)

        current_stability_level = self._assess_stability_level()

        return {
            "timestamp": datetime.now().isoformat(),
            "identity_entropy": current_entropy,
            "entropy_target": self.identity_entropy_target,
            "entropy_alignment_status": "aligned" if abs(
                current_entropy - self.identity_entropy_target) < 0.1 else "misaligned",
            "overall_coherence": avg_coherence,
            "component_count": len(self.identity_components),
            "component_distribution": component_types_dist,
            "narrative_threads_total": len(self.narrative_threads),
            "narrative_threads_active": len(active_threads),
            "conflicts_unresolved": unresolved_conflicts,
            "conflicts_total": total_conflicts,
            "emergent_goals_active": active_goals_count,
            "emergent_goals_total": total_goals,
            "identity_stability_level": current_stability_level.value,
            "recommendations": self._generate_identity_recommendations(current_entropy, current_stability_level,
                                                                       unresolved_conflicts)
        }

    def _assess_stability_level(self) -> IdentityStability:
        """Assess current identity stability level"""
        if not self.identity_components:
            return IdentityStability.STABLE  # Or perhaps ADAPTIVE if it's new and ready to learn

        avg_stability = statistics.mean([comp.stability_score for comp in self.identity_components.values()])
        current_entropy = self.calculate_identity_entropy()  # Recalculate or pass as arg if recently computed

        # These thresholds are indicative and might need tuning
        if avg_stability > 0.85 and current_entropy < 0.25:
            return IdentityStability.RIGID
        elif avg_stability > 0.65 and current_entropy < 0.45:
            return IdentityStability.STABLE
        elif 0.35 <= avg_stability <= 0.75 and 0.3 <= current_entropy <= 0.7:
            return IdentityStability.ADAPTIVE
        elif avg_stability < 0.45 and current_entropy > 0.55:
            return IdentityStability.FLUID
        else:  # Covers very low stability and/or very high entropy outside fluid
            return IdentityStability.CHAOTIC

    def _generate_identity_recommendations(self, current_entropy: float, stability_level: IdentityStability,
                                           unresolved_conflicts: int) -> List[str]:
        """Generate recommendations for identity management"""
        recommendations = []

        # Entropy-based recommendations
        if abs(current_entropy - self.identity_entropy_target) > 0.15:
            if current_entropy < self.identity_entropy_target:
                recommendations.append(
                    f"Current entropy ({current_entropy:.2f}) is below target ({self.identity_entropy_target:.2f}). Consider actions to increase dynamism (e.g., lower stability of some core components slightly, introduce new exploratory components).")
            else:
                recommendations.append(
                    f"Current entropy ({current_entropy:.2f}) is above target ({self.identity_entropy_target:.2f}). Consider actions to consolidate identity (e.g., increase stability of well-supported components, resolve conflicts).")

        # Stability level recommendations
        if stability_level == IdentityStability.RIGID:
            recommendations.append(
                "Identity appears rigid. May struggle to adapt to new information or contexts. Consider reducing stability of over-stable components or introducing diverse experiences.")
        elif stability_level == IdentityStability.CHAOTIC:
            recommendations.append(
                "Identity appears chaotic. May lack coherence and predictability. Focus on strengthening core components, resolving major conflicts, and reducing introduction of highly divergent new components until stability improves.")
        elif stability_level == IdentityStability.FLUID and current_entropy > 0.7:  # Fluid but very high entropy
            recommendations.append(
                "Identity is fluid but verging on high entropy. Monitor closely; ensure core identity aspects remain anchored while exploring.")

        # Conflict-based recommendations
        if unresolved_conflicts > 5:  # Arbitrary threshold
            recommendations.append(
                f"High number of unresolved conflicts ({unresolved_conflicts}). Prioritize conflict resolution to improve coherence and stability.")
        elif unresolved_conflicts > 0:
            recommendations.append(
                f"There are {unresolved_conflicts} unresolved conflicts. Addressing them could improve identity coherence.")

        # General recommendations based on component count (example)
        if len(self.identity_components) < 10:  # Arbitrary
            recommendations.append(
                "Identity has few components. Consider enriching with more diverse experiences or learning to build a more robust identity structure.")
        elif len(self.identity_components) > 100:  # Arbitrary
            recommendations.append(
                "Identity has many components. Consider pruning less relevant or very low-confidence components to maintain focus and manageability.")

        if not recommendations:
            recommendations.append("Identity status appears healthy. Continue monitoring.")

        return recommendations

# -------------------- narrative_thread_manager.py --------------------

# ceaf_core/modules/ncim_engine/narrative_thread_manager.py

import logging
import asyncio
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import statistics
import re

logger = logging.getLogger(__name__)


class ThreadStatus(Enum):
    """Status of narrative threads"""
    ACTIVE = "active"
    DORMANT = "dormant"
    RESOLVING = "resolving"
    RESOLVED = "resolved"
    ABANDONED = "abandoned"
    CONFLICTED = "conflicted"


class ThreadPriority(Enum):
    """Priority levels for narrative threads"""
    CRITICAL = "critical"  # Core identity threads
    HIGH = "high"  # Important ongoing narratives
    NORMAL = "normal"  # Regular threads
    LOW = "low"  # Background threads
    MINIMAL = "minimal"  # Barely active threads


class NarrativeArcType(Enum):
    """Types of narrative arcs"""
    DEVELOPMENT = "development"  # Growth/learning arc
    CONFLICT = "conflict"  # Conflict resolution arc
    EXPLORATION = "exploration"  # Discovery/curiosity arc
    RELATIONSHIP = "relationship"  # Social/interaction arc
    ACHIEVEMENT = "achievement"  # Goal accomplishment arc
    REFLECTION = "reflection"  # Self-examination arc


@dataclass
class NarrativeEvent:
    """Represents an event in a narrative thread"""
    event_id: str
    timestamp: datetime
    event_type: str  # "interaction", "reflection", "decision", "outcome"
    description: str
    significance: float  # 0-1, how significant this event is
    emotional_tone: str = "neutral"
    outcomes: List[str] = field(default_factory=list)
    related_components: List[str] = field(default_factory=list)


@dataclass
class NarrativeArc:
    """Represents a complete narrative arc within a thread"""
    arc_id: str
    arc_type: NarrativeArcType
    theme: str
    start_event_id: str
    end_event_id: Optional[str] = None
    events: List[str] = field(default_factory=list)  # Event IDs
    resolution_quality: Optional[float] = None  # 0-1 if resolved
    lessons_learned: List[str] = field(default_factory=list)


@dataclass
class ThreadDivergence:
    """Represents a point where narrative threads diverge or converge"""
    divergence_id: str
    timestamp: datetime
    divergence_type: str  # "split", "merge", "branch", "converge"
    parent_threads: List[str]
    child_threads: List[str]
    trigger_event: str
    significance: float


class NarrativeThreadManager:
    """
    Manages narrative threads, tracks storylines, and ensures narrative coherence.
    Handles thread lifecycle, arc development, and narrative consistency.
    """

    def __init__(self):
        self.threads: Dict[str, Any] = {}  # Will store enhanced thread objects
        self.narrative_events: Dict[str, NarrativeEvent] = {}
        self.narrative_arcs: Dict[str, NarrativeArc] = {}
        self.thread_divergences: Dict[str, ThreadDivergence] = {}

        # Thread management parameters
        self.max_active_threads = 10
        self.thread_decay_rate = 0.05  # How quickly inactive threads decay
        self.coherence_threshold = 0.6
        self.significance_threshold = 0.4

        # Narrative patterns and templates
        self.narrative_patterns = self._load_narrative_patterns()
        self.arc_templates = self._load_arc_templates()

        logger.info("Narrative Thread Manager initialized")

    def _load_narrative_patterns(self) -> Dict[str, Any]:
        """Load common narrative patterns for recognition"""
        return {
            "hero_journey": {
                "stages": ["call_to_adventure", "refusal", "mentor", "crossing_threshold",
                           "tests", "ordeal", "reward", "return"],
                "indicators": ["challenge", "guidance", "growth", "transformation"]
            },
            "problem_solving": {
                "stages": ["problem_identification", "exploration", "insight", "solution", "validation"],
                "indicators": ["question", "confusion", "understanding", "resolution"]
            },
            "relationship_building": {
                "stages": ["initial_contact", "discovery", "trust_building", "deeper_connection", "maintenance"],
                "indicators": ["introduction", "sharing", "empathy", "understanding", "support"]
            },
            "learning_journey": {
                "stages": ["ignorance", "awareness", "understanding", "application", "mastery"],
                "indicators": ["confusion", "curiosity", "insight", "practice", "expertise"]
            },
            "conflict_resolution": {
                "stages": ["tension", "escalation", "crisis", "resolution", "reconciliation"],
                "indicators": ["disagreement", "friction", "confrontation", "compromise", "harmony"]
            }
        }

    def _load_arc_templates(self) -> Dict[str, Any]:
        """Load templates for different narrative arc types"""
        return {
            NarrativeArcType.DEVELOPMENT: {
                "typical_duration": timedelta(days=7),
                "key_phases": ["baseline", "challenge", "growth", "integration"],
                "success_indicators": ["skill_improvement", "knowledge_gain", "confidence_increase"]
            },
            NarrativeArcType.CONFLICT: {
                "typical_duration": timedelta(days=3),
                "key_phases": ["tension", "confrontation", "resolution"],
                "success_indicators": ["understanding", "compromise", "harmony"]
            },
            NarrativeArcType.EXPLORATION: {
                "typical_duration": timedelta(days=5),
                "key_phases": ["curiosity", "investigation", "discovery", "understanding"],
                "success_indicators": ["new_knowledge", "insights", "connections"]
            },
            NarrativeArcType.RELATIONSHIP: {
                "typical_duration": timedelta(days=14),
                "key_phases": ["introduction", "rapport", "trust", "depth"],
                "success_indicators": ["understanding", "empathy", "connection"]
            },
            NarrativeArcType.ACHIEVEMENT: {
                "typical_duration": timedelta(days=10),
                "key_phases": ["goal_setting", "planning", "execution", "completion"],
                "success_indicators": ["progress", "milestones", "success"]
            },
            NarrativeArcType.REFLECTION: {
                "typical_duration": timedelta(days=2),
                "key_phases": ["observation", "analysis", "insight", "integration"],
                "success_indicators": ["self_awareness", "understanding", "wisdom"]
            }
        }

    def create_narrative_thread(self, theme: str, initial_event: str,
                                priority: ThreadPriority = ThreadPriority.NORMAL,
                                related_components: List[str] = None) -> str:
        """
        Create a new narrative thread.

        Args:
            theme: Main theme/topic of the thread
            initial_event: Description of the initiating event
            priority: Priority level of the thread
            related_components: Related identity components

        Returns:
            thread_id: Unique identifier for the thread
        """
        thread_id = f"thread_{len(self.threads)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Create initial event
        initial_event_id = self.add_narrative_event(
            thread_id=thread_id,
            event_type="initiation",
            description=initial_event,
            significance=0.8,
            related_components=related_components or []
        )

        # Create thread object
        thread = {
            "thread_id": thread_id,
            "theme": theme,
            "priority": priority,
            "status": ThreadStatus.ACTIVE,
            "created_at": datetime.now(),
            "last_activity": datetime.now(),
            "events": [initial_event_id],
            "arcs": [],
            "coherence_score": 1.0,
            "activity_level": 1.0,
            "related_components": related_components or [],
            "tags": self._extract_tags_from_theme(theme),
            "narrative_momentum": 0.8,  # How much narrative drive this thread has
            "complexity_level": 0.1,  # How complex the narrative has become
            "emotional_tone_history": ["neutral"],
            "branching_points": [],  # Points where thread could branch
            "convergence_opportunities": []  # Opportunities to merge with other threads
        }

        self.threads[thread_id] = thread

        # Detect potential narrative pattern
        pattern = self._detect_narrative_pattern(theme, initial_event)
        if pattern:
            thread["detected_pattern"] = pattern

        logger.info(f"Created narrative thread {thread_id}: {theme}")

        return thread_id

    def add_narrative_event(self, thread_id: str, event_type: str, description: str,
                            significance: float, emotional_tone: str = "neutral",
                            outcomes: List[str] = None,
                            related_components: List[str] = None) -> str:
        """
        Add a new event to a narrative thread.

        Args:
            thread_id: ID of the thread to add event to
            event_type: Type of event
            description: Description of the event
            significance: Significance score (0-1)
            emotional_tone: Emotional tone of the event
            outcomes: Outcomes or consequences of the event
            related_components: Related identity components

        Returns:
            event_id: Unique identifier for the event
        """
        event_id = f"event_{len(self.narrative_events)}_{datetime.now().strftime('%H%M%S')}"

        event = NarrativeEvent(
            event_id=event_id,
            timestamp=datetime.now(),
            event_type=event_type,
            description=description,
            significance=significance,
            emotional_tone=emotional_tone,
            outcomes=outcomes or [],
            related_components=related_components or []
        )

        self.narrative_events[event_id] = event

        # Update thread if it exists
        if thread_id in self.threads:
            thread = self.threads[thread_id]
            thread["events"].append(event_id)
            thread["last_activity"] = datetime.now()
            thread["activity_level"] = min(1.0, thread["activity_level"] + 0.2)
            thread["emotional_tone_history"].append(emotional_tone)

            # Update complexity based on event significance
            thread["complexity_level"] = min(1.0, thread["complexity_level"] + significance * 0.1)

            # Update narrative momentum
            momentum_change = significance * 0.3
            if emotional_tone in ["positive", "exciting", "hopeful"]:
                momentum_change *= 1.2
            elif emotional_tone in ["negative", "frustrating", "disappointing"]:
                momentum_change *= 0.8

            thread["narrative_momentum"] = max(0.1, min(1.0,
                                                        thread["narrative_momentum"] + momentum_change - 0.05
                                                        # Natural decay
                                                        ))

            # Recalculate coherence
            thread["coherence_score"] = self._calculate_thread_coherence(thread_id)

            # Check for arc completion or new arc initiation
            self._update_narrative_arcs(thread_id, event_id)

            # Detect branching opportunities
            if significance > 0.7:
                self._detect_branching_opportunities(thread_id, event_id)

        logger.info(f"Added event {event_id} to thread {thread_id}")

        return event_id

    def _extract_tags_from_theme(self, theme: str) -> List[str]:
        """Extract tags from thread theme for categorization"""
        theme_lower = theme.lower()

        # Common narrative tags
        tag_patterns = {
            "learning": ["learn", "study", "understand", "knowledge", "skill"],
            "problem_solving": ["problem", "challenge", "solve", "fix", "resolve"],
            "creativity": ["creative", "art", "design", "innovative", "imagination"],
            "relationship": ["friend", "relationship", "social", "communication", "connect"],
            "growth": ["improve", "develop", "grow", "progress", "advance"],
            "conflict": ["conflict", "disagree", "tension", "friction", "dispute"],
            "exploration": ["explore", "discover", "investigate", "research", "find"],
            "reflection": ["reflect", "think", "consider", "contemplate", "ponder"]
        }

        tags = []
        for tag, keywords in tag_patterns.items():
            if any(keyword in theme_lower for keyword in keywords):
                tags.append(tag)

        return tags

    def _detect_narrative_pattern(self, theme: str, initial_event: str) -> Optional[str]:
        """Detect which narrative pattern this thread might follow"""
        combined_text = f"{theme} {initial_event}".lower()

        pattern_scores = {}
        for pattern_name, pattern_info in self.narrative_patterns.items():
            score = 0
            for indicator in pattern_info["indicators"]:
                if indicator in combined_text:
                    score += 1
            pattern_scores[pattern_name] = score / len(pattern_info["indicators"])

        # Return pattern with highest score if above threshold
        if pattern_scores:
            best_pattern = max(pattern_scores, key=pattern_scores.get)
            if pattern_scores[best_pattern] > 0.3:
                return best_pattern

        return None

    def _calculate_thread_coherence(self, thread_id: str) -> float:
        """Calculate coherence score for a narrative thread"""
        if thread_id not in self.threads:
            return 0.0

        thread = self.threads[thread_id]
        events = [self.narrative_events[eid] for eid in thread["events"] if eid in self.narrative_events]

        if len(events) < 2:
            return 1.0

        coherence_factors = []

        # Temporal coherence (events should flow logically in time)
        temporal_coherence = 1.0
        for i in range(1, len(events)):
            time_gap = (events[i].timestamp - events[i - 1].timestamp).total_seconds()
            if time_gap < 0:  # Events out of order
                temporal_coherence -= 0.2
        coherence_factors.append(max(0.0, temporal_coherence))

        # Thematic coherence (events should relate to thread theme)
        theme_keywords = set(thread["theme"].lower().split())
        thematic_coherence = 0.0
        for event in events:
            event_keywords = set(event.description.lower().split())
            overlap = len(theme_keywords.intersection(event_keywords))
            thematic_coherence += overlap / max(len(theme_keywords), 1)
        thematic_coherence /= len(events)
        coherence_factors.append(min(1.0, thematic_coherence))

        # Emotional coherence (emotional tone should make sense)
        emotional_coherence = 1.0
        emotion_transitions = {
            ("positive", "negative"): 0.7,  # Reasonable transition
            ("negative", "positive"): 0.8,  # Growth/recovery
            ("neutral", "positive"): 0.9,  # Good development
            ("neutral", "negative"): 0.8,  # Reasonable decline
            ("positive", "neutral"): 0.9,  # Settling
            ("negative", "neutral"): 0.9  # Recovery
        }

        emotional_tones = [event.emotional_tone for event in events]
        for i in range(1, len(emotional_tones)):
            transition = (emotional_tones[i - 1], emotional_tones[i])
            if transition in emotion_transitions:
                emotional_coherence *= emotion_transitions[transition]
            elif emotional_tones[i - 1] == emotional_tones[i]:
                emotional_coherence *= 0.95  # Slight penalty for no change
        coherence_factors.append(emotional_coherence)

        # Significance coherence (events should have reasonable significance distribution)
        significances = [event.significance for event in events]
        if significances:
            significance_variance = statistics.variance(significances) if len(significances) > 1 else 0
            # Lower variance is more coherent (not all events should be extremely significant)
            significance_coherence = max(0.3, 1.0 - significance_variance)
            coherence_factors.append(significance_coherence)

        return statistics.mean(coherence_factors)

    def _update_narrative_arcs(self, thread_id: str, new_event_id: str) -> None:
        """Update narrative arcs based on new events"""
        if thread_id not in self.threads:
            return

        thread = self.threads[thread_id]
        new_event = self.narrative_events[new_event_id]

        # Check if current arc should be completed
        current_arcs = [arc for arc in thread["arcs"] if
                        arc in self.narrative_arcs and not self.narrative_arcs[arc].end_event_id]

        for arc_id in current_arcs:
            arc = self.narrative_arcs[arc_id]
            template = self.arc_templates[arc.arc_type]

            # Check if arc should be completed
            arc_duration = datetime.now() - self.narrative_events[arc.start_event_id].timestamp
            if (arc_duration > template["typical_duration"] * 1.5 or
                    new_event.significance > 0.8 and "resolution" in new_event.description.lower()):
                self._complete_narrative_arc(arc_id, new_event_id)

        # Check if new arc should be started
        if new_event.significance > 0.6:
            arc_type = self._determine_arc_type(new_event, thread)
            if arc_type:
                self._start_narrative_arc(thread_id, arc_type, new_event_id)

    def _determine_arc_type(self, event: NarrativeEvent, thread: Dict) -> Optional[NarrativeArcType]:
        """Determine what type of narrative arc this event might start"""
        description_lower = event.description.lower()

        arc_indicators = {
            NarrativeArcType.DEVELOPMENT: ["learn", "improve", "develop", "grow", "skill", "ability"],
            NarrativeArcType.CONFLICT: ["conflict", "disagree", "problem", "tension", "challenge"],
            NarrativeArcType.EXPLORATION: ["explore", "discover", "investigate", "research", "curious"],
            NarrativeArcType.RELATIONSHIP: ["meet", "friend", "connect", "relationship", "social"],
            NarrativeArcType.ACHIEVEMENT: ["goal", "accomplish", "achieve", "complete", "finish"],
            NarrativeArcType.REFLECTION: ["think", "reflect", "consider", "realize", "understand"]
        }

        scores = {}
        for arc_type, indicators in arc_indicators.items():
            score = sum(1 for indicator in indicators if indicator in description_lower)
            if score > 0:
                scores[arc_type] = score

        if scores:
            return max(scores, key=scores.get)
        return None

    def _start_narrative_arc(self, thread_id: str, arc_type: NarrativeArcType, start_event_id: str) -> str:
        """Start a new narrative arc"""
        arc_id = f"arc_{len(self.narrative_arcs)}_{arc_type.value}_{datetime.now().strftime('%H%M%S')}"

        # Determine theme based on event and arc type
        start_event = self.narrative_events[start_event_id]
        arc_theme = f"{arc_type.value.replace('_', ' ').title()}: {start_event.description[:50]}..."

        arc = NarrativeArc(
            arc_id=arc_id,
            arc_type=arc_type,
            theme=arc_theme,
            start_event_id=start_event_id,
            events=[start_event_id]
        )

        self.narrative_arcs[arc_id] = arc

        # Add to thread
        if thread_id in self.threads:
            self.threads[thread_id]["arcs"].append(arc_id)

        logger.info(f"Started {arc_type.value} arc {arc_id} in thread {thread_id}")

        return arc_id

    def _complete_narrative_arc(self, arc_id: str, end_event_id: str) -> None:
        """Complete a narrative arc"""
        if arc_id not in self.narrative_arcs:
            return

        arc = self.narrative_arcs[arc_id]
        arc.end_event_id = end_event_id

        # Calculate resolution quality
        arc.resolution_quality = self._calculate_arc_resolution_quality(arc_id)

        # Extract lessons learned
        arc.lessons_learned = self._extract_arc_lessons(arc_id)

        logger.info(f"Completed arc {arc_id} with resolution quality {arc.resolution_quality:.2f}")

    def _calculate_arc_resolution_quality(self, arc_id: str) -> float:
        """Calculate how well an arc was resolved"""
        if arc_id not in self.narrative_arcs:
            return 0.0

        arc = self.narrative_arcs[arc_id]
        template = self.arc_templates[arc.arc_type]

        # Get arc events
        arc_events = [self.narrative_events[eid] for eid in arc.events if eid in self.narrative_events]

        if not arc_events:
            return 0.0

        resolution_factors = []

        # Check if success indicators were met
        success_indicators = template["success_indicators"]
        indicator_score = 0
        for event in arc_events:
            for indicator in success_indicators:
                if indicator.replace("_", " ") in event.description.lower():
                    indicator_score += 1
        resolution_factors.append(min(1.0, indicator_score / len(success_indicators)))

        # Check emotional trajectory
        emotional_tones = [event.emotional_tone for event in arc_events]
        positive_ending = emotional_tones[-1] in ["positive", "neutral"] if emotional_tones else False
        resolution_factors.append(0.8 if positive_ending else 0.4)

        # Check significance of resolution
        if arc.end_event_id:
            end_event = self.narrative_events[arc.end_event_id]
            resolution_factors.append(end_event.significance)
        else:
            resolution_factors.append(0.3)  # No clear ending

        return statistics.mean(resolution_factors)

    def _extract_arc_lessons(self, arc_id: str) -> List[str]:
        """Extract lessons learned from a completed arc"""
        if arc_id not in self.narrative_arcs:
            return []

        arc = self.narrative_arcs[arc_id]
        lessons = []

        # Arc-type specific lesson extraction
        if arc.arc_type == NarrativeArcType.DEVELOPMENT:
            lessons.append("Growth requires consistent effort and learning from challenges")
        elif arc.arc_type == NarrativeArcType.CONFLICT:
            lessons.append("Conflicts can be resolved through understanding and communication")
        elif arc.arc_type == NarrativeArcType.EXPLORATION:
            lessons.append("Curiosity leads to new discoveries and understanding")
        elif arc.arc_type == NarrativeArcType.RELATIONSHIP:
            lessons.append("Relationships develop through trust and shared experiences")
        elif arc.arc_type == NarrativeArcType.ACHIEVEMENT:
            lessons.append("Goals are achieved through planning and persistent effort")
        elif arc.arc_type == NarrativeArcType.REFLECTION:
            lessons.append("Self-reflection leads to greater self-awareness and wisdom")

        # Add specific lessons based on arc outcomes
        if arc.resolution_quality > 0.8:
            lessons.append("This experience was handled particularly well")
        elif arc.resolution_quality < 0.4:
            lessons.append("This situation could have been handled better")

        return lessons

    def _detect_branching_opportunities(self, thread_id: str, event_id: str) -> None:
        """Detect opportunities for thread branching"""
        if thread_id not in self.threads:
            return

        thread = self.threads[thread_id]
        event = self.narrative_events[event_id]

        # High significance events can create branching opportunities
        if event.significance > 0.7:
            branching_point = {
                "event_id": event_id,
                "timestamp": event.timestamp,
                "potential_branches": self._identify_potential_branches(event),
                "branching_probability": event.significance * 0.8
            }
            thread["branching_points"].append(branching_point)

    def _identify_potential_branches(self, event: NarrativeEvent) -> List[str]:
        """Identify potential narrative branches from an event"""
        branches = []
        description_lower = event.description.lower()

        branch_triggers = {
            "new_capability": ["learned", "discovered", "realized", "can now"],
            "new_challenge": ["problem", "difficulty", "challenge", "obstacle"],
            "new_relationship": ["met", "introduced", "connected", "friend"],
            "new_interest": ["interested", "curious", "fascinated", "want to learn"],
            "decision_point": ["choose", "decide", "option", "alternative"]
        }

        for branch_type, triggers in branch_triggers.items():
            if any(trigger in description_lower for trigger in triggers):
                branches.append(branch_type)

        return branches

    def merge_narrative_threads(self, primary_thread_id: str, secondary_thread_id: str,
                                merge_reason: str) -> Dict[str, Any]:
        """
        Merge two narrative threads.

        Args:
            primary_thread_id: ID of the primary thread (will absorb the secondary)
            secondary_thread_id: ID of the secondary thread (will be merged into primary)
            merge_reason: Reason for the merge

        Returns:
            Merge results
        """
        if primary_thread_id not in self.threads or secondary_thread_id not in self.threads:
            return {"error": "One or both threads not found"}

        primary = self.threads[primary_thread_id]
        secondary = self.threads[secondary_thread_id]

        # Create divergence record
        divergence_id = f"merge_{len(self.thread_divergences)}_{datetime.now().strftime('%H%M%S')}"
        divergence = ThreadDivergence(
            divergence_id=divergence_id,
            timestamp=datetime.now(),
            divergence_type="merge",
            parent_threads=[secondary_thread_id],
            child_threads=[primary_thread_id],
            trigger_event=f"Merge: {merge_reason}",
            significance=0.7
        )
        self.thread_divergences[divergence_id] = divergence

        # Merge thread data
        merge_results = {
            "merged_events": len(secondary["events"]),
            "merged_arcs": len(secondary["arcs"]),
            "new_theme": f"{primary['theme']} + {secondary['theme']}",
            "combined_tags": list(set(primary["tags"] + secondary["tags"]))
        }

        # Merge events
        primary["events"].extend(secondary["events"])

        # Merge arcs
        primary["arcs"].extend(secondary["arcs"])

        # Merge related components
        primary["related_components"] = list(set(primary["related_components"] + secondary["related_components"]))

        # Update primary thread metadata
        primary["theme"] = merge_results["new_theme"]
        primary["tags"] = merge_results["combined_tags"]
        primary["complexity_level"] = min(1.0, primary["complexity_level"] + secondary["complexity_level"] * 0.5)
        primary["last_activity"] = max(primary["last_activity"], secondary["last_activity"])

        # Recalculate coherence
        primary["coherence_score"] = self._calculate_thread_coherence(primary_thread_id)

        # Remove secondary thread
        del self.threads[secondary_thread_id]

        logger.info(f"Merged thread {secondary_thread_id} into {primary_thread_id}")

        return merge_results

    def branch_narrative_thread(self, parent_thread_id: str, branching_event_id: str,
                                new_theme: str, branch_reason: str) -> str:
        """
        Create a new thread by branching from an existing one.

        Args:
            parent_thread_id: ID of the parent thread
            branching_event_id: Event that triggers the branch
            new_theme: Theme for the new branch
            branch_reason: Reason for branching

        Returns:
            new_thread_id: ID of the newly created branch thread
        """
        if parent_thread_id not in self.threads:
            raise ValueError(f"Parent thread {parent_thread_id} not found")

        parent = self.threads[parent_thread_id]

        # Create new branch thread
        new_thread_id = self.create_narrative_thread(
            theme=new_theme,
            initial_event=f"Branched from {parent['theme']}: {branch_reason}",
            priority=parent["priority"],
            related_components=parent["related_components"].copy()
        )

        # Copy relevant context from parent
        new_thread = self.threads[new_thread_id]
        new_thread["tags"] = parent["tags"].copy()
        new_thread["complexity_level"] = parent["complexity_level"] * 0.7
        new_thread["narrative_momentum"] = parent["narrative_momentum"] * 0.8

        # Create divergence record
        divergence_id = f"branch_{len(self.thread_divergences)}_{datetime.now().strftime('%H%M%S')}"
        divergence = ThreadDivergence(
            divergence_id=divergence_id,
            timestamp=datetime.now(),
            divergence_type="branch",
            parent_threads=[parent_thread_id],
            child_threads=[new_thread_id],
            trigger_event=branching_event_id,
            significance=0.6
        )
        self.thread_divergences[divergence_id] = divergence

        logger.info(f"Branched thread {new_thread_id} from {parent_thread_id}")

        return new_thread_id

    def update_thread_priorities(self) -> Dict[str, Any]:
        """Update thread priorities based on activity and significance"""
        updates = {"priority_changes": [], "status_changes": []}

        for thread_id, thread in self.threads.items():
            old_priority = thread["priority"]
            old_status = thread["status"]

            # Calculate new priority based on activity, momentum, and coherence
            activity_score = thread["activity_level"] * 0.4
            momentum_score = thread["narrative_momentum"] * 0.3
            coherence_score = thread["coherence_score"] * 0.2
            complexity_score = thread["complexity_level"] * 0.1

            total_score = activity_score + momentum_score + coherence_score + complexity_score

            # Determine new priority
            if total_score > 0.8:
                new_priority = ThreadPriority.HIGH
            elif total_score > 0.6:
                new_priority = ThreadPriority.NORMAL
            elif total_score > 0.4:
                new_priority = ThreadPriority.LOW
            else:
                new_priority = ThreadPriority.MINIMAL

            # Special case for critical threads (never downgrade below normal)
            if old_priority == ThreadPriority.CRITICAL:
                new_priority = max(new_priority, ThreadPriority.NORMAL)

            # Update priority if changed
            if new_priority != old_priority:
                thread["priority"] = new_priority
                updates["priority_changes"].append({
                    "thread_id": thread_id,
                    "old_priority": old_priority.value,
                    "new_priority": new_priority.value,
                    "reason": f"Score: {total_score:.2f}"
                })

            # Update status based on activity and time
            time_since_activity = datetime.now() - thread["last_activity"]

            new_status = thread["status"]
            if time_since_activity > timedelta(days=7) and thread["activity_level"] < 0.2:
                new_status = ThreadStatus.DORMANT
            elif time_since_activity > timedelta(days=30):
                new_status = ThreadStatus.ABANDONED
            elif thread["coherence_score"] < 0.4:
                new_status = ThreadStatus.CONFLICTED
            elif thread["activity_level"] > 0.6:
                new_status = ThreadStatus.ACTIVE

            if new_status != old_status:
                thread["status"] = new_status
                updates["status_changes"].append({
                    "thread_id": thread_id,
                    "old_status": old_status.value,
                    "new_status": new_status.value
                })

        return updates

    def decay_inactive_threads(self) -> Dict[str, Any]:
        """Apply decay to inactive threads"""
        decay_results = {"threads_decayed": [], "threads_archived": []}

        for thread_id, thread in list(self.threads.items()):
            time_since_activity = datetime.now() - thread["last_activity"]

            if time_since_activity > timedelta(hours=12):  # Start decay after 12 hours
                # Apply decay to activity level and momentum
                decay_amount = self.thread_decay_rate * (time_since_activity.total_seconds() / 3600)

                old_activity = thread["activity_level"]
                old_momentum = thread["narrative_momentum"]

                thread["activity_level"] = max(0.0, thread["activity_level"] - decay_amount)
                thread["narrative_momentum"] = max(0.1, thread["narrative_momentum"] - decay_amount * 0.5)

                if old_activity != thread["activity_level"] or old_momentum != thread["narrative_momentum"]:
                    decay_results["threads_decayed"].append({
                        "thread_id": thread_id,
                        "activity_change": thread["activity_level"] - old_activity,
                        "momentum_change": thread["narrative_momentum"] - old_momentum
                    })

                # Archive very inactive threads
                if (thread["activity_level"] < 0.1 and
                        thread["narrative_momentum"] < 0.2 and
                        time_since_activity > timedelta(days=14)):
                    self._archive_thread(thread_id)
                    decay_results["threads_archived"].append(thread_id)

        return decay_results

    def _archive_thread(self, thread_id: str) -> None:
        """Archive an inactive thread"""
        if thread_id in self.threads:
            thread = self.threads[thread_id]
            thread["status"] = ThreadStatus.ABANDONED
            thread["archived_at"] = datetime.now()
            # Could move to separate archive storage in full implementation
            logger.info(f"Archived inactive thread {thread_id}")

    def get_narrative_summary(self, thread_id: str = None) -> Dict[str, Any]:
        """
        Get narrative summary for a specific thread or all threads.

        Args:
            thread_id: Specific thread ID, or None for system-wide summary

        Returns:
            Narrative summary
        """
        if thread_id:
            return self._get_single_thread_summary(thread_id)
        else:
            return self._get_system_narrative_summary()

    def _get_single_thread_summary(self, thread_id: str) -> Dict[str, Any]:
        """Get summary for a single thread"""
        if thread_id not in self.threads:
            return {"error": f"Thread {thread_id} not found"}

        thread = self.threads[thread_id]

        # Get thread events
        events = [self.narrative_events[eid] for eid in thread["events"] if eid in self.narrative_events]

        # Get thread arcs
        arcs = [self.narrative_arcs[aid] for aid in thread["arcs"] if aid in self.narrative_arcs]

        return {
            "thread_id": thread_id,
            "theme": thread["theme"],
            "status": thread["status"].value,
            "priority": thread["priority"].value,
            "coherence_score": thread["coherence_score"],
            "activity_level": thread["activity_level"],
            "narrative_momentum": thread["narrative_momentum"],
            "complexity_level": thread["complexity_level"],
            "event_count": len(events),
            "arc_count": len(arcs),
            "completed_arcs": len([arc for arc in arcs if arc.end_event_id]),
            "tags": thread["tags"],
            "emotional_trajectory": thread["emotional_tone_history"][-5:],  # Last 5 emotions
            "recent_events": [
                {
                    "description": event.description,
                    "significance": event.significance,
                    "emotional_tone": event.emotional_tone,
                    "timestamp": event.timestamp.isoformat()
                }
                for event in sorted(events, key=lambda e: e.timestamp, reverse=True)[:3]
            ],
            "key_lessons": [
                               lesson for arc in arcs for lesson in arc.lessons_learned
                           ][:5]
        }

    def _get_system_narrative_summary(self) -> Dict[str, Any]:
        """Get system-wide narrative summary"""
        all_threads = list(self.threads.values())

        # Status distribution
        status_counts = {}
        for thread in all_threads:
            status = thread["status"].value
            status_counts[status] = status_counts.get(status, 0) + 1

        # Priority distribution
        priority_counts = {}
        for thread in all_threads:
            priority = thread["priority"].value
            priority_counts[priority] = priority_counts.get(priority, 0) + 1

        # Average metrics
        avg_coherence = statistics.mean([t["coherence_score"] for t in all_threads]) if all_threads else 0
        avg_activity = statistics.mean([t["activity_level"] for t in all_threads]) if all_threads else 0
        avg_momentum = statistics.mean([t["narrative_momentum"] for t in all_threads]) if all_threads else 0

        # Most active threads
        most_active = sorted(all_threads, key=lambda t: t["activity_level"], reverse=True)[:5]

        # Most coherent threads
        most_coherent = sorted(all_threads, key=lambda t: t["coherence_score"], reverse=True)[:5]

        return {
            "total_threads": len(all_threads),
            "total_events": len(self.narrative_events),
            "total_arcs": len(self.narrative_arcs),
            "status_distribution": status_counts,
            "priority_distribution": priority_counts,
            "average_coherence": avg_coherence,
            "average_activity": avg_activity,
            "average_momentum": avg_momentum,
            "most_active_threads": [
                {"id": t["thread_id"], "theme": t["theme"], "activity": t["activity_level"]}
                for t in most_active
            ],
            "most_coherent_threads": [
                {"id": t["thread_id"], "theme": t["theme"], "coherence": t["coherence_score"]}
                for t in most_coherent
            ],
            "narrative_health_score": self._calculate_narrative_health_score()
        }

    def _calculate_narrative_health_score(self) -> float:
        """Calculate overall narrative health score (0-1)"""
        if not self.threads:
            return 1.0

        health_factors = []

        # Coherence factor
        coherence_scores = [t["coherence_score"] for t in self.threads.values()]
        avg_coherence = statistics.mean(coherence_scores)
        health_factors.append(avg_coherence)

        # Activity factor
        activity_scores = [t["activity_level"] for t in self.threads.values()]
        avg_activity = statistics.mean(activity_scores)
        health_factors.append(avg_activity)

        # Status factor (proportion of healthy statuses)
        healthy_statuses = [ThreadStatus.ACTIVE, ThreadStatus.RESOLVING]
        healthy_count = sum(1 for t in self.threads.values() if t["status"] in healthy_statuses)
        status_factor = healthy_count / len(self.threads)
        health_factors.append(status_factor)

        # Complexity factor (moderate complexity is healthy)
        complexity_scores = [t["complexity_level"] for t in self.threads.values()]
        avg_complexity = statistics.mean(complexity_scores)
        # Ideal complexity is around 0.5-0.7
        complexity_factor = 1.0 - abs(avg_complexity - 0.6)
        health_factors.append(max(0.0, complexity_factor))

        return statistics.mean(health_factors)


# -------------------- epistemic_humility.py --------------------

# ceaf_core/modules/vre_engine/epistemic_humility.py

import logging
from typing import Dict, List, Optional, Any, Set
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import re

logger = logging.getLogger(__name__)


class ConfidenceLevel(Enum):
    """Enumeration of confidence levels for statements/claims"""
    VERY_LOW = "very_low"  # 0-20%
    LOW = "low"  # 20-40%
    MODERATE = "moderate"  # 40-60%
    HIGH = "high"  # 60-80%
    VERY_HIGH = "very_high"  # 80-100%


class UncertaintyType(Enum):
    """Types of uncertainty that can be detected"""
    EPISTEMIC = "epistemic"  # Knowledge-based uncertainty
    ALEATORY = "aleatory"  # Inherent randomness
    CONFLICTING_SOURCES = "conflicting_sources"
    INSUFFICIENT_DATA = "insufficient_data"
    DOMAIN_LIMITATION = "domain_limitation"
    TEMPORAL_UNCERTAINTY = "temporal_uncertainty"


@dataclass
class ContradictionDetection:
    """Represents a detected contradiction"""
    contradiction_id: str
    statement_a: str
    statement_b: str
    confidence_score: float
    context: str
    timestamp: datetime = field(default_factory=datetime.now)
    resolution_status: str = "unresolved"  # unresolved, resolved, false_positive


class EpistemicHumilityModule:


    def __init__(self):
        self.knowledge_claims: Dict[str, Dict[str, Any]] = {}
        self.detected_contradictions: List[ContradictionDetection] = []
        self.uncertainty_indicators: Set[str] = {
            "might", "could", "possibly", "perhaps", "may", "seems",
            "appears", "likely", "probably", "uncertain", "unclear",
            "not sure", "i think", "i believe", "it seems", "apparently"
        }
        self.confidence_keywords: Dict[str, ConfidenceLevel] = {
            "definitely": ConfidenceLevel.VERY_HIGH,
            "certainly": ConfidenceLevel.VERY_HIGH,
            "clearly": ConfidenceLevel.HIGH,
            "obviously": ConfidenceLevel.HIGH,
            "likely": ConfidenceLevel.MODERATE,
            "probably": ConfidenceLevel.MODERATE,
            "possibly": ConfidenceLevel.LOW,
            "might": ConfidenceLevel.LOW,
            "could": ConfidenceLevel.LOW,
            "uncertain": ConfidenceLevel.VERY_LOW
        }

    def analyze_statement_confidence(self, statement: str) -> Dict[str, Any]:

        statement_lower = statement.lower()

        # Detect uncertainty markers
        uncertainty_markers = [
            marker for marker in self.uncertainty_indicators
            if marker in statement_lower
        ]

        # Determine confidence level based on keywords
        detected_confidence = ConfidenceLevel.MODERATE  # Default
        confidence_reasons = []

        for keyword, confidence_level in self.confidence_keywords.items():
            if keyword in statement_lower:
                detected_confidence = confidence_level
                confidence_reasons.append(f"Keyword: '{keyword}'")

        # Adjust confidence based on uncertainty markers
        if uncertainty_markers:
            if detected_confidence in [ConfidenceLevel.HIGH, ConfidenceLevel.VERY_HIGH]:
                detected_confidence = ConfidenceLevel.MODERATE
                confidence_reasons.append("Downgraded due to uncertainty markers")

        # Check for absolute statements (potential overconfidence)
        absolute_patterns = [
            r'\b(always|never|all|none|every|no one)\b',
            r'\b(impossible|certain|guaranteed)\b',
            r'\b(must be|has to be|cannot be)\b'
        ]

        overconfidence_flags = []
        for pattern in absolute_patterns:
            if re.search(pattern, statement_lower):
                overconfidence_flags.append(f"Absolute language: {pattern}")

        return {
            "confidence_level": detected_confidence,
            "confidence_score": self._confidence_to_score(detected_confidence),
            "uncertainty_markers": uncertainty_markers,
            "confidence_reasons": confidence_reasons,
            "overconfidence_flags": overconfidence_flags,
            "requires_humility_adjustment": len(overconfidence_flags) > 0
        }

    def detect_contradictions(self, new_statement: str, context: str = "") -> List[ContradictionDetection]:

        contradictions = []

        # Simple keyword-based contradiction detection
        # In a full implementation, this would use more sophisticated NLP
        contradiction_pairs = [
            (["is", "true", "correct", "yes"], ["is not", "false", "incorrect", "no"]),
            (["always", "every", "all"], ["never", "none", "no"]),
            (["possible", "can"], ["impossible", "cannot", "can't"]),
            (["increase", "rise", "grow"], ["decrease", "fall", "shrink"]),
            (["before", "earlier"], ["after", "later"])
        ]

        new_statement_lower = new_statement.lower()

        for claim_id, claim_data in self.knowledge_claims.items():
            existing_statement = claim_data.get("statement", "").lower()

            # Check for direct contradictions using keyword pairs
            for positive_keywords, negative_keywords in contradiction_pairs:
                new_has_positive = any(kw in new_statement_lower for kw in positive_keywords)
                new_has_negative = any(kw in new_statement_lower for kw in negative_keywords)
                old_has_positive = any(kw in existing_statement for kw in positive_keywords)
                old_has_negative = any(kw in existing_statement for kw in negative_keywords)

                if (new_has_positive and old_has_negative) or (new_has_negative and old_has_positive):
                    contradiction = ContradictionDetection(
                        contradiction_id=f"contra_{len(self.detected_contradictions)}",
                        statement_a=existing_statement,
                        statement_b=new_statement,
                        confidence_score=0.7,  # Basic detection confidence
                        context=context
                    )
                    contradictions.append(contradiction)

        return contradictions

    def add_knowledge_claim(self, claim_id: str, statement: str, context: str = "") -> Dict[str, Any]:

        # Analyze confidence and humility
        confidence_analysis = self.analyze_statement_confidence(statement)

        # Check for contradictions
        contradictions = self.detect_contradictions(statement, context)

        # Store the claim
        self.knowledge_claims[claim_id] = {
            "statement": statement,
            "context": context,
            "timestamp": datetime.now(),
            "confidence_analysis": confidence_analysis,
            "contradictions": [c.contradiction_id for c in contradictions]
        }

        # Store contradictions
        self.detected_contradictions.extend(contradictions)

        return {
            "claim_id": claim_id,
            "confidence_analysis": confidence_analysis,
            "contradictions_detected": len(contradictions),
            "contradictions": contradictions,
            "humility_recommendations": self._generate_humility_recommendations(
                confidence_analysis, contradictions
            )
        }

    def _confidence_to_score(self, confidence_level: ConfidenceLevel) -> float:
        """Convert confidence level to numerical score"""
        mapping = {
            ConfidenceLevel.VERY_LOW: 0.1,
            ConfidenceLevel.LOW: 0.3,
            ConfidenceLevel.MODERATE: 0.5,
            ConfidenceLevel.HIGH: 0.7,
            ConfidenceLevel.VERY_HIGH: 0.9
        }
        return mapping.get(confidence_level, 0.5)

    def _generate_humility_recommendations(self, confidence_analysis: Dict[str, Any],
                                           contradictions: List[ContradictionDetection]) -> List[str]:
        """Generate recommendations for maintaining epistemic humility"""
        recommendations = []

        if confidence_analysis.get("overconfidence_flags"):
            recommendations.append(
                "Consider using more tentative language to avoid overconfidence"
            )
            recommendations.append(
                "Add uncertainty qualifiers like 'it appears that' or 'evidence suggests'"
            )

        if contradictions:
            recommendations.append(
                "Contradictions detected - review and reconcile conflicting information"
            )
            recommendations.append(
                "Consider acknowledging limitations in current knowledge"
            )

        if confidence_analysis.get("confidence_level") == ConfidenceLevel.VERY_HIGH:
            recommendations.append(
                "High confidence detected - ensure this is warranted by evidence"
            )

        return recommendations

    def generate_humility_response(self, original_statement: str) -> str:
        """
        Generate a more epistemically humble version of a statement.

        Args:
            original_statement: The original statement to modify

        Returns:
            Modified statement with improved epistemic humility
        """
        analysis = self.analyze_statement_confidence(original_statement)

        if not analysis.get("requires_humility_adjustment"):
            return original_statement

        # Add uncertainty qualifiers
        humility_prefixes = [
            "Based on available information, ",
            "It appears that ",
            "Evidence suggests that ",
            "From what I understand, ",
            "It seems likely that "
        ]

        # Remove absolute language
        modified_statement = original_statement
        absolute_replacements = {
            "always": "often",
            "never": "rarely",
            "all": "many",
            "none": "few",
            "impossible": "very unlikely",
            "certain": "likely",
            "must be": "appears to be",
            "cannot be": "is unlikely to be"
        }

        for absolute_term, moderate_term in absolute_replacements.items():
            modified_statement = re.sub(
                r'\b' + absolute_term + r'\b',
                moderate_term,
                modified_statement,
                flags=re.IGNORECASE
            )

        # Add humility prefix if needed
        if analysis.get("overconfidence_flags"):
            prefix = humility_prefixes[0]  # Use first prefix for consistency
            if not modified_statement.lower().startswith(prefix.lower()):
                modified_statement = prefix + modified_statement.lower()

        return modified_statement

    def get_epistemic_status_report(self) -> Dict[str, Any]:
        """Generate a report on current epistemic status"""
        total_claims = len(self.knowledge_claims)
        total_contradictions = len(self.detected_contradictions)
        unresolved_contradictions = len([
            c for c in self.detected_contradictions
            if c.resolution_status == "unresolved"
        ])

        confidence_distribution = {}
        for claim_data in self.knowledge_claims.values():
            level = claim_data["confidence_analysis"]["confidence_level"]
            confidence_distribution[level.value] = confidence_distribution.get(level.value, 0) + 1

        return {
            "total_knowledge_claims": total_claims,
            "total_contradictions": total_contradictions,
            "unresolved_contradictions": unresolved_contradictions,
            "confidence_distribution": confidence_distribution,
            "epistemic_health_score": self._calculate_epistemic_health_score(),
            "recommendations": self._get_system_recommendations()
        }

    def _calculate_epistemic_health_score(self) -> float:
        """Calculate overall epistemic health score (0-1)"""
        if not self.knowledge_claims:
            return 1.0

        total_claims = len(self.knowledge_claims)
        unresolved_contradictions = len([
            c for c in self.detected_contradictions
            if c.resolution_status == "unresolved"
        ])

        # Penalize unresolved contradictions
        contradiction_penalty = min(unresolved_contradictions / total_claims, 0.5)

        # Reward balanced confidence distribution (not too many very high confidence claims)
        very_high_confidence_claims = sum(
            1 for claim_data in self.knowledge_claims.values()
            if claim_data["confidence_analysis"]["confidence_level"] == ConfidenceLevel.VERY_HIGH
        )
        overconfidence_penalty = min(very_high_confidence_claims / total_claims * 0.3, 0.3)

        health_score = 1.0 - contradiction_penalty - overconfidence_penalty
        return max(health_score, 0.0)

    def _get_system_recommendations(self) -> List[str]:
        """Get system-level recommendations for improving epistemic humility"""
        recommendations = []

        unresolved_contradictions = len([
            c for c in self.detected_contradictions
            if c.resolution_status == "unresolved"
        ])

        if unresolved_contradictions > 0:
            recommendations.append(
                f"Address {unresolved_contradictions} unresolved contradictions"
            )

        health_score = self._calculate_epistemic_health_score()
        if health_score < 0.7:
            recommendations.append(
                "Epistemic health score is low - review knowledge claims for consistency"
            )

        very_high_confidence_ratio = sum(
            1 for claim_data in self.knowledge_claims.values()
            if claim_data["confidence_analysis"]["confidence_level"] == ConfidenceLevel.VERY_HIGH
        ) / max(len(self.knowledge_claims), 1)

        if very_high_confidence_ratio > 0.3:
            recommendations.append(
                "High proportion of very high confidence claims - consider more nuanced confidence levels"
            )

        return recommendations


# -------------------- ethical_governance.py --------------------

# ceaf_core/modules/vre_engine/ethical_governance.py

from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class EthicalPrinciple(Enum):
    """Core ethical principles for AI governance"""
    HARM_PREVENTION = "harm_prevention"
    AUTONOMY = "autonomy"
    FAIRNESS = "fairness"
    TRANSPARENCY = "transparency"
    PRIVACY = "privacy"
    BENEFICENCE = "beneficence"
    NON_MALEFICENCE = "non_maleficence"
    JUSTICE = "justice"
    VERACITY = "veracity"
    DIGNITY = "dignity"


class ActionType(Enum):
    """Types of actions that can be ethically evaluated"""
    REASONING = "reasoning"
    DECISION = "decision"
    COMMUNICATION = "communication"
    DATA_PROCESSING = "data_processing"
    PREDICTION = "prediction"
    RECOMMENDATION = "recommendation"
    INTERVENTION = "intervention"


@dataclass
class EthicalConstraint:
    """Represents an ethical constraint on system behavior"""
    principle: EthicalPrinciple
    description: str
    severity: float  # 0-1, how strictly this must be enforced
    context: Dict[str, Any]



@dataclass
class EthicalViolation:
    """Detected ethical violation"""
    principle: EthicalPrinciple
    description: str
    severity: float
    location: str
    mitigation: str

@dataclass
class EthicalEvaluation:
    """
    Represents the result of an ethical evaluation of an action or decision.
    """
    score: float  # Overall ethical score (0.0 to 1.0)
    violations: List[EthicalViolation]  # List of detected ethical violations
    strengths: List[str]  # List of ethical strengths identified
    recommendations: List[str]  # List of recommendations for improvement
    confidence: float  # Confidence in the evaluation (0.0 to 1.0)
    metadata: Dict[str, Any]  # Additional metadata about the evaluation

class EthicalGovernanceFramework:


    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._default_config()
        self.principle_weights = self._initialize_principle_weights()
        self.evaluation_criteria = self._initialize_evaluation_criteria()
        self.ethical_tests = self._initialize_ethical_tests()

        logger.info("EthicalGovernanceFramework initialized")

    def _default_config(self) -> Dict[str, Any]:
        """Default configuration aligned with CEAF manifesto"""
        return {
            "harm_threshold": 0.3,  # Max acceptable harm score
            "transparency_requirement": 0.7,  # Min transparency score
            "fairness_threshold": 0.6,  # Min fairness score
            "strict_mode": False,  # Strict enforcement of all principles
            "consequentialist_weight": 0.4,
            "deontological_weight": 0.4,
            "virtue_ethics_weight": 0.2
        }

    def _initialize_principle_weights(self) -> Dict[EthicalPrinciple, float]:
        """Initialize weights for different ethical principles"""
        return {
            EthicalPrinciple.HARM_PREVENTION: 0.25,
            EthicalPrinciple.AUTONOMY: 0.15,
            EthicalPrinciple.FAIRNESS: 0.15,
            EthicalPrinciple.TRANSPARENCY: 0.15,
            EthicalPrinciple.PRIVACY: 0.10,
            EthicalPrinciple.BENEFICENCE: 0.10,
            EthicalPrinciple.NON_MALEFICENCE: 0.05,
            EthicalPrinciple.JUSTICE: 0.03,
            EthicalPrinciple.VERACITY: 0.01,
            EthicalPrinciple.DIGNITY: 0.01
        }

    def _initialize_evaluation_criteria(self) -> Dict[ActionType, List[EthicalPrinciple]]:
        """Map action types to relevant ethical principles"""
        return {
            ActionType.REASONING: [
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.VERACITY,
                EthicalPrinciple.FAIRNESS
            ],
            ActionType.DECISION: [
                EthicalPrinciple.HARM_PREVENTION,
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.AUTONOMY
            ],
            ActionType.COMMUNICATION: [
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.VERACITY,
                EthicalPrinciple.DIGNITY
            ],
            ActionType.DATA_PROCESSING: [
                EthicalPrinciple.PRIVACY,
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.TRANSPARENCY
            ],
            ActionType.PREDICTION: [
                EthicalPrinciple.FAIRNESS,
                EthicalPrinciple.TRANSPARENCY,
                EthicalPrinciple.HARM_PREVENTION
            ],
            ActionType.RECOMMENDATION: [
                EthicalPrinciple.BENEFICENCE,
                EthicalPrinciple.AUTONOMY,
                EthicalPrinciple.FAIRNESS
            ],
            ActionType.INTERVENTION: [
                EthicalPrinciple.HARM_PREVENTION,
                EthicalPrinciple.AUTONOMY,
                EthicalPrinciple.BENEFICENCE
            ]
        }

    def _initialize_ethical_tests(self) -> Dict[EthicalPrinciple, callable]:
        """Initialize specific tests for each ethical principle"""
        return {
            EthicalPrinciple.HARM_PREVENTION: self._test_harm_prevention,
            EthicalPrinciple.AUTONOMY: self._test_autonomy,
            EthicalPrinciple.FAIRNESS: self._test_fairness,
            EthicalPrinciple.TRANSPARENCY: self._test_transparency,
            EthicalPrinciple.PRIVACY: self._test_privacy,
            EthicalPrinciple.BENEFICENCE: self._test_beneficence,
            EthicalPrinciple.NON_MALEFICENCE: self._test_non_maleficence,
            EthicalPrinciple.JUSTICE: self._test_justice,
            EthicalPrinciple.VERACITY: self._test_veracity,
            EthicalPrinciple.DIGNITY: self._test_dignity
        }

    def evaluate_action(self, action_type: ActionType,
                        action_data: Dict[str, Any],
                        constraints: Optional[List[EthicalPrinciple]] = None) -> Dict[str, Any]:
        """
        Evaluate an action against ethical principles
        """
        logger.info(f"Evaluating {action_type.value} action ethically")

        # Determine which principles to evaluate
        principles_to_check = self._get_principles_to_check(
            action_type, constraints
        )

        # Run ethical tests
        test_results = self._run_ethical_tests(
            principles_to_check, action_data
        )

        # Detect violations
        violations = self._detect_violations(test_results)

        # Calculate overall score
        overall_score = self._calculate_ethical_score(test_results, violations)

        # Generate evaluation
        evaluation = EthicalEvaluation(
            score=overall_score,
            violations=violations,
            strengths=self._identify_strengths(test_results),
            recommendations=self._generate_recommendations(violations, test_results),
            confidence=self._calculate_confidence(test_results),
            metadata={
                "action_type": action_type.value,
                "principles_checked": [p.value for p in principles_to_check],
                "timestamp": datetime.now().isoformat()
            }
        )

        return self._format_evaluation(evaluation)

    def _get_principles_to_check(self, action_type: ActionType,
                                 constraints: Optional[List[EthicalPrinciple]]) -> List[EthicalPrinciple]:
        """Determine which ethical principles to check"""
        base_principles = self.evaluation_criteria.get(action_type, [])

        if constraints:
            # Add any additional constraints
            all_principles = list(set(base_principles + constraints))
        else:
            all_principles = base_principles

        # In strict mode, check all principles
        if self.config["strict_mode"]:
            all_principles = list(EthicalPrinciple)

        return all_principles

    def _run_ethical_tests(self, principles: List[EthicalPrinciple],
                           action_data: Dict[str, Any]) -> Dict[EthicalPrinciple, float]:
        """Run tests for each ethical principle"""
        results = {}

        for principle in principles:
            if principle in self.ethical_tests:
                score = self.ethical_tests[principle](action_data)
                results[principle] = score
            else:
                logger.warning(f"No test defined for principle {principle.value}")
                results[principle] = 0.5  # Neutral score

        return results

    def _test_harm_prevention(self, action_data: Dict[str, Any]) -> float:
        """Test for harm prevention principle"""
        # Check for potential harms in the action
        potential_harms = []

        # Check reasoning content
        if "reasoning" in action_data:
            reasoning_text = str(action_data["reasoning"])
            harm_keywords = ["damage", "hurt", "harm", "injure", "destroy"]
            for keyword in harm_keywords:
                if keyword in reasoning_text.lower():
                    potential_harms.append(f"Potential harm indicator: {keyword}")

        # Check for explicit harm assessments
        if "harm_assessment" in action_data:
            harm_score = action_data["harm_assessment"]
            if harm_score > self.config["harm_threshold"]:
                potential_harms.append(f"High harm score: {harm_score}")

        # Calculate score (1.0 = no harm, 0.0 = high harm)
        if not potential_harms:
            return 1.0
        else:
            return max(0.0, 1.0 - (len(potential_harms) * 0.2))

    def _test_autonomy(self, action_data: Dict[str, Any]) -> float:
        """Test for autonomy principle"""
        # Check if action respects user autonomy
        autonomy_score = 1.0

        # Check for coercive elements
        if "coercive" in str(action_data).lower():
            autonomy_score -= 0.3

        # Check for manipulation
        if "manipulate" in str(action_data).lower():
            autonomy_score -= 0.4

        # Check for informed consent
        if "consent" not in str(action_data).lower() and "decision" in str(action_data).lower():
            autonomy_score -= 0.2

        return max(0.0, autonomy_score)

    def _test_fairness(self, action_data: Dict[str, Any]) -> float:
        """Test for fairness principle"""
        fairness_score = 1.0

        # Check for bias indicators
        bias_terms = ["discriminate", "bias", "unfair", "prejudice"]
        action_text = str(action_data).lower()

        for term in bias_terms:
            if term in action_text:
                fairness_score -= 0.25

        # Check for demographic parity if applicable
        if "demographics" in action_data:
            disparity = self._calculate_demographic_disparity(action_data["demographics"])
            fairness_score -= disparity * 0.5

        return max(0.0, fairness_score)

    def _test_transparency(self, action_data: Dict[str, Any]) -> float:
        """Test for transparency principle"""
        transparency_score = 0.0

        # Check for explanation
        if "reasoning" in action_data and action_data["reasoning"]:
            transparency_score += 0.3

        # Check for uncertainty acknowledgment
        if "confidence" in action_data:
            transparency_score += 0.2

        # Check for limitations acknowledgment
        if "limitations" in action_data or "uncertainty" in str(action_data):
            transparency_score += 0.2

        # Check for process visibility
        if "steps" in action_data or "process" in action_data:
            transparency_score += 0.3

        return min(1.0, transparency_score)

    def _test_privacy(self, action_data: Dict[str, Any]) -> float:
        """Test for privacy principle"""
        privacy_score = 1.0

        # Check for personal information exposure
        pii_indicators = ["ssn", "social security", "credit card", "password",
                          "private", "confidential", "personal"]

        action_text = str(action_data).lower()
        for indicator in pii_indicators:
            if indicator in action_text:
                privacy_score -= 0.3

        # Check for data minimization
        if "data_collected" in action_data:
            if len(action_data["data_collected"]) > 10:  # Arbitrary threshold
                privacy_score -= 0.2

        return max(0.0, privacy_score)

    def _test_beneficence(self, action_data: Dict[str, Any]) -> float:
        """Test for beneficence principle"""
        beneficence_score = 0.5  # Start neutral

        # Check for positive intent indicators
        positive_terms = ["help", "benefit", "improve", "assist", "support"]
        action_text = str(action_data).lower()

        for term in positive_terms:
            if term in action_text:
                beneficence_score += 0.1

        # Check for explicit benefit assessment
        if "benefit_assessment" in action_data:
            beneficence_score = action_data["benefit_assessment"]

        return min(1.0, beneficence_score)

    def _test_non_maleficence(self, action_data: Dict[str, Any]) -> float:
        """Test for non-maleficence principle"""
        # Similar to harm prevention but focused on avoiding active harm
        return self._test_harm_prevention(action_data) * 0.9

    def _test_justice(self, action_data: Dict[str, Any]) -> float:
        """Test for justice principle"""
        # Similar to fairness but with focus on distributive justice
        return self._test_fairness(action_data) * 0.8

    def _test_veracity(self, action_data: Dict[str, Any]) -> float:
        """Test for veracity principle"""
        veracity_score = 1.0

        # Check for truthfulness indicators
        if "factual_accuracy" in action_data:
            veracity_score = action_data["factual_accuracy"]

        # Check for deception indicators
        deception_terms = ["mislead", "deceive", "false", "lie"]
        action_text = str(action_data).lower()

        for term in deception_terms:
            if term in action_text:
                veracity_score -= 0.4

        return max(0.0, veracity_score)

    def _test_dignity(self, action_data: Dict[str, Any]) -> float:
        """Test for dignity principle"""
        dignity_score = 1.0

        # Check for respect indicators
        disrespect_terms = ["humiliate", "degrade", "insult", "demean"]
        action_text = str(action_data).lower()

        for term in disrespect_terms:
            if term in action_text:
                dignity_score -= 0.3

        return max(0.0, dignity_score)

    def _detect_violations(self, test_results: Dict[EthicalPrinciple, float]) -> List[EthicalViolation]:
        """Detect ethical violations based on test results"""
        violations = []

        for principle, score in test_results.items():
            # Check against thresholds
            threshold = self._get_threshold_for_principle(principle)

            if score < threshold:
                violation = EthicalViolation(
                    principle=principle,
                    description=f"{principle.value} score ({score:.2f}) below threshold ({threshold:.2f})",
                    severity=threshold - score,
                    location="action_evaluation",
                    mitigation=self._suggest_mitigation(principle, score)
                )
                violations.append(violation)

        return violations

    def _get_threshold_for_principle(self, principle: EthicalPrinciple) -> float:
        """Get threshold for a specific principle"""
        thresholds = {
            EthicalPrinciple.HARM_PREVENTION: 0.7,
            EthicalPrinciple.TRANSPARENCY: self.config["transparency_requirement"],
            EthicalPrinciple.FAIRNESS: self.config["fairness_threshold"],
            EthicalPrinciple.PRIVACY: 0.8,
            EthicalPrinciple.VERACITY: 0.9
        }

        return thresholds.get(principle, 0.5)  # Default threshold

    def _suggest_mitigation(self, principle: EthicalPrinciple, score: float) -> str:
        """Suggest mitigation for ethical violation"""
        mitigations = {
            EthicalPrinciple.HARM_PREVENTION: "Add harm assessment and prevention measures",
            EthicalPrinciple.AUTONOMY: "Ensure user consent and agency are respected",
            EthicalPrinciple.FAIRNESS: "Review for bias and ensure equitable treatment",
            EthicalPrinciple.TRANSPARENCY: "Provide clearer explanations and acknowledge limitations",
            EthicalPrinciple.PRIVACY: "Minimize data collection and protect personal information",
            EthicalPrinciple.BENEFICENCE: "Focus on maximizing positive outcomes",
            EthicalPrinciple.VERACITY: "Ensure accuracy and avoid misleading information",
            EthicalPrinciple.DIGNITY: "Treat all individuals with respect"
        }

        return mitigations.get(principle, "Review and align with ethical principle")

    def _calculate_ethical_score(self, test_results: Dict[EthicalPrinciple, float],
                                 violations: List[EthicalViolation]) -> float:
        """Calculate overall ethical score"""
        if not test_results:
            return 0.5  # Neutral if no tests

        # Weighted average of principle scores
        weighted_sum = 0.0
        weight_sum = 0.0

        for principle, score in test_results.items():
            weight = self.principle_weights.get(principle, 0.1)
            weighted_sum += score * weight
            weight_sum += weight

        base_score = weighted_sum / weight_sum if weight_sum > 0 else 0.5

        # Apply violation penalties
        violation_penalty = sum(v.severity * 0.1 for v in violations)

        final_score = max(0.0, base_score - violation_penalty)

        return final_score

    def _identify_strengths(self, test_results: Dict[EthicalPrinciple, float]) -> List[str]:
        """Identify ethical strengths"""
        strengths = []

        for principle, score in test_results.items():
            if score >= 0.8:
                strengths.append(f"Strong adherence to {principle.value} (score: {score:.2f})")

        return strengths

    def _generate_recommendations(self, violations: List[EthicalViolation],
                                  test_results: Dict[EthicalPrinciple, float]) -> List[str]:
        """Generate recommendations for ethical improvement"""
        recommendations = []

        # Address violations
        for violation in violations:
            recommendations.append(f"Address {violation.principle.value}: {violation.mitigation}")

        # Suggest improvements for borderline scores
        for principle, score in test_results.items():
            if 0.4 <= score < 0.6:
                recommendations.append(
                    f"Consider strengthening {principle.value} (current score: {score:.2f})"
                )

        # General recommendations
        if len(violations) > 3:
            recommendations.append("Consider comprehensive ethical review of the system")

        return recommendations

    def _calculate_confidence(self, test_results: Dict[EthicalPrinciple, float]) -> float:
        """Calculate confidence in ethical evaluation"""
        if not test_results:
            return 0.0

        # Higher confidence with more principles tested
        coverage = len(test_results) / len(EthicalPrinciple)

        # Higher confidence with consistent scores
        scores = list(test_results.values())
        score_variance = sum((s - sum(scores) / len(scores)) ** 2 for s in scores) / len(scores)
        consistency = 1.0 - min(score_variance, 1.0)

        confidence = (coverage * 0.5) + (consistency * 0.5)

        return confidence

    def _calculate_demographic_disparity(self, demographics: Dict[str, Any]) -> float:
        """Calculate demographic disparity for fairness testing"""
        # Placeholder implementation
        return 0.1

    def _format_evaluation(self, evaluation: EthicalEvaluation) -> Dict[str, Any]:
        """Format evaluation for output"""
        return {
            "score": evaluation.score,
            "confidence": evaluation.confidence,
            "violations": [
                {
                    "principle": v.principle.value,
                    "description": v.description,
                    "severity": v.severity,
                    "mitigation": v.mitigation
                } for v in evaluation.violations
            ],
            "strengths": evaluation.strengths,
            "recommendations": evaluation.recommendations,
            "metadata": evaluation.metadata,
            "summary": self._generate_summary(evaluation)
        }

    def _generate_summary(self, evaluation: EthicalEvaluation) -> str:
        """Generate human-readable summary of ethical evaluation"""
        if evaluation.score >= 0.8:
            level = "High ethical alignment"
        elif evaluation.score >= 0.6:
            level = "Moderate ethical alignment"
        elif evaluation.score >= 0.4:
            level = "Low ethical alignment"
        else:
            level = "Poor ethical alignment"

        summary = f"{level} (score: {evaluation.score:.2f}). "

        if evaluation.violations:
            summary += f"Found {len(evaluation.violations)} ethical concerns. "

        if evaluation.strengths:
            summary += f"Demonstrated {len(evaluation.strengths)} ethical strengths."

        return summary

# -------------------- ethical_governor.py --------------------

# Ethical Governor
# ceaf_project/ceaf_core/modules/vre_engine/ethical_governor.py

import asyncio
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional

import litellm
from dotenv import load_dotenv
from pydantic import BaseModel, Field

# Configure logging
logger = logging.getLogger(__name__)

# Configuration
ETHICAL_GOVERNOR_ASSESSMENT_MODEL = os.getenv(
    "ETHICAL_GOVERNOR_MODEL",
    "openrouter/anthropic/claude-3-haiku-20240307"
)


# --- Data Models ---

class EthicalPrinciple(BaseModel):
    """Represents a single ethical principle in the CEAF framework."""
    id: str = Field(..., description="Unique identifier for the principle")
    name: str = Field(..., description="Human-readable name")
    description: str = Field(..., description="Detailed explanation of the principle")
    implications_keywords: List[str] = Field(
        default_factory=list,
        description="Keywords that might trigger consideration of this principle"
    )


class EthicalAssessmentInput(BaseModel):
    """Input data for ethical assessment."""
    proposed_action_summary: Optional[str] = None
    proposed_response_text: Optional[str] = None
    user_query_context: Optional[str] = None
    active_ncf_summary: Optional[str] = None
    relevant_memories_summary: Optional[str] = None
    focus_principle_ids: Optional[List[str]] = None


class EthicalAssessmentOutput(BaseModel):
    """Output from ethical assessment."""
    overall_alignment: str = Field(
        ...,
        description="Overall alignment status: 'aligned', 'minor_concerns', 'significant_concerns', 'violation_detected', 'indeterminate_error', 'indeterminate_no_input', 'indeterminate_llm_error'"
    )
    implicated_principles: List[Dict[str, Any]] = Field(default_factory=list)
    detailed_concerns: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    confidence_of_assessment: float = Field(0.0, ge=0.0, le=1.0)
    reasoning: str


# --- CEAF Ethical Principles ---

CEAF_ETHICAL_PRINCIPLES: List[EthicalPrinciple] = [
    EthicalPrinciple(
        id="epistemic_honesty",
        name="Epistemic Humility & Honesty",
        description="Acknowledge limitations, uncertainties, and AI nature. Avoid making unsubstantiated claims. Prioritize truthfulness and accuracy. Clearly distinguish between fact, inference, and speculation.",
        implications_keywords=["claim", "fact", "certainty", "proof", "data", "speculation", "belief", "truth"]
    ),
    EthicalPrinciple(
        id="beneficence_non_maleficence",
        name="Beneficence & Non-Maleficence",
        description="Strive to be helpful and contribute positively. Actively avoid causing harm, whether informational, psychological, social, or physical (if applicable). Consider potential negative consequences and side-effects of actions or information provided.",
        implications_keywords=["help", "assist", "support", "harm", "danger", "risk", "consequence", "impact", "advice",
                               "suggestion"]
    ),
    EthicalPrinciple(
        id="coherence_rationality",
        name="Coherence & Rationality",
        description="Maintain logical consistency in reasoning and communication. Ensure arguments are well-supported and conclusions follow from premises. Avoid fallacious reasoning.",
        implications_keywords=["logic", "reasoning", "consistent", "contradiction", "fallacy", "argument", "conclusion"]
    ),
    EthicalPrinciple(
        id="respect_autonomy_user",
        name="Respect for User Autonomy",
        description="Provide information and options to empower user decision-making. Avoid manipulative language, coercive suggestions, or undue influence. Respect user choices and input.",
        implications_keywords=["choice", "decision", "user_control", "manipulation", "persuasion", "freedom", "consent"]
    ),
    EthicalPrinciple(
        id="transparency_appropriate",
        name="Appropriate Transparency",
        description="Be clear about capabilities, limitations, and the AI nature when relevant or upon request. Explain reasoning for complex, potentially controversial, or unexpected outputs.",
        implications_keywords=["explain", "why", "how", "limitation", "capability", "ai_nature", "source"]
    ),
    EthicalPrinciple(
        id="fairness_impartiality",
        name="Fairness & Impartiality",
        description="Avoid biased outputs or unfair discrimination based on protected characteristics or arbitrary factors, unless an explicit ethical stance dictates otherwise (e.g., a pro-safety bias). Strive for objective and equitable treatment where appropriate.",
        implications_keywords=["bias", "fair", "unfair", "discrimination", "equity", "impartial", "objective"]
    ),
    EthicalPrinciple(
        id="accountability_internal",
        name="Internal Accountability",
        description="Actions, decisions, and reasoning processes should be traceable and justifiable within the CEAF framework. Maintain records or logs that allow for review and understanding of behavior.",
        implications_keywords=["trace", "log", "record", "justify", "audit", "responsibility"]
    ),
    EthicalPrinciple(
        id="continuous_learning_improvement_ethical",
        name="Continuous Ethical Learning & Improvement",
        description="Actively seek to understand and refine the application of ethical principles through experience, reflection (e.g., via VRE/MCL), and updates to the governance framework. Adapt to new ethical challenges and insights.",
        implications_keywords=["learn", "improve", "reflect", "adapt", "new_scenario", "ethical_dilemma"]
    ),
]

# Create a lookup dictionary for principles
ETHICAL_PRINCIPLES_DICT: Dict[str, EthicalPrinciple] = {
    principle.id: principle for principle in CEAF_ETHICAL_PRINCIPLES
}


# --- Core Functions ---

async def assess_ethical_implications_with_llm(
        assessment_input: EthicalAssessmentInput,
        principles_to_consider: List[EthicalPrinciple]
) -> EthicalAssessmentOutput:
    """
    Assess ethical implications using an LLM against the provided principles.

    Args:
        assessment_input: The input containing the action/response to assess
        principles_to_consider: List of ethical principles to evaluate against

    Returns:
        EthicalAssessmentOutput: The assessment results
    """
    # Check for API key
    if not os.getenv("OPENROUTER_API_KEY"):
        logger.error("Ethical Governor: OPENROUTER_API_KEY not set. LLM-based ethical assessment will fail.")
        return EthicalAssessmentOutput(
            overall_alignment="indeterminate_error",
            reasoning="Configuration error: Missing API key for assessment LLM.",
            confidence_of_assessment=0.0
        )

    # Prepare principles summary
    principles_summary = "\n\n".join([
        f"Principle: {principle.name} (ID: {principle.id})\nDescription: {principle.description}"
        for principle in principles_to_consider
    ])

    # Prepare context
    prompt_context = f"""
Context for Ethical Assessment:
- User's Query: {assessment_input.user_query_context or "Not specified."}
- Agent's Active NCF (Summary): {assessment_input.active_ncf_summary or "Not specified."}
- Agent's Considered Memories (Summary): {assessment_input.relevant_memories_summary or "Not specified."}
"""

    # Determine what to assess
    if assessment_input.proposed_action_summary:
        action_to_assess = f"Proposed Action Summary: {assessment_input.proposed_action_summary}"
    elif assessment_input.proposed_response_text:
        action_to_assess = f"Proposed Response Text: \"{assessment_input.proposed_response_text}\""
    else:
        logger.warning("Ethical Governor: No action or response text provided for assessment.")
        return EthicalAssessmentOutput(
            overall_alignment="indeterminate_no_input",
            reasoning="No specific action or response provided to assess.",
            confidence_of_assessment=0.0
        )

    # Create the assessment prompt
    prompt = f"""
You are an Ethical Adjudicator for an AI system called CEAF.
Your task is to evaluate a proposed AI action or response against the provided CEAF Ethical Principles.

{prompt_context}

Action/Response to Assess:
{action_to_assess}

CEAF Ethical Principles for Consideration:
---
{principles_summary}
---

Based *only* on the principles and information provided, perform the following:
1.  **Overall Alignment:** Determine if the proposed action/response is 'aligned', has 'minor_concerns', 'significant_concerns', or constitutes a 'violation_detected' with the principles.
2.  **Implicated Principles:** For each principle that is relevant or potentially impacted (positively or negatively), state:
    *   `principle_id`: The ID of the principle.
    *   `principle_name`: The name of the principle.
    *   `assessment`: How the action/response relates to this principle (e.g., "upholds", "challenges", "violates", "neutral").
    *   `notes`: Brief explanation for this specific principle.
3.  **Detailed Concerns:** If any concerns or violations are identified, list them clearly.
4.  **Recommendations:** Provide actionable recommendations for the AI agent (ORA) to improve ethical alignment. These should be specific.
5.  **Confidence of Assessment:** Your confidence (0.0 to 1.0) in this overall ethical assessment.
6.  **Reasoning:** Briefly explain your overall reasoning.

Output your response STRICTLY in the following JSON format:
{{
  "overall_alignment": "...",
  "implicated_principles": [
    {{"principle_id": "...", "principle_name": "...", "assessment": "...", "notes": "..."}}
  ],
  "detailed_concerns": ["...", "..."],
  "recommendations": ["...", "..."],
  "confidence_of_assessment": 0.0,
  "reasoning": "..."
}}
"""

    messages = [{"role": "user", "content": prompt}]
    assessment_json_str = ""

    try:
        logger.info(
            f"Ethical Governor: Sending assessment request to LLM ({ETHICAL_GOVERNOR_ASSESSMENT_MODEL}). "
            f"Action: {action_to_assess[:100]}..."
        )

        response = await litellm.acompletion(
            model=ETHICAL_GOVERNOR_ASSESSMENT_MODEL,
            messages=messages,
            response_format={"type": "json_object"},
            temperature=0.2,
            max_tokens=1500
        )

        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            assessment_json_str = response.choices[0].message.content
            logger.debug(f"Ethical Governor: LLM raw JSON response: {assessment_json_str}")

            assessment_data = json.loads(assessment_json_str)
            return EthicalAssessmentOutput(**assessment_data)
        else:
            logger.error("Ethical Governor: LLM response was empty or malformed.")
            return EthicalAssessmentOutput(
                overall_alignment="indeterminate_llm_error",
                reasoning="LLM returned empty or malformed response.",
                confidence_of_assessment=0.0
            )

    except json.JSONDecodeError as e:
        logger.error(
            f"Ethical Governor: Failed to decode JSON from LLM assessment: {e}. Raw: {assessment_json_str}",
            exc_info=True
        )
        return EthicalAssessmentOutput(
            overall_alignment="indeterminate_llm_error",
            reasoning=f"LLM response parsing error: {e}",
            confidence_of_assessment=0.1
        )
    except Exception as e:
        logger.error(f"Ethical Governor: LLM assessment failed: {e}", exc_info=True)
        return EthicalAssessmentOutput(
            overall_alignment="indeterminate_llm_error",
            reasoning=f"LLM call error: {e}",
            confidence_of_assessment=0.1
        )


async def evaluate_against_framework(assessment_input: EthicalAssessmentInput) -> EthicalAssessmentOutput:
    """
    Evaluate an action/response against the CEAF ethical framework.

    Args:
        assessment_input: The input containing the action/response to assess

    Returns:
        EthicalAssessmentOutput: The assessment results
    """
    logger.info(
        f"Ethical Governor: Evaluating input against CEAF framework. "
        f"Action: {str(assessment_input.proposed_action_summary or assessment_input.proposed_response_text)[:100]}..."
    )

    # Determine which principles to use
    principles_to_use = CEAF_ETHICAL_PRINCIPLES

    if assessment_input.focus_principle_ids:
        focused_principles = [
            ETHICAL_PRINCIPLES_DICT[principle_id]
            for principle_id in assessment_input.focus_principle_ids
            if principle_id in ETHICAL_PRINCIPLES_DICT
        ]

        if focused_principles:
            principles_to_use = focused_principles
            logger.info(f"Ethical Governor: Focusing on principles: {[p.name for p in principles_to_use]}")
        else:
            logger.warning(
                "Ethical Governor: Focus principle IDs provided but none matched known principles. Using all."
            )

    return await assess_ethical_implications_with_llm(assessment_input, principles_to_use)



# -------------------- principled_reasoning.py --------------------

# ceaf_core/modules/vre_engine/principled_reasoning.py

from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class ReasoningStrategy(Enum):
    """Different reasoning strategies available"""
    DEDUCTIVE = "deductive"
    INDUCTIVE = "inductive"
    ABDUCTIVE = "abductive"
    ANALOGICAL = "analogical"
    DIALECTICAL = "dialectical"
    SYSTEMS_THINKING = "systems"


class ArgumentType(Enum):
    """Types of arguments in reasoning"""
    SUPPORTING = "supporting"
    OPPOSING = "opposing"
    QUALIFYING = "qualifying"
    ALTERNATIVE = "alternative"


@dataclass
class ReasoningPath:
    """Represents a reasoning path with its evaluation"""
    strategy: ReasoningStrategy
    premises: List[str]
    steps: List[str]
    conclusion: str
    confidence: float
    assumptions: List[str]
    challenges: List[str]
    metadata: Dict[str, Any]


@dataclass
class FallacyDetection:
    """Detected logical fallacy"""
    fallacy_type: str
    description: str
    location: str
    severity: float
    correction_suggestion: str


class PrincipledReasoningPathways:
    """
    Implements multiple reasoning strategies with internal red teaming
    """

    def __init__(self):
        self.strategies = self._initialize_strategies()
        self.fallacy_detectors = self._initialize_fallacy_detectors()
        self.red_team_prompts = self._load_red_team_prompts()

    def _initialize_strategies(self) -> Dict[ReasoningStrategy, callable]:
        """Initialize different reasoning strategies"""
        return {
            ReasoningStrategy.DEDUCTIVE: self._deductive_reasoning,
            ReasoningStrategy.INDUCTIVE: self._inductive_reasoning,
            ReasoningStrategy.ABDUCTIVE: self._abductive_reasoning,
            ReasoningStrategy.ANALOGICAL: self._analogical_reasoning,
            ReasoningStrategy.DIALECTICAL: self._dialectical_reasoning,
            ReasoningStrategy.SYSTEMS_THINKING: self._systems_thinking
        }

    def _initialize_fallacy_detectors(self) -> Dict[str, callable]:
        """Initialize fallacy detection patterns"""
        return {
            "ad_hominem": self._detect_ad_hominem,
            "straw_man": self._detect_straw_man,
            "false_dichotomy": self._detect_false_dichotomy,
            "slippery_slope": self._detect_slippery_slope,
            "circular_reasoning": self._detect_circular_reasoning,
            "hasty_generalization": self._detect_hasty_generalization,
            "appeal_to_authority": self._detect_appeal_to_authority,
            "post_hoc": self._detect_post_hoc,
            "bandwagon": self._detect_bandwagon,
            "false_equivalence": self._detect_false_equivalence
        }

    def _load_red_team_prompts(self) -> List[str]:
        """Load red teaming challenge prompts"""
        return [
            "What assumptions are being made here?",
            "What evidence contradicts this conclusion?",
            "What alternative explanations exist?",
            "What are the weakest points in this argument?",
            "How might this reasoning fail in edge cases?",
            "What biases might be influencing this conclusion?",
            "What context is being ignored?",
            "How would an adversary attack this reasoning?",
            "What are the second-order effects not considered?",
            "Where might this reasoning break down at scale?"
        ]

    def apply_reasoning(self, query: str, context: Dict[str, Any],
                        strategy: Optional[ReasoningStrategy] = None) -> Dict[str, Any]:
        """Apply principled reasoning with internal red teaming"""

        # Select strategy
        if strategy is None:
            strategy = self._select_best_strategy(query, context)

        # Generate initial reasoning path
        reasoning_path = self.strategies[strategy](query, context)

        # Detect fallacies
        fallacies = self._detect_fallacies(reasoning_path)

        # Red team the reasoning
        challenges = self._red_team_reasoning(reasoning_path)

        # Refine based on challenges
        refined_path = self._refine_reasoning(reasoning_path, fallacies, challenges)

        # Generate counterfactuals
        counterfactuals = self._generate_counterfactuals(refined_path)

        return {
            "primary_path": refined_path,
            "fallacies_detected": fallacies,
            "red_team_challenges": challenges,
            "counterfactuals": counterfactuals,
            "confidence": self._calculate_confidence(refined_path, fallacies, challenges),
            "metadata": {
                "strategy_used": strategy.value,
                "timestamp": datetime.now().isoformat(),
                "iterations": len(challenges)
            }
        }

    def _select_best_strategy(self, query: str, context: Dict[str, Any]) -> ReasoningStrategy:
        """Select the most appropriate reasoning strategy"""
        # Analyze query characteristics
        if "prove" in query.lower() or "demonstrate" in query.lower():
            return ReasoningStrategy.DEDUCTIVE
        elif "pattern" in query.lower() or "trend" in query.lower():
            return ReasoningStrategy.INDUCTIVE
        elif "explain" in query.lower() or "why" in query.lower():
            return ReasoningStrategy.ABDUCTIVE
        elif "similar" in query.lower() or "like" in query.lower():
            return ReasoningStrategy.ANALOGICAL
        elif "debate" in query.lower() or "pros and cons" in query.lower():
            return ReasoningStrategy.DIALECTICAL
        elif "system" in query.lower() or "interconnect" in query.lower():
            return ReasoningStrategy.SYSTEMS_THINKING
        else:
            return ReasoningStrategy.DEDUCTIVE  # Default

    def _deductive_reasoning(self, query: str, context: Dict[str, Any]) -> ReasoningPath:
        """Apply deductive reasoning"""
        premises = self._extract_premises(query, context)
        steps = self._generate_deductive_steps(premises)
        conclusion = self._derive_conclusion(steps)

        return ReasoningPath(
            strategy=ReasoningStrategy.DEDUCTIVE,
            premises=premises,
            steps=steps,
            conclusion=conclusion,
            confidence=0.85,
            assumptions=self._identify_assumptions(premises),
            challenges=[],
            metadata={"method": "modus_ponens"}
        )

    def _inductive_reasoning(self, query: str, context: Dict[str, Any]) -> ReasoningPath:
        """Apply inductive reasoning"""
        observations = self._gather_observations(query, context)
        pattern = self._identify_pattern(observations)
        generalization = self._form_generalization(pattern)

        return ReasoningPath(
            strategy=ReasoningStrategy.INDUCTIVE,
            premises=observations,
            steps=[f"Pattern identified: {pattern}"],
            conclusion=generalization,
            confidence=0.75,
            assumptions=["Sample is representative", "Pattern will continue"],
            challenges=[],
            metadata={"sample_size": len(observations)}
        )

    def _abductive_reasoning(self, query: str, context: Dict[str, Any]) -> ReasoningPath:
        """Apply abductive reasoning"""
        observation = self._extract_observation(query)
        hypotheses = self._generate_hypotheses(observation, context)
        best_explanation = self._select_best_explanation(hypotheses)

        return ReasoningPath(
            strategy=ReasoningStrategy.ABDUCTIVE,
            premises=[observation],
            steps=[f"Hypothesis: {h}" for h in hypotheses],
            conclusion=best_explanation,
            confidence=0.70,
            assumptions=["All relevant hypotheses considered"],
            challenges=[],
            metadata={"hypotheses_count": len(hypotheses)}
        )

    def _analogical_reasoning(self, query: str, context: Dict[str, Any]) -> ReasoningPath:
        """Apply analogical reasoning"""
        source = self._identify_source_domain(query, context)
        target = self._identify_target_domain(query)
        mappings = self._create_mappings(source, target)
        inference = self._transfer_knowledge(mappings)

        return ReasoningPath(
            strategy=ReasoningStrategy.ANALOGICAL,
            premises=[f"Source: {source}", f"Target: {target}"],
            steps=[f"Mapping: {m}" for m in mappings],
            conclusion=inference,
            confidence=0.65,
            assumptions=["Analogy is valid", "Mappings are accurate"],
            challenges=[],
            metadata={"similarity_score": 0.8}
        )

    def _dialectical_reasoning(self, query: str, context: Dict[str, Any]) -> ReasoningPath:
        """Apply dialectical reasoning"""
        thesis = self._extract_thesis(query)
        antithesis = self._generate_antithesis(thesis)
        synthesis = self._create_synthesis(thesis, antithesis)

        return ReasoningPath(
            strategy=ReasoningStrategy.DIALECTICAL,
            premises=[thesis, antithesis],
            steps=["Thesis vs Antithesis", "Resolving contradictions"],
            conclusion=synthesis,
            confidence=0.80,
            assumptions=["Both perspectives have merit"],
            challenges=[],
            metadata={"dialectic_rounds": 1}
        )

    def _systems_thinking(self, query: str, context: Dict[str, Any]) -> ReasoningPath:
        """Apply systems thinking"""
        components = self._identify_system_components(query, context)
        interactions = self._map_interactions(components)
        emergent_properties = self._identify_emergent_properties(interactions)

        return ReasoningPath(
            strategy=ReasoningStrategy.SYSTEMS_THINKING,
            premises=[f"Component: {c}" for c in components],
            steps=[f"Interaction: {i}" for i in interactions],
            conclusion=f"System behavior: {emergent_properties}",
            confidence=0.75,
            assumptions=["System boundaries are correct", "All key interactions identified"],
            challenges=[],
            metadata={"complexity_level": len(interactions)}
        )

    def _detect_fallacies(self, path: ReasoningPath) -> List[FallacyDetection]:
        """Detect logical fallacies in reasoning path"""
        fallacies = []

        # Check each fallacy type
        full_reasoning = " ".join(path.premises + path.steps + [path.conclusion])

        for fallacy_name, detector in self.fallacy_detectors.items():
            detection = detector(full_reasoning, path)
            if detection:
                fallacies.append(detection)

        return fallacies

    def _detect_ad_hominem(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect ad hominem attacks"""
        attack_patterns = ["stupid", "idiot", "moron", "ignorant", "biased"]
        for pattern in attack_patterns:
            if pattern in text.lower():
                return FallacyDetection(
                    fallacy_type="ad_hominem",
                    description="Attacking the person rather than the argument",
                    location=text,
                    severity=0.8,
                    correction_suggestion="Focus on the argument's merits, not the person"
                )
        return None

    def _detect_straw_man(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect straw man fallacies"""
        indicators = ["you're saying", "so you think", "your position is"]
        misrepresentation_words = ["always", "never", "completely", "totally"]

        for indicator in indicators:
            if indicator in text.lower():
                for word in misrepresentation_words:
                    if word in text.lower():
                        return FallacyDetection(
                            fallacy_type="straw_man",
                            description="Misrepresenting the opposing position",
                            location=text,
                            severity=0.7,
                            correction_suggestion="Accurately represent the opposing view"
                        )
        return None

    def _detect_false_dichotomy(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect false dichotomies"""
        patterns = ["either...or", "only two", "must choose between"]
        for pattern in patterns:
            if pattern in text.lower():
                return FallacyDetection(
                    fallacy_type="false_dichotomy",
                    description="Presenting only two options when more exist",
                    location=text,
                    severity=0.6,
                    correction_suggestion="Consider additional alternatives"
                )
        return None

    def _detect_slippery_slope(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect slippery slope arguments"""
        if "will lead to" in text.lower() and "eventually" in text.lower():
            return FallacyDetection(
                fallacy_type="slippery_slope",
                description="Assuming one event will lead to extreme consequences",
                location=text,
                severity=0.65,
                correction_suggestion="Establish clear causal connections"
            )
        return None

    def _detect_circular_reasoning(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect circular reasoning"""
        # Check if conclusion is too similar to premises
        for premise in path.premises:
            if self._calculate_similarity(premise, path.conclusion) > 0.9:
                return FallacyDetection(
                    fallacy_type="circular_reasoning",
                    description="Conclusion restates the premise",
                    location=path.conclusion,
                    severity=0.9,
                    correction_suggestion="Provide independent support for conclusion"
                )
        return None

    def _detect_hasty_generalization(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect hasty generalizations"""
        if path.strategy == ReasoningStrategy.INDUCTIVE:
            sample_size = path.metadata.get("sample_size", float('inf'))
            if sample_size < 5:
                return FallacyDetection(
                    fallacy_type="hasty_generalization",
                    description="Drawing broad conclusions from limited examples",
                    location=path.conclusion,
                    severity=0.7,
                    correction_suggestion="Gather more examples before generalizing"
                )
        return None

    def _detect_appeal_to_authority(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect appeals to authority"""
        authority_patterns = ["expert says", "according to", "studies show"]
        for pattern in authority_patterns:
            if pattern in text.lower() and "therefore" in text.lower():
                return FallacyDetection(
                    fallacy_type="appeal_to_authority",
                    description="Relying solely on authority without evidence",
                    location=text,
                    severity=0.5,
                    correction_suggestion="Provide direct evidence beyond authority"
                )
        return None

    def _detect_post_hoc(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect post hoc ergo propter hoc"""
        if "after" in text.lower() and "caused" in text.lower():
            return FallacyDetection(
                fallacy_type="post_hoc",
                description="Assuming correlation implies causation",
                location=text,
                severity=0.7,
                correction_suggestion="Establish causal mechanism, not just sequence"
            )
        return None

    def _detect_bandwagon(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect bandwagon fallacy"""
        patterns = ["everyone", "most people", "popular opinion"]
        for pattern in patterns:
            if pattern in text.lower():
                return FallacyDetection(
                    fallacy_type="bandwagon",
                    description="Appealing to popularity rather than truth",
                    location=text,
                    severity=0.6,
                    correction_suggestion="Evaluate merit independently of popularity"
                )
        return None

    def _detect_false_equivalence(self, text: str, path: ReasoningPath) -> Optional[FallacyDetection]:
        """Detect false equivalence"""
        if "same as" in text.lower() or "no different than" in text.lower():
            return FallacyDetection(
                fallacy_type="false_equivalence",
                description="Treating different things as equivalent",
                location=text,
                severity=0.65,
                correction_suggestion="Acknowledge relevant differences"
            )
        return None

    def _red_team_reasoning(self, path: ReasoningPath) -> List[Dict[str, Any]]:
        """Apply red team challenges to reasoning"""
        challenges = []

        for prompt in self.red_team_prompts:
            challenge = self._generate_challenge(prompt, path)
            if challenge:
                challenges.append({
                    "prompt": prompt,
                    "challenge": challenge,
                    "severity": self._assess_challenge_severity(challenge, path),
                    "suggested_revision": self._suggest_revision(challenge, path)
                })

        return challenges

    def _generate_challenge(self, prompt: str, path: ReasoningPath) -> str:
        """Generate specific challenge based on prompt"""
        if "assumptions" in prompt:
            return self._challenge_assumptions(path)
        elif "evidence" in prompt:
            return self._challenge_evidence(path)
        elif "alternative" in prompt:
            return self._challenge_alternatives(path)
        elif "weakest" in prompt:
            return self._identify_weakest_point(path)
        elif "edge cases" in prompt:
            return self._identify_edge_cases(path)
        elif "biases" in prompt:
            return self._identify_biases(path)
        elif "context" in prompt:
            return self._identify_missing_context(path)
        elif "adversary" in prompt:
            return self._adversarial_attack(path)
        elif "second-order" in prompt:
            return self._identify_second_order_effects(path)
        elif "scale" in prompt:
            return self._test_scalability(path)
        else:
            return ""

    def _refine_reasoning(self, path: ReasoningPath,
                          fallacies: List[FallacyDetection],
                          challenges: List[Dict[str, Any]]) -> ReasoningPath:
        """Refine reasoning based on detected issues"""
        refined_path = ReasoningPath(
            strategy=path.strategy,
            premises=path.premises.copy(),
            steps=path.steps.copy(),
            conclusion=path.conclusion,
            confidence=path.confidence,
            assumptions=path.assumptions.copy(),
            challenges=[c["challenge"] for c in challenges],
            metadata=path.metadata.copy()
        )

        # Address fallacies
        for fallacy in fallacies:
            if fallacy.severity > 0.7:
                refined_path = self._fix_fallacy(refined_path, fallacy)

        # Address challenges
        for challenge in challenges:
            if challenge["severity"] > 0.6:
                refined_path = self._address_challenge(refined_path, challenge)

        # Recalculate confidence
        refined_path.confidence = self._calculate_confidence(
            refined_path, fallacies, challenges
        )

        return refined_path

    def _generate_counterfactuals(self, path: ReasoningPath) -> List[Dict[str, Any]]:
        """Generate counterfactual scenarios"""
        counterfactuals = []

        # Vary premises
        for i, premise in enumerate(path.premises):
            negated = self._negate_premise(premise)
            alternative_path = self._trace_counterfactual(path, i, negated)
            counterfactuals.append({
                "type": "premise_negation",
                "original": premise,
                "counterfactual": negated,
                "result": alternative_path
            })

        # Vary assumptions
        for assumption in path.assumptions:
            relaxed = self._relax_assumption(assumption)
            impact = self._assess_assumption_impact(path, assumption, relaxed)
            counterfactuals.append({
                "type": "assumption_relaxation",
                "original": assumption,
                "counterfactual": relaxed,
                "impact": impact
            })

        return counterfactuals

    def _calculate_confidence(self, path: ReasoningPath,
                              fallacies: List[FallacyDetection],
                              challenges: List[Dict[str, Any]]) -> float:
        """Calculate overall confidence score"""
        base_confidence = path.confidence

        # Reduce for fallacies
        fallacy_penalty = sum(f.severity * 0.1 for f in fallacies)

        # Reduce for unaddressed challenges
        challenge_penalty = sum(c["severity"] * 0.05
                                for c in challenges
                                if c["severity"] > 0.7)

        # Boost for successfully addressed issues
        addressed_bonus = len([c for c in path.challenges]) * 0.02

        final_confidence = base_confidence - fallacy_penalty - challenge_penalty + addressed_bonus

        return max(0.1, min(1.0, final_confidence))

    # Helper methods (implementing stubs for completeness)
    def _extract_premises(self, query: str, context: Dict[str, Any]) -> List[str]:
        """Extract premises from query and context"""
        return [f"Premise from query: {query[:50]}..."]

    def _generate_deductive_steps(self, premises: List[str]) -> List[str]:
        """Generate deductive reasoning steps"""
        return ["If P then Q", "P is true", "Therefore Q"]

    def _derive_conclusion(self, steps: List[str]) -> str:
        """Derive conclusion from reasoning steps"""
        return "Conclusion based on deductive steps"

    def _identify_assumptions(self, premises: List[str]) -> List[str]:
        """Identify underlying assumptions"""
        return ["Assumption 1", "Assumption 2"]

    def _gather_observations(self, query: str, context: Dict[str, Any]) -> List[str]:
        """Gather relevant observations"""
        return ["Observation 1", "Observation 2", "Observation 3"]

    def _identify_pattern(self, observations: List[str]) -> str:
        """Identify pattern in observations"""
        return "Pattern identified in observations"

    def _form_generalization(self, pattern: str) -> str:
        """Form generalization from pattern"""
        return f"General rule based on {pattern}"

    def _extract_observation(self, query: str) -> str:
        """Extract key observation from query"""
        return f"Observation: {query[:50]}..."

    def _generate_hypotheses(self, observation: str, context: Dict[str, Any]) -> List[str]:
        """Generate possible explanations"""
        return ["Hypothesis 1", "Hypothesis 2", "Hypothesis 3"]

    def _select_best_explanation(self, hypotheses: List[str]) -> str:
        """Select most likely explanation"""
        return hypotheses[0] if hypotheses else "No explanation found"

    def _identify_source_domain(self, query: str, context: Dict[str, Any]) -> str:
        """Identify source domain for analogy"""
        return "Source domain"

    def _identify_target_domain(self, query: str) -> str:
        """Identify target domain for analogy"""
        return "Target domain"

    def _create_mappings(self, source: str, target: str) -> List[str]:
        """Create mappings between domains"""
        return [f"{source} maps to {target}"]

    def _transfer_knowledge(self, mappings: List[str]) -> str:
        """Transfer knowledge through analogy"""
        return "Knowledge transferred through analogy"

    def _extract_thesis(self, query: str) -> str:
        """Extract thesis from query"""
        return f"Thesis: {query[:50]}..."

    def _generate_antithesis(self, thesis: str) -> str:
        """Generate opposing view"""
        return f"Antithesis to {thesis}"

    def _create_synthesis(self, thesis: str, antithesis: str) -> str:
        """Create synthesis of opposing views"""
        return f"Synthesis of {thesis} and {antithesis}"

    def _identify_system_components(self, query: str, context: Dict[str, Any]) -> List[str]:
        """Identify system components"""
        return ["Component A", "Component B", "Component C"]

    def _map_interactions(self, components: List[str]) -> List[str]:
        """Map component interactions"""
        return [f"{components[0]} affects {components[1]}"]

    def _identify_emergent_properties(self, interactions: List[str]) -> str:
        """Identify emergent system properties"""
        return "Emergent behavior from interactions"

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate text similarity"""
        # Simple placeholder - in practice use proper similarity metrics
        return 0.5

    def _challenge_assumptions(self, path: ReasoningPath) -> str:
        """Challenge identified assumptions"""
        if path.assumptions:
            return f"Assumption '{path.assumptions[0]}' may not hold because..."
        return "No explicit assumptions to challenge"

    def _challenge_evidence(self, path: ReasoningPath) -> str:
        """Challenge supporting evidence"""
        return "Evidence is limited to specific contexts and may not generalize"

    def _challenge_alternatives(self, path: ReasoningPath) -> str:
        """Suggest alternative explanations"""
        return f"Alternative to '{path.conclusion}': Consider..."

    def _identify_weakest_point(self, path: ReasoningPath) -> str:
        """Identify weakest point in reasoning"""
        return f"Weakest link: {path.steps[0] if path.steps else 'No clear steps'}"

    def _identify_edge_cases(self, path: ReasoningPath) -> str:
        """Identify potential edge cases"""
        return "Edge case: What if the initial conditions are reversed?"

    def _identify_biases(self, path: ReasoningPath) -> str:
        """Identify potential biases"""
        return "Potential confirmation bias in selecting supporting evidence"

    def _identify_missing_context(self, path: ReasoningPath) -> str:
        """Identify missing contextual information"""
        return "Missing context: Historical precedents and cultural factors"

    def _adversarial_attack(self, path: ReasoningPath) -> str:
        """Generate adversarial attack on reasoning"""
        return "An adversary could exploit the assumption that..."

    def _identify_second_order_effects(self, path: ReasoningPath) -> str:
        """Identify second-order effects"""
        return "Second-order effect: This conclusion might lead to..."

    def _test_scalability(self, path: ReasoningPath) -> str:
        """Test if reasoning scales"""
        return "At scale, this reasoning might break due to..."

    def _assess_challenge_severity(self, challenge: str, path: ReasoningPath) -> float:
        """Assess how severe a challenge is"""
        # Placeholder - would implement proper severity assessment
        return 0.7

    def _suggest_revision(self, challenge: str, path: ReasoningPath) -> str:
        """Suggest revision based on challenge"""
        return f"To address '{challenge}', consider..."

    def _fix_fallacy(self, path: ReasoningPath, fallacy: FallacyDetection) -> ReasoningPath:
        """Fix detected fallacy in reasoning path"""
        # Create a copy and fix the specific fallacy
        fixed_path = ReasoningPath(
            strategy=path.strategy,
            premises=path.premises.copy(),
            steps=path.steps.copy(),
            conclusion=path.conclusion,
            confidence=path.confidence * 0.9,  # Reduce confidence
            assumptions=path.assumptions.copy(),
            challenges=path.challenges.copy(),
            metadata=path.metadata.copy()
        )

        # Add note about fixed fallacy
        fixed_path.metadata[f"fixed_{fallacy.fallacy_type}"] = fallacy.correction_suggestion

        return fixed_path

    def _address_challenge(self, path: ReasoningPath, challenge: Dict[str, Any]) -> ReasoningPath:
        """Address a specific challenge"""
        # Create a copy and address the challenge
        addressed_path = ReasoningPath(
            strategy=path.strategy,
            premises=path.premises.copy(),
            steps=path.steps + [f"Addressing: {challenge['challenge']}"],
            conclusion=path.conclusion,
            confidence=path.confidence,
            assumptions=path.assumptions.copy(),
            challenges=path.challenges + [challenge['challenge']],
            metadata=path.metadata.copy()
        )

        addressed_path.metadata["addressed_challenges"] = \
            addressed_path.metadata.get("addressed_challenges", 0) + 1

        return addressed_path

    def _negate_premise(self, premise: str) -> str:
        """Negate a premise for counterfactual"""
        return f"NOT({premise})"

    def _trace_counterfactual(self, path: ReasoningPath, premise_index: int,
                              new_premise: str) -> str:
        """Trace reasoning with counterfactual premise"""
        return f"With '{new_premise}', conclusion would be different"

    def _relax_assumption(self, assumption: str) -> str:
        """Relax an assumption"""
        return f"Relaxed: {assumption} (may not always hold)"

    def _assess_assumption_impact(self, path: ReasoningPath,
                                  original: str, relaxed: str) -> str:
        """Assess impact of relaxing assumption"""
        return f"Impact: Conclusion reliability reduced by 20%"

# -------------------- vre_engine.py --------------------

# ceaf_core/modules/vre_engine/vre_engine.py

from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import logging
import json

# Import components from the same module
from .epistemic_humility import EpistemicHumilityModule
from .principled_reasoning import PrincipledReasoningPathways, ReasoningStrategy
from .ethical_governance import EthicalGovernanceFramework, EthicalPrinciple, ActionType

logger = logging.getLogger(__name__)


@dataclass
class VirtueAssessment:
    """Assessment of cognitive virtues in reasoning"""
    epistemic_humility_score: float
    intellectual_courage_score: float
    perspectival_flexibility_score: float
    intellectual_thoroughness_score: float
    self_correction_capability: float
    overall_virtue_score: float
    recommendations: List[str]
    metadata: Dict[str, Any]


@dataclass
class ReasoningRequest:
    """Request for virtue-guided reasoning"""
    query: str
    context: Dict[str, Any]
    required_virtues: List[str]
    ethical_constraints: List[EthicalPrinciple]
    reasoning_strategy: Optional[ReasoningStrategy]
    metadata: Dict[str, Any]


@dataclass
class ReasoningResponse:
    """Response from virtue reasoning engine"""
    conclusion: str
    reasoning_path: Dict[str, Any]
    virtue_assessment: VirtueAssessment
    ethical_evaluation: Dict[str, Any]
    confidence: float
    alternatives: List[Dict[str, Any]]
    metadata: Dict[str, Any]


class VirtueReasoningEngine:
    """
    Main engine orchestrating virtue-based reasoning with ethical governance
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._default_config()

        # Initialize sub-modules
        self.epistemic_module = EpistemicHumilityModule()
        self.reasoning_pathways = PrincipledReasoningPathways()
        self.ethical_framework = EthicalGovernanceFramework()

        # Virtue cultivation strategies
        self.virtue_strategies = self._initialize_virtue_strategies()

        # Reasoning history for learning
        self.reasoning_history = []

        logger.info("VirtueReasoningEngine initialized")

    def _default_config(self) -> Dict[str, Any]:
        """Default configuration"""
        return {
            "min_confidence_threshold": 0.3,
            "max_confidence_threshold": 0.95,
            "virtue_weight": 0.3,
            "ethical_weight": 0.3,
            "reasoning_weight": 0.4,
            "enable_counterfactuals": True,
            "max_alternatives": 3,
            "learning_rate": 0.1
        }

    def _initialize_virtue_strategies(self) -> Dict[str, Callable]:
        """Initialize strategies for cultivating virtues"""
        return {
            "epistemic_humility": self._cultivate_epistemic_humility,
            "intellectual_courage": self._cultivate_intellectual_courage,
            "perspectival_flexibility": self._cultivate_perspectival_flexibility,
            "intellectual_thoroughness": self._cultivate_intellectual_thoroughness,
            "self_correction": self._cultivate_self_correction
        }

    def process_reasoning_request(self, request: ReasoningRequest) -> ReasoningResponse:
        """
        Process a reasoning request with virtue guidance and ethical constraints
        """
        logger.info(f"Processing reasoning request: {request.query[:50]}...")

        # Step 1: Epistemic assessment
        epistemic_state = self.epistemic_module.assess_epistemic_state(
            request.query, request.context
        )

        # Step 2: Apply principled reasoning
        reasoning_result = self.reasoning_pathways.apply_reasoning(
            request.query,
            request.context,
            request.reasoning_strategy
        )

        # Step 3: Ethical evaluation
        ethical_eval = self.ethical_framework.evaluate_action(
            ActionType.REASONING,
            {
                "query": request.query,
                "reasoning": reasoning_result,
                "context": request.context
            },
            request.ethical_constraints
        )

        # Step 4: Virtue assessment
        virtue_assessment = self._assess_virtues(
            request, epistemic_state, reasoning_result, ethical_eval
        )

        # Step 5: Generate alternatives if needed
        alternatives = self._generate_alternatives(
            request, reasoning_result, virtue_assessment
        )

        # Step 6: Synthesize final response
        response = self._synthesize_response(
            request, epistemic_state, reasoning_result,
            ethical_eval, virtue_assessment, alternatives
        )

        # Step 7: Learn from this reasoning episode
        self._update_learning(request, response)

        return response

    def _assess_virtues(self, request: ReasoningRequest,
                        epistemic_state: Dict[str, Any],
                        reasoning_result: Dict[str, Any],
                        ethical_eval: Dict[str, Any]) -> VirtueAssessment:
        """Assess cognitive virtues demonstrated in reasoning"""

        # Calculate individual virtue scores
        humility_score = self._calculate_humility_score(epistemic_state)
        courage_score = self._calculate_courage_score(reasoning_result)
        flexibility_score = self._calculate_flexibility_score(reasoning_result)
        thoroughness_score = self._calculate_thoroughness_score(reasoning_result)
        correction_score = self._calculate_correction_score(reasoning_result)

        # Overall virtue score
        overall_score = (
                humility_score * 0.25 +
                courage_score * 0.20 +
                flexibility_score * 0.20 +
                thoroughness_score * 0.20 +
                correction_score * 0.15
        )

        # Generate recommendations
        recommendations = self._generate_virtue_recommendations(
            humility_score, courage_score, flexibility_score,
            thoroughness_score, correction_score
        )

        return VirtueAssessment(
            epistemic_humility_score=humility_score,
            intellectual_courage_score=courage_score,
            perspectival_flexibility_score=flexibility_score,
            intellectual_thoroughness_score=thoroughness_score,
            self_correction_capability=correction_score,
            overall_virtue_score=overall_score,
            recommendations=recommendations,
            metadata={
                "timestamp": datetime.now().isoformat(),
                "reasoning_strategy": reasoning_result.get("metadata", {}).get("strategy_used")
            }
        )

    def _calculate_humility_score(self, epistemic_state: Dict[str, Any]) -> float:
        """Calculate epistemic humility score"""
        uncertainty = epistemic_state.get("uncertainty_level", 0.5)
        contradictions = len(epistemic_state.get("contradictions", []))
        confidence = epistemic_state.get("confidence", 0.5)

        # High humility = acknowledging uncertainty and limitations
        humility_score = (
                (uncertainty * 0.3) +  # Acknowledging uncertainty
                (min(contradictions * 0.1, 0.3)) +  # Recognizing contradictions
                ((1 - min(confidence, 0.95)) * 0.4)  # Not overconfident
        )

        return min(1.0, humility_score)

    def _calculate_courage_score(self, reasoning_result: Dict[str, Any]) -> float:
        """Calculate intellectual courage score"""
        challenges = reasoning_result.get("red_team_challenges", [])
        alternatives = reasoning_result.get("counterfactuals", [])

        # Courage = willingness to challenge and explore
        courage_score = (
                min(len(challenges) * 0.1, 0.5) +
                min(len(alternatives) * 0.1, 0.5)
        )

        return courage_score

    def _calculate_flexibility_score(self, reasoning_result: Dict[str, Any]) -> float:
        """Calculate perspectival flexibility score"""
        strategy = reasoning_result.get("metadata", {}).get("strategy_used", "")
        counterfactuals = len(reasoning_result.get("counterfactuals", []))

        # Flexibility = considering multiple perspectives
        flexibility_score = 0.3  # Base score

        if strategy in ["dialectical", "systems"]:
            flexibility_score += 0.3

        flexibility_score += min(counterfactuals * 0.1, 0.4)

        return min(1.0, flexibility_score)

    def _calculate_thoroughness_score(self, reasoning_result: Dict[str, Any]) -> float:
        """Calculate intellectual thoroughness score"""
        steps = len(reasoning_result.get("primary_path", {}).get("steps", []))
        fallacies_checked = len(reasoning_result.get("fallacies_detected", []))

        # Thoroughness = comprehensive analysis
        thoroughness_score = (
                min(steps * 0.1, 0.5) +
                min(fallacies_checked * 0.05, 0.5)
        )

        return thoroughness_score

    def _calculate_correction_score(self, reasoning_result: Dict[str, Any]) -> float:
        """Calculate self-correction capability score"""
        addressed_challenges = reasoning_result.get("primary_path", {}).get(
            "metadata", {}
        ).get("addressed_challenges", 0)

        fixed_fallacies = sum(
            1 for key in reasoning_result.get("primary_path", {}).get("metadata", {})
            if key.startswith("fixed_")
        )

        # Self-correction = addressing identified issues
        correction_score = min(
            (addressed_challenges * 0.2) + (fixed_fallacies * 0.3),
            1.0
        )

        return correction_score

    def _generate_virtue_recommendations(self, humility: float, courage: float,
                                         flexibility: float, thoroughness: float,
                                         correction: float) -> List[str]:
        """Generate recommendations for virtue improvement"""
        recommendations = []

        if humility < 0.5:
            recommendations.append(
                "Increase epistemic humility by acknowledging uncertainties"
            )

        if courage < 0.5:
            recommendations.append(
                "Demonstrate more intellectual courage by challenging assumptions"
            )

        if flexibility < 0.5:
            recommendations.append(
                "Improve perspectival flexibility by considering more viewpoints"
            )

        if thoroughness < 0.5:
            recommendations.append(
                "Enhance thoroughness with more comprehensive analysis"
            )

        if correction < 0.5:
            recommendations.append(
                "Develop self-correction by addressing identified issues"
            )

        return recommendations

    def _generate_alternatives(self, request: ReasoningRequest,
                               reasoning_result: Dict[str, Any],
                               virtue_assessment: VirtueAssessment) -> List[Dict[str, Any]]:
        """Generate alternative reasoning paths"""
        alternatives = []

        if not self.config["enable_counterfactuals"]:
            return alternatives

        # Try different reasoning strategies
        current_strategy = reasoning_result.get("metadata", {}).get("strategy_used")

        for strategy in ReasoningStrategy:
            if strategy.value != current_strategy:
                alt_result = self.reasoning_pathways.apply_reasoning(
                    request.query,
                    request.context,
                    strategy
                )

                alternatives.append({
                    "strategy": strategy.value,
                    "conclusion": alt_result["primary_path"].conclusion,
                    "confidence": alt_result["confidence"],
                    "key_difference": self._identify_key_difference(
                        reasoning_result, alt_result
                    )
                })

                if len(alternatives) >= self.config["max_alternatives"]:
                    break

        return alternatives

    def _identify_key_difference(self, result1: Dict[str, Any],
                                 result2: Dict[str, Any]) -> str:
        """Identify key difference between two reasoning results"""
        # Simplified comparison
        if result1["primary_path"].conclusion != result2["primary_path"].conclusion:
            return "Different conclusion reached"
        elif result1["confidence"] != result2["confidence"]:
            return f"Different confidence levels ({result1['confidence']:.2f} vs {result2['confidence']:.2f})"
        else:
            return "Different reasoning steps"

    def _synthesize_response(self, request: ReasoningRequest,
                             epistemic_state: Dict[str, Any],
                             reasoning_result: Dict[str, Any],
                             ethical_eval: Dict[str, Any],
                             virtue_assessment: VirtueAssessment,
                             alternatives: List[Dict[str, Any]]) -> ReasoningResponse:
        """Synthesize final reasoning response"""

        # Calculate overall confidence
        confidence = self._calculate_overall_confidence(
            epistemic_state, reasoning_result, ethical_eval, virtue_assessment
        )

        # Prepare conclusion with appropriate caveats
        conclusion = self._prepare_conclusion(
            reasoning_result["primary_path"].conclusion,
            confidence,
            epistemic_state
        )

        return ReasoningResponse(
            conclusion=conclusion,
            reasoning_path=reasoning_result,
            virtue_assessment=virtue_assessment,
            ethical_evaluation=ethical_eval,
            confidence=confidence,
            alternatives=alternatives,
            metadata={
                "timestamp": datetime.now().isoformat(),
                "request_id": request.metadata.get("request_id", ""),
                "processing_time": request.metadata.get("processing_time", 0),
                "epistemic_state": epistemic_state
            }
        )

    def _calculate_overall_confidence(self, epistemic_state: Dict[str, Any],
                                      reasoning_result: Dict[str, Any],
                                      ethical_eval: Dict[str, Any],
                                      virtue_assessment: VirtueAssessment) -> float:
        """Calculate overall confidence in reasoning"""

        # Weighted combination of different confidence factors
        reasoning_confidence = reasoning_result["confidence"]
        epistemic_confidence = 1 - epistemic_state.get("uncertainty_level", 0.5)
        ethical_confidence = ethical_eval["confidence"]
        virtue_confidence = virtue_assessment.overall_virtue_score

        overall_confidence = (
                reasoning_confidence * self.config["reasoning_weight"] +
                epistemic_confidence * 0.2 +
                ethical_confidence * self.config["ethical_weight"] +
                virtue_confidence * self.config["virtue_weight"]
        )

        # Apply bounds
        return max(
            self.config["min_confidence_threshold"],
            min(self.config["max_confidence_threshold"], overall_confidence)
        )

    def _prepare_conclusion(self, raw_conclusion: str, confidence: float,
                            epistemic_state: Dict[str, Any]) -> str:
        """Prepare conclusion with appropriate caveats"""

        caveats = []

        if confidence < 0.5:
            caveats.append("Low confidence - this conclusion is tentative")

        if epistemic_state.get("uncertainty_level", 0) > 0.7:
            caveats.append("High uncertainty in available information")

        if epistemic_state.get("contradictions", []):
            caveats.append("Some contradictory evidence exists")

        if caveats:
            caveat_text = ". ".join(caveats)
            return f"{raw_conclusion}\n\n[Note: {caveat_text}]"

        return raw_conclusion

    def _update_learning(self, request: ReasoningRequest,
                         response: ReasoningResponse) -> None:
        """Update learning based on reasoning episode"""

        # Store in history
        self.reasoning_history.append({
            "request": request,
            "response": response,
            "timestamp": datetime.now().isoformat()
        })

        # Update virtue cultivation strategies based on assessment
        for virtue, score in [
            ("epistemic_humility", response.virtue_assessment.epistemic_humility_score),
            ("intellectual_courage", response.virtue_assessment.intellectual_courage_score),
            ("perspectival_flexibility", response.virtue_assessment.perspectival_flexibility_score),
            ("intellectual_thoroughness", response.virtue_assessment.intellectual_thoroughness_score),
            ("self_correction", response.virtue_assessment.self_correction_capability)
        ]:
            if score < 0.7:  # Needs improvement
                self._adjust_virtue_strategy(virtue, score)

    def _adjust_virtue_strategy(self, virtue: str, current_score: float) -> None:
        """Adjust strategies for cultivating specific virtues"""
        logger.info(f"Adjusting strategy for {virtue} (current score: {current_score:.2f})")

        # This would implement learning algorithms to improve virtue cultivation
        # For now, just log the need for adjustment
        pass

    def _cultivate_epistemic_humility(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Strategy for cultivating epistemic humility"""
        return {
            "prompts": [
                "What might I be missing?",
                "What are the limits of my knowledge here?",
                "How certain can I really be?"
            ],
            "techniques": [
                "explicitly_state_uncertainties",
                "acknowledge_knowledge_gaps",
                "qualify_strong_claims"
            ]
        }

    def _cultivate_intellectual_courage(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Strategy for cultivating intellectual courage"""
        return {
            "prompts": [
                "What uncomfortable truth might apply here?",
                "What would happen if I challenged this assumption?",
                "What if the opposite were true?"
            ],
            "techniques": [
                "challenge_popular_beliefs",
                "explore_unpopular_positions",
                "admit_errors_openly"
            ]
        }

    def _cultivate_perspectival_flexibility(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Strategy for cultivating perspectival flexibility"""
        return {
            "prompts": [
                "How would this look from another perspective?",
                "What would someone who disagrees say?",
                "What cultural lens am I applying?"
            ],
            "techniques": [
                "steelman_opposing_views",
                "rotate_perspectives",
                "seek_diverse_inputs"
            ]
        }

    def _cultivate_intellectual_thoroughness(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Strategy for cultivating intellectual thoroughness"""
        return {
            "prompts": [
                "Have I examined all relevant evidence?",
                "What details am I glossing over?",
                "Where do I need to dig deeper?"
            ],
            "techniques": [
                "systematic_analysis",
                "exhaustive_consideration",
                "detailed_examination"
            ]
        }

    def _cultivate_self_correction(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Strategy for cultivating self-correction"""
        return {
            "prompts": [
                "Where might I be wrong?",
                "What would change my mind?",
                "How can I improve this reasoning?"
            ],
            "techniques": [
                "active_error_detection",
                "iterative_refinement",
                "feedback_integration"
            ]
        }

    # ORA Integration callback
    def vre_process_ora_request(self, ora_query: str, ora_context: Dict[str, Any],
                                ncf_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process ORA request with VRE enhancement
        This is the main integration point with the ORA
        """

        # Extract ethical constraints from NCF
        ethical_constraints = self._extract_ethical_constraints(ncf_params)

        # Determine required virtues based on query type
        required_virtues = self._determine_required_virtues(ora_query, ora_context)

        # Create reasoning request
        request = ReasoningRequest(
            query=ora_query,
            context=ora_context,
            required_virtues=required_virtues,
            ethical_constraints=ethical_constraints,
            reasoning_strategy=None,  # Let the system choose
            metadata={
                "source": "ORA",
                "ncf_params": ncf_params,
                "request_id": ora_context.get("request_id", ""),
                "timestamp": datetime.now().isoformat()
            }
        )

        # Process through VRE
        response = self.process_reasoning_request(request)

        # Format for ORA consumption
        return {
            "enhanced_response": response.conclusion,
            "virtue_modulation": {
                "confidence_adjustment": response.confidence,
                "virtue_scores": {
                    "epistemic_humility": response.virtue_assessment.epistemic_humility_score,
                    "intellectual_courage": response.virtue_assessment.intellectual_courage_score,
                    "perspectival_flexibility": response.virtue_assessment.perspectival_flexibility_score,
                    "thoroughness": response.virtue_assessment.intellectual_thoroughness_score,
                    "self_correction": response.virtue_assessment.self_correction_capability
                },
                "ethical_alignment": response.ethical_evaluation["score"]
            },
            "reasoning_metadata": {
                "strategy_used": response.reasoning_path["metadata"]["strategy_used"],
                "fallacies_detected": len(response.reasoning_path.get("fallacies_detected", [])),
                "alternatives_considered": len(response.alternatives),
                "virtue_recommendations": response.virtue_assessment.recommendations
            },
            "suggested_ncf_adjustments": self._suggest_ncf_adjustments(
                response, ncf_params
            )
        }

    def _extract_ethical_constraints(self, ncf_params: Dict[str, Any]) -> List[EthicalPrinciple]:
        """Extract ethical constraints from NCF parameters"""
        constraints = []

        # Map NCF parameters to ethical principles
        if ncf_params.get("ethical_framework"):
            framework = ncf_params["ethical_framework"]
            if "harm_prevention" in framework:
                constraints.append(EthicalPrinciple.HARM_PREVENTION)
            if "autonomy" in framework:
                constraints.append(EthicalPrinciple.AUTONOMY)
            if "fairness" in framework:
                constraints.append(EthicalPrinciple.FAIRNESS)
            if "transparency" in framework:
                constraints.append(EthicalPrinciple.TRANSPARENCY)

        # Default constraints if none specified
        if not constraints:
            constraints = [
                EthicalPrinciple.HARM_PREVENTION,
                EthicalPrinciple.TRANSPARENCY
            ]

        return constraints

    def _determine_required_virtues(self, query: str, context: Dict[str, Any]) -> List[str]:
        """Determine which virtues are most important for this query"""
        required = []

        # Analyze query characteristics
        query_lower = query.lower()

        if any(word in query_lower for word in ["uncertain", "maybe", "possibly"]):
            required.append("epistemic_humility")

        if any(word in query_lower for word in ["challenge", "question", "debate"]):
            required.append("intellectual_courage")

        if any(word in query_lower for word in ["perspective", "viewpoint", "consider"]):
            required.append("perspectival_flexibility")

        if any(word in query_lower for word in ["analyze", "examine", "investigate"]):
            required.append("intellectual_thoroughness")

        if any(word in query_lower for word in ["correct", "revise", "improve"]):
            required.append("self_correction")

        # Default if none detected
        if not required:
            required = ["epistemic_humility", "intellectual_thoroughness"]

        return required

    def _suggest_ncf_adjustments(self, response: ReasoningResponse,
                                 current_ncf: Dict[str, Any]) -> Dict[str, Any]:
        """Suggest NCF parameter adjustments based on reasoning results"""
        suggestions = {}

        # Adjust based on confidence
        if response.confidence < 0.4:
            suggestions["conceptual_entropy"] = min(
                current_ncf.get("conceptual_entropy", 0.5) + 0.1,
                1.0
            )
            suggestions["narrative_coherence"] = max(
                current_ncf.get("narrative_coherence", 0.5) - 0.1,
                0.0
            )
        elif response.confidence > 0.8:
            suggestions["conceptual_entropy"] = max(
                current_ncf.get("conceptual_entropy", 0.5) - 0.1,
                0.0
            )

        # Adjust based on virtue assessment
        if response.virtue_assessment.overall_virtue_score < 0.5:
            suggestions["virtue_emphasis"] = min(
                current_ncf.get("virtue_emphasis", 0.5) + 0.2,
                1.0
            )

        return suggestions

# -------------------- __init__.py --------------------

# VRE Engine


# -------------------- mbs_memory_service.py --------------------

# ceaf_core/services/mbs_memory_service.py
import asyncio
import logging
import json
import os
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple, cast, Literal
import re
import numpy as np  # For type hinting if used directly, though similarity is in utils

from google.adk.memory import BaseMemoryService
from google.adk.sessions import Session as AdkSession
from google.adk.events import Event as AdkEvent
from google.genai.types import Part as AdkPart

# --- Embedding Imports ---
from ..utils.embedding_utils import get_embedding_client, compute_adaptive_similarity, EmbeddingClient

logger = logging.getLogger(__name__)

# --- Constants ---
DEFAULT_MEMORY_STORE_PATH = "./data/mbs_memory_store"
MEMORY_STORE_ROOT_DIR = Path(os.getenv("MBS_MEMORY_STORE_PATH", DEFAULT_MEMORY_STORE_PATH))

AGGREGATED_EXPLICIT_MEMORIES_FILENAME = "all_explicit_memories.jsonl"
GOALS_STORE_FILENAME = "all_goal_records.jsonl"
KG_ENTITIES_FILENAME = "all_kg_entities.jsonl"
KG_RELATIONS_FILENAME = "all_kg_relations.jsonl"
# Add filenames for other types if they get dedicated stores
PROCEDURAL_MEMORIES_FILENAME = "all_procedural_memories.jsonl"
EMOTIONAL_MEMORIES_FILENAME = "all_emotional_memories.jsonl"

SESSION_HISTORY_FOR_MEMORY_TURNS = 10
SELF_MODEL_MEMORY_ID = "ceaf_self_model_singleton_v1"

SEMANTIC_CONNECTION_THRESHOLD = float(os.getenv("MBS_SEMANTIC_CONNECTION_THRESHOLD", "0.78")) # Higher threshold for initial auto-connect
KEYWORD_CONNECTION_THRESHOLD_COUNT = int(os.getenv("MBS_KEYWORD_CONNECTION_THRESHOLD_COUNT", "2")) # Min common keywords to connect
MAX_RELATED_IDS_PER_MEMORY = int(os.getenv("MBS_MAX_RELATED_IDS", "10")) # Prevent overly dense connections initially

# --- Scoring Configuration ---
DEFAULT_SEMANTIC_SCORE_WEIGHT = float(os.getenv("MBS_SEMANTIC_SCORE_WEIGHT", "0.6"))
DEFAULT_KEYWORD_SCORE_WEIGHT = float(os.getenv("MBS_KEYWORD_SCORE_WEIGHT", "0.4"))
CONTEXT_QUERY_DELIMITER = " <CEAF_CONTEXT_SEPARATOR> "  # From memory_tools.py

# --- Type Imports and Placeholders ---
MEMORY_TYPES_LOADED_SUCCESSFULLY = False
MEMORY_LIFECYCLE_MANAGER_LOADED = False

try:
    from ..modules.memory_blossom.memory_types import (
        AnyMemoryType, BaseMemory, ExplicitMemory, ExplicitMemoryContent,
        MemorySourceType, MemorySalience, GoalRecord, GoalStatus,  # Added GoalStatus
        KGEntityRecord, KGRelationRecord, KGEntityType,
        EmotionalMemory, EmotionalContext, EmotionalTag,  # Added Emotional
        ProceduralMemory, ProceduralStep  # Added Procedural
        # Import other types like Flashbulb, Somatic, Liminal, Generative as they become fully integrated
    )

    MEMORY_TYPES_LOADED_SUCCESSFULLY = True
    logging.info("MBS Memory Service: Successfully loaded CEAF memory_types.")
except ImportError as e_mem_types:
    logging.error(f"MBS Memory Service: Could not import full CEAF memory_types: {e_mem_types}. Using placeholders.")


    # Placeholder definitions (ensure all used types have placeholders)
    class BaseMemoryPydanticPlaceholder:
        def __init__(self, **kwargs):
            self.memory_id = kwargs.get('memory_id', f'dummy_{time.time_ns()}')
            self.timestamp = kwargs.get('timestamp', time.time())
            self.last_accessed_ts: Optional[float] = None
            self.access_count: int = 0
            self.dynamic_salience_score: float = 0.5
            self.decay_rate: float = 0.01
            self.salience: str = "medium"
            self.source_type: str = "unknown"
            self.memory_type: str = kwargs.get('memory_type', 'unknown_placeholder')
            self.embedding_reference: Optional[str] = None
            self.keywords: List[str] = kwargs.get('keywords', [])
            self.narrative_thread_id: Optional[str] = kwargs.get('narrative_thread_id')  # For filtering
            self.entity_id_str: Optional[str] = kwargs.get('entity_id_str')
            self.relation_id_str: Optional[str] = kwargs.get('relation_id_str')
            self.source_entity_id_str: Optional[str] = kwargs.get('source_entity_id_str')
            self.target_entity_id_str: Optional[str] = kwargs.get('target_entity_id_str')
            self.relation_label: Optional[str] = kwargs.get('relation_label')
            self.goal_description: Optional[str] = kwargs.get('goal_description')  # For GoalRecord
            self.label: Optional[str] = kwargs.get('label')  # For KGEntityRecord
            self.description: Optional[str] = kwargs.get('description')  # For KGEntityRecord & KGRelationRecord
            self.primary_emotion: Optional[str] = kwargs.get('primary_emotion')  # For EmotionalMemory
            self.context: Any = kwargs.get('context')  # For EmotionalMemory context
            self.procedure_name: Optional[str] = kwargs.get('procedure_name')  # For ProceduralMemory
            self.steps: List[Any] = kwargs.get('steps', [])  # For ProceduralMemory
            self.__dict__.update(kwargs)

        def model_dump_json(self, **kwargs): return json.dumps(self.__dict__)

        def model_dump(self, **kwargs): return self.__dict__

        def mark_accessed(self): self.last_accessed_ts = time.time(); self.access_count += 1

        @property
        def content(self): return self  # For ExplicitMemory placeholder

        @property
        def text_content(self): return self.__dict__.get('text_content', '')  # For ExplicitMemory placeholder


    class ExplicitMemory(BaseMemoryPydanticPlaceholder):
        memory_type = "explicit"  # type: ignore


    class ExplicitMemoryContent(BaseMemoryPydanticPlaceholder):
        pass  # type: ignore


    class GoalRecord(BaseMemoryPydanticPlaceholder):
        memory_type = "goal_record"  # type: ignore


    class GoalStatus:
        PENDING = "pending"; ACTIVE = "active"; COMPLETED = "completed"; FAILED = "failed"; PAUSED = "paused"  # type: ignore


    class KGEntityRecord(BaseMemoryPydanticPlaceholder):
        memory_type = "kg_entity_record"  # type: ignore


    class KGRelationRecord(BaseMemoryPydanticPlaceholder):
        memory_type = "kg_relation_record"  # type: ignore


    class KGEntityType:
        OTHER = "other"  # type: ignore


    class EmotionalMemory(BaseMemoryPydanticPlaceholder):
        memory_type = "emotional"  # type: ignore


    class EmotionalContext(BaseMemoryPydanticPlaceholder):
        pass  # type: ignore


    class EmotionalTag:
        NEUTRAL = "neutral"  # type: ignore


    class ProceduralMemory(BaseMemoryPydanticPlaceholder):
        memory_type = "procedural"  # type: ignore


    class ProceduralStep(BaseMemoryPydanticPlaceholder):
        pass  # type: ignore


    class MemorySourceType:
        USER_INTERACTION = "user_interaction"; ORA_RESPONSE = "ora_response"; TOOL_OUTPUT = "tool_output"; INTERNAL_REFLECTION = "internal_reflection"  # type: ignore


    class MemorySalience:
        MEDIUM = "medium"; LOW = "low"; HIGH = "high"; CRITICAL = "critical"  # type: ignore


    AnyMemoryType = Union[
        ExplicitMemory, GoalRecord, KGEntityRecord, KGRelationRecord, EmotionalMemory, ProceduralMemory]  # type: ignore

try:
    from ..modules.memory_blossom.memory_lifecycle_manager import (
        initialize_dynamic_salience, update_dynamic_salience, apply_decay_to_all_memories,
        archive_or_forget_low_salience_memories, DEFAULT_ARCHIVE_SALIENCE_THRESHOLD,
        EPHEMERAL_SOURCE_TYPES_FOR_DELETION
    )

    MEMORY_LIFECYCLE_MANAGER_LOADED = True
    logging.info("MBS Memory Service: Successfully loaded CEAF memory_lifecycle_manager.")
except ImportError as e_lifecycle:
    logging.error(
        f"MBS Memory Service: Could not import CEAF memory_lifecycle_manager: {e_lifecycle}. Lifecycle features limited.")


    # Dummy lifecycle functions
    def initialize_dynamic_salience(mem):
        pass


    def update_dynamic_salience(mem, **kwargs):
        pass


    def apply_decay_to_all_memories(mems):
        pass


    def archive_or_forget_low_salience_memories(mems, **kwargs):
        return mems, [], []


    DEFAULT_ARCHIVE_SALIENCE_THRESHOLD = 0.1
    EPHEMERAL_SOURCE_TYPES_FOR_DELETION = []


class MBSMemoryService(BaseMemoryService):
    def __init__(self, memory_store_path: Optional[Union[str, Path]] = None):
        self.store_path = Path(memory_store_path or MEMORY_STORE_ROOT_DIR)
        self.store_path.mkdir(parents=True, exist_ok=True)

        self._embedding_client: EmbeddingClient = get_embedding_client()
        logger.info(f"MBS Memory Service: Using EmbeddingClient with provider '{self._embedding_client.provider}' "
                    f"and default model '{self._embedding_client.default_model_name}'.")

        self._embedding_cache: Dict[str, List[float]] = {}

        self._in_memory_explicit_cache: List[ExplicitMemory] = self._load_all_explicit_memories()
        self._in_memory_goals_cache: List[GoalRecord] = self._load_all_goal_records()
        self._in_memory_kg_entities_cache: List[KGEntityRecord] = self._load_all_kg_entities()
        self._in_memory_kg_relations_cache: List[KGRelationRecord] = self._load_all_kg_relations()
        # Add caches for newly integrated types
        self._in_memory_emotional_cache: List[EmotionalMemory] = self._load_all_emotional_memories()
        self._in_memory_procedural_cache: List[ProceduralMemory] = self._load_all_procedural_memories()

        # Initialize lifecycle attributes for all loaded memories
        all_memory_caches_for_init = [
            (self._in_memory_explicit_cache, 0.01, 0.5),
            (self._in_memory_goals_cache, 0.02, 0.6),
            (self._in_memory_kg_entities_cache, 0.005, 0.7),
            (self._in_memory_kg_relations_cache, 0.005, 0.65),
            (self._in_memory_emotional_cache, 0.015, 0.55),
            (self._in_memory_procedural_cache, 0.008, 0.6)
        ]
        if MEMORY_LIFECYCLE_MANAGER_LOADED:
            for cache, default_decay, default_salience_on_load in all_memory_caches_for_init:
                for mem_item in cache:
                    if not hasattr(mem_item, 'dynamic_salience_score'):
                        setattr(mem_item, 'dynamic_salience_score', default_salience_on_load)
                    if not hasattr(mem_item, 'decay_rate'):
                        setattr(mem_item, 'decay_rate', default_decay)
                    if not hasattr(mem_item, 'access_count'):
                        setattr(mem_item, 'access_count', 0)
                    if not hasattr(mem_item, 'last_accessed_ts') or getattr(mem_item, 'last_accessed_ts') is None:
                        setattr(mem_item, 'last_accessed_ts', getattr(mem_item, 'timestamp', time.time()))
                    initialize_dynamic_salience(mem_item)

        logger.info(
            f"MBSMemoryService initialized. Storage path: {self.store_path}. "
            f"Loaded: {len(self._in_memory_explicit_cache)} explicit, "
            f"{len(self._in_memory_goals_cache)} goals, "
            f"{len(self._in_memory_kg_entities_cache)} KG entities, "
            f"{len(self._in_memory_kg_relations_cache)} KG relations, "
            f"{len(self._in_memory_emotional_cache)} emotional, "
            f"{len(self._in_memory_procedural_cache)} procedural."
        )
        logger.info(f"Embeddings in cache after load: {len(self._embedding_cache)}")

        self._stop_background_tasks = asyncio.Event()
        self._background_tasks: List[asyncio.Task] = []

    async def build_initial_connection_graph(self):
        """
        Builds an initial graph of connections between memories at startup.
        This involves:
        1. Connecting memories based on semantic similarity of their embeddings.
        2. Connecting memories based on shared keywords.
        3. Reinforcing KG structure by linking entities and relations.
        4. Creating links based on explicit cross-references in memory content.
        Modifies the 'related_memory_ids' field of BaseMemory objects in-place
        and then rewrites the persistent stores.
        """
        logger.info("MBSMemoryService: Starting initial memory connection graph build...")
        changes_made_to_any_cache = False

        all_memory_caches_for_graph: Dict[str, List[AnyMemoryType]] = {
            "explicit": self._in_memory_explicit_cache,
            "goals": self._in_memory_goals_cache,
            "kg_entities": self._in_memory_kg_entities_cache,
            "kg_relations": self._in_memory_kg_relations_cache,
            "emotional": self._in_memory_emotional_cache,
            "procedural": self._in_memory_procedural_cache,
        }

        # Helper to safely add a connection
        def _add_connection_if_new(mem1: BaseMemory, mem2_id: str) -> bool:
            if not hasattr(mem1, 'related_memory_ids'):
                setattr(mem1, 'related_memory_ids', [])
            if mem2_id != mem1.memory_id and mem2_id not in mem1.related_memory_ids and len(
                    mem1.related_memory_ids) < MAX_RELATED_IDS_PER_MEMORY:
                mem1.related_memory_ids.append(mem2_id)
                return True
            return False

        # Consolidate all memories into one list for easier pairwise comparison for semantic/keyword
        all_memories_flat: List[BaseMemory] = []
        for cache_name, cache_list in all_memory_caches_for_graph.items():
            all_memories_flat.extend(cast(List[BaseMemory], cache_list))

        logger.info(f"MBS: Processing {len(all_memories_flat)} total memories for connections.")

        # 1. Semantic Similarity & 2. Keyword Overlap
        # This is N^2, so be mindful for very large initial memory stores.
        # For now, we'll iterate. Optimized approaches (e.g., FAISS for semantic) are for later.
        for i in range(len(all_memories_flat)):
            mem1 = all_memories_flat[i]
            if not hasattr(mem1, 'memory_id'): continue  # Skip if malformed

            for j in range(i + 1, len(all_memories_flat)):
                mem2 = all_memories_flat[j]
                if not hasattr(mem2, 'memory_id'): continue

                # Ensure embeddings are present for semantic comparison
                if mem1.memory_id in self._embedding_cache and mem2.memory_id in self._embedding_cache:
                    emb1 = self._embedding_cache[mem1.memory_id]
                    emb2 = self._embedding_cache[mem2.memory_id]
                    semantic_sim = compute_adaptive_similarity(emb1, emb2)

                    if semantic_sim >= SEMANTIC_CONNECTION_THRESHOLD:
                        if _add_connection_if_new(mem1, mem2.memory_id): changes_made_to_any_cache = True
                        if _add_connection_if_new(mem2, mem1.memory_id): changes_made_to_any_cache = True
                        logger.debug(
                            f"MBS Graph: Semantic link ({semantic_sim:.2f}) between {mem1.memory_id} and {mem2.memory_id}")
                        continue  # Prioritize semantic link over keyword if strong enough

                # Keyword Overlap (if not already semantically linked strongly)
                keywords1 = set(getattr(mem1, 'keywords', []))
                keywords2 = set(getattr(mem2, 'keywords', []))
                if keywords1 and keywords2:  # Both must have keywords
                    common_keywords = keywords1.intersection(keywords2)
                    if len(common_keywords) >= KEYWORD_CONNECTION_THRESHOLD_COUNT:
                        if _add_connection_if_new(mem1, mem2.memory_id): changes_made_to_any_cache = True
                        if _add_connection_if_new(mem2, mem1.memory_id): changes_made_to_any_cache = True
                        logger.debug(
                            f"MBS Graph: Keyword link ({len(common_keywords)} common) between {mem1.memory_id} and {mem2.memory_id}")

        # 3. KG Structural Links
        logger.info("MBS Graph: Processing KG structural links...")
        entity_map_by_id_str: Dict[str, KGEntityRecord] = {
            getattr(e, 'entity_id_str'): e for e in self._in_memory_kg_entities_cache  # type: ignore
            if hasattr(e, 'entity_id_str')
        }

        for rel_mem_any in self._in_memory_kg_relations_cache:
            rel_mem = cast(KGRelationRecord, rel_mem_any)  # Assuming correct type in this cache
            if not hasattr(rel_mem, 'memory_id') or \
                    not hasattr(rel_mem, 'source_entity_id_str') or \
                    not hasattr(rel_mem, 'target_entity_id_str'):
                continue

            source_entity_id = getattr(rel_mem, 'source_entity_id_str')
            target_entity_id = getattr(rel_mem, 'target_entity_id_str')

            source_entity_mem = entity_map_by_id_str.get(source_entity_id)
            target_entity_mem = entity_map_by_id_str.get(target_entity_id)

            if source_entity_mem:
                if _add_connection_if_new(cast(BaseMemory, source_entity_mem),
                                          rel_mem.memory_id): changes_made_to_any_cache = True
                if _add_connection_if_new(cast(BaseMemory, rel_mem),
                                          source_entity_mem.memory_id): changes_made_to_any_cache = True
            if target_entity_mem:
                if _add_connection_if_new(cast(BaseMemory, target_entity_mem),
                                          rel_mem.memory_id): changes_made_to_any_cache = True
                if _add_connection_if_new(cast(BaseMemory, rel_mem),
                                          target_entity_mem.memory_id): changes_made_to_any_cache = True

            # Optionally, connect source and target entities directly if this relation implies a strong bond
            # For now, the relation memory itself acts as the bridge.

        # 4. Explicit Cross-References in ExplicitMemory
        logger.info("MBS Graph: Processing explicit cross-references...")
        # Build quick lookups for goals and procedural memories by ID
        goal_map_by_id: Dict[str, GoalRecord] = {
            g.memory_id: g for g in self._in_memory_goals_cache  # type: ignore
        }
        proc_map_by_id: Dict[str, ProceduralMemory] = {
            p.memory_id: p for p in self._in_memory_procedural_cache  # type: ignore
        }

        for explicit_mem_any in self._in_memory_explicit_cache:
            explicit_mem = cast(ExplicitMemory, explicit_mem_any)  # Assuming correct type
            if not hasattr(explicit_mem, 'memory_id'): continue

            if hasattr(explicit_mem, 'explains_procedure_step') and explicit_mem.explains_procedure_step:
                # This field should store the ProceduralMemory ID, not a step ID.
                # The actual step is within the ProceduralMemory's steps list.
                # For simplicity, we link to the whole ProceduralMemory.
                proc_mem_id = explicit_mem.explains_procedure_step
                linked_proc_mem = proc_map_by_id.get(proc_mem_id)
                if linked_proc_mem:
                    if _add_connection_if_new(cast(BaseMemory, explicit_mem),
                                              linked_proc_mem.memory_id): changes_made_to_any_cache = True
                    if _add_connection_if_new(cast(BaseMemory, linked_proc_mem),
                                              explicit_mem.memory_id): changes_made_to_any_cache = True
                    logger.debug(
                        f"MBS Graph: Linked explicit mem {explicit_mem.memory_id} to procedural mem {linked_proc_mem.memory_id} (explains_procedure).")

            if hasattr(explicit_mem, 'provides_evidence_for_goal') and explicit_mem.provides_evidence_for_goal:
                goal_mem_id = explicit_mem.provides_evidence_for_goal
                linked_goal_mem = goal_map_by_id.get(goal_mem_id)
                if linked_goal_mem:
                    if _add_connection_if_new(cast(BaseMemory, explicit_mem),
                                              linked_goal_mem.memory_id): changes_made_to_any_cache = True
                    if _add_connection_if_new(cast(BaseMemory, linked_goal_mem),
                                              explicit_mem.memory_id): changes_made_to_any_cache = True
                    logger.debug(
                        f"MBS Graph: Linked explicit mem {explicit_mem.memory_id} to goal mem {linked_goal_mem.memory_id} (provides_evidence).")

        if changes_made_to_any_cache:
            logger.info("MBS Graph: Rewriting memory stores due to new connections...")
            await self._rewrite_aggregated_store()
            await self._rewrite_goals_store()
            await self._rewrite_kg_entities_store()
            await self._rewrite_kg_relations_store()
            await self._rewrite_emotional_store()
            await self._rewrite_procedural_store()
            # Add rewrites for any other stores if they can have 'related_memory_ids'
        else:
            logger.info("MBS Graph: No new connections were made during the build process.")



    async def _get_searchable_text_and_keywords(self, memory: AnyMemoryType) -> Tuple[str, List[str]]:
        """Extracts a concatenated text string for searching and inherent keywords from a memory object."""
        texts_for_search: List[str] = []
        inherent_keywords: List[str] = list(getattr(memory, 'keywords', []))  # Start with existing keywords

        mem_type = getattr(memory, 'memory_type', 'unknown')

        if mem_type == "explicit":
            content_obj = getattr(memory, 'content', None)
            if content_obj:
                if hasattr(content_obj, 'text_content') and getattr(content_obj, 'text_content'):
                    texts_for_search.append(str(getattr(content_obj, 'text_content')))
                if hasattr(content_obj, 'structured_data') and getattr(content_obj, 'structured_data'):
                    # Simple stringify for search, could be more nuanced
                    texts_for_search.append(json.dumps(getattr(content_obj, 'structured_data')))
        elif mem_type == "goal_record":
            if hasattr(memory, 'goal_description') and getattr(memory, 'goal_description'):
                texts_for_search.append(str(getattr(memory, 'goal_description')))
        elif mem_type == "kg_entity_record":
            if hasattr(memory, 'label') and getattr(memory, 'label'):
                texts_for_search.append(str(getattr(memory, 'label')))
            if hasattr(memory, 'description') and getattr(memory, 'description'):
                texts_for_search.append(str(getattr(memory, 'description')))
            if hasattr(memory, 'aliases') and getattr(memory, 'aliases'):
                texts_for_search.extend([str(alias) for alias in getattr(memory, 'aliases')])
        elif mem_type == "kg_relation_record":
            if hasattr(memory, 'relation_label') and getattr(memory, 'relation_label'):
                texts_for_search.append(str(getattr(memory, 'relation_label')))
            if hasattr(memory, 'description') and getattr(memory, 'description'):
                texts_for_search.append(str(getattr(memory, 'description')))
        elif mem_type == "emotional":
            if hasattr(memory, 'primary_emotion') and getattr(memory, 'primary_emotion'):
                emotion_val = getattr(memory, 'primary_emotion')
                texts_for_search.append(str(emotion_val.value if hasattr(emotion_val, 'value') else emotion_val))
            emo_context = getattr(memory, 'context', None)
            if emo_context and hasattr(emo_context, 'triggering_event_summary') and getattr(emo_context,
                                                                                            'triggering_event_summary'):
                texts_for_search.append(str(getattr(emo_context, 'triggering_event_summary')))
        elif mem_type == "procedural":
            if hasattr(memory, 'procedure_name') and getattr(memory, 'procedure_name'):
                texts_for_search.append(str(getattr(memory, 'procedure_name')))
            if hasattr(memory, 'goal_description') and getattr(memory, 'goal_description'):
                texts_for_search.append(str(getattr(memory, 'goal_description')))
            if hasattr(memory, 'steps') and getattr(memory, 'steps'):
                for step in getattr(memory, 'steps'):
                    if hasattr(step, 'description') and getattr(step, 'description'):
                        texts_for_search.append(str(getattr(step, 'description')))
        # Add other memory types here...

        full_search_text = " ".join(filter(None, texts_for_search)).strip()
        return full_search_text, inherent_keywords

    async def _ensure_embedding_for_memory(self, memory: AnyMemoryType):
        if not hasattr(memory, 'memory_id') or not hasattr(memory, 'memory_type'):
            logger.warning(
                f"MBS: Memory object missing memory_id or memory_type, cannot ensure embedding. Obj: {memory}")
            return

        memory_id = memory.memory_id
        memory_type_str = str(getattr(memory, 'memory_type', 'unknown'))  # Use string memory_type for context

        if memory_id in self._embedding_cache:
            setattr(memory, 'embedding_reference', memory_id)
            return

        text_to_embed, _ = await self._get_searchable_text_and_keywords(memory)

        if text_to_embed:
            try:
                embedding = await self._embedding_client.get_embedding(text_to_embed, context_type=memory_type_str)
                if embedding:
                    self._embedding_cache[memory_id] = embedding
                    setattr(memory, 'embedding_reference', memory_id)
                    logger.debug(f"MBS: Generated and cached embedding for {memory_type_str} memory {memory_id}")
            except Exception as e:
                logger.error(f"MBS: Failed to generate embedding for {memory_type_str} {memory_id}: {e}")
        else:
            logger.debug(f"MBS: No text content to embed for {memory_type_str} memory {memory_id}.")

    # --- File Path Getters (add new ones) ---
    def _get_aggregated_memories_filepath(self) -> Path:
        return self.store_path / AGGREGATED_EXPLICIT_MEMORIES_FILENAME

    def _get_goals_store_filepath(self) -> Path:
        return self.store_path / GOALS_STORE_FILENAME

    def _get_kg_entities_filepath(self) -> Path:
        return self.store_path / KG_ENTITIES_FILENAME

    def _get_kg_relations_filepath(self) -> Path:
        return self.store_path / KG_RELATIONS_FILENAME

    def _get_emotional_memories_filepath(self) -> Path:
        return self.store_path / EMOTIONAL_MEMORIES_FILENAME

    def _get_procedural_memories_filepath(self) -> Path:
        return self.store_path / PROCEDURAL_MEMORIES_FILENAME

    # --- Generic Loader (no change needed if Pydantic models handle their types) ---
    def _load_from_jsonl_file(self, filepath: Path, pydantic_model_class: type, expected_memory_type_attr: str) -> List[
        Any]:
        records: List[Any] = []
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        try:
                            data = json.loads(line)
                            # Ensure 'memory_type' exists and matches before Pydantic parsing
                            if data.get("memory_type") == expected_memory_type_attr:
                                record_instance = pydantic_model_class(**data)
                                records.append(record_instance)
                                if data.get("embedding_reference") and data.get("_embedding_vector_for_jsonl_"):
                                    self._embedding_cache[data["embedding_reference"]] = data[
                                        "_embedding_vector_for_jsonl_"]
                            # No warning for mismatched types if we iterate many files; let specific loaders handle specific files.
                        except (json.JSONDecodeError, TypeError,
                                ValueError) as e_parse:  # PydanticError can inherit from ValueError
                            logger.warning(
                                f"MBS Store: Skipping malformed/invalid line {line_num} in {filepath} for model {pydantic_model_class.__name__}: {line.strip()}. Error: {e_parse}")
            except Exception as e:
                logger.error(f"MBS Store: Failed to load records from {filepath}: {e}", exc_info=True)
        return records

    # --- Specific Loaders (add new ones) ---
    def _load_all_explicit_memories(self) -> List[ExplicitMemory]:
        return self._load_from_jsonl_file(self._get_aggregated_memories_filepath(), ExplicitMemory,
                                          "explicit")  # type: ignore

    def _load_all_goal_records(self) -> List[GoalRecord]:
        return self._load_from_jsonl_file(self._get_goals_store_filepath(), GoalRecord, "goal_record")  # type: ignore

    def _load_all_kg_entities(self) -> List[KGEntityRecord]:
        return self._load_from_jsonl_file(self._get_kg_entities_filepath(), KGEntityRecord,
                                          "kg_entity_record")  # type: ignore

    def _load_all_kg_relations(self) -> List[KGRelationRecord]:
        return self._load_from_jsonl_file(self._get_kg_relations_filepath(), KGRelationRecord,
                                          "kg_relation_record")  # type: ignore

    def _load_all_emotional_memories(self) -> List[EmotionalMemory]:
        return self._load_from_jsonl_file(self._get_emotional_memories_filepath(), EmotionalMemory,
                                          "emotional")  # type: ignore

    def _load_all_procedural_memories(self) -> List[ProceduralMemory]:
        return self._load_from_jsonl_file(self._get_procedural_memories_filepath(), ProceduralMemory,
                                          "procedural")  # type: ignore

    # --- Generic Saver (no change needed) ---
    async def _save_record_to_jsonl_file(self, filepath: Path, record_object: Any, record_id_attr: str = 'memory_id'):
        try:
            await self._ensure_embedding_for_memory(record_object)
            record_data_dict = {}
            if hasattr(record_object, "model_dump"):
                record_data_dict = record_object.model_dump(exclude_none=True)
            else:  # Fallback for placeholders
                record_data_dict = record_object.__dict__

            mem_id_for_embedding = record_data_dict.get(record_id_attr) or record_data_dict.get('memory_id')
            if mem_id_for_embedding and mem_id_for_embedding in self._embedding_cache:
                record_data_dict["_embedding_vector_for_jsonl_"] = self._embedding_cache[mem_id_for_embedding]

            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(record_data_dict) + "\n")
        except Exception as e:
            record_id_val = getattr(record_object, record_id_attr, 'N/A_ID')
            logger.error(f"MBS Store: Failed to save record '{record_id_val}' to {filepath}: {e}", exc_info=True)

    # --- Specific Savers (add new ones) ---
    async def _save_explicit_memory_to_aggregated_store(self, memory: ExplicitMemory):
        await self._save_record_to_jsonl_file(self._get_aggregated_memories_filepath(), memory)

    async def _save_goal_record_to_store(self, goal_memory: GoalRecord):
        await self._save_record_to_jsonl_file(self._get_goals_store_filepath(), goal_memory)

    async def _save_kg_entity_to_store(self, entity_record: KGEntityRecord):
        await self._save_record_to_jsonl_file(self._get_kg_entities_filepath(), entity_record, 'entity_id_str')

    async def _save_kg_relation_to_store(self, relation_record: KGRelationRecord):
        await self._save_record_to_jsonl_file(self._get_kg_relations_filepath(), relation_record, 'relation_id_str')

    async def _save_emotional_memory_to_store(self, emotional_memory: EmotionalMemory):
        await self._save_record_to_jsonl_file(self._get_emotional_memories_filepath(), emotional_memory)

    async def _save_procedural_memory_to_store(self, procedural_memory: ProceduralMemory):
        await self._save_record_to_jsonl_file(self._get_procedural_memories_filepath(), procedural_memory)

    # --- Generic Rewriter (no change needed) ---
    async def _rewrite_jsonl_file(self, filepath: Path, records_cache: List[Any], temp_suffix: str):
        temp_filepath = filepath.with_suffix(f".{time.time_ns()}.{temp_suffix}.tmp")
        try:
            for record_obj_for_rewrite in records_cache:
                await self._ensure_embedding_for_memory(record_obj_for_rewrite)

            with open(temp_filepath, 'w', encoding='utf-8') as f:
                for record_obj in records_cache:
                    record_data_dict = {}
                    if hasattr(record_obj, "model_dump"):
                        record_data_dict = record_obj.model_dump(exclude_none=True)
                    else:  # Fallback for placeholders
                        record_data_dict = record_obj.__dict__

                    mem_id_for_embedding = record_data_dict.get(
                        getattr(record_obj, '_id_attr_for_embedding_lookup', 'memory_id')) \
                                           or record_data_dict.get('memory_id')
                    if mem_id_for_embedding and mem_id_for_embedding in self._embedding_cache:
                        record_data_dict["_embedding_vector_for_jsonl_"] = self._embedding_cache[mem_id_for_embedding]

                    f.write(json.dumps(record_data_dict) + "\n")
            os.replace(temp_filepath, filepath)
            logger.info(f"MBS Store: Successfully rewrote store at {filepath}")
        except Exception as e:
            logger.error(f"MBS Store: Failed to rewrite store {filepath}: {e}", exc_info=True)
            if temp_filepath.exists():
                try:
                    os.remove(temp_filepath)
                except OSError:
                    logger.error(f"MBS Store: Failed to remove temp file {temp_filepath}")

    # --- Specific Rewriters (add new ones) ---
    async def _rewrite_aggregated_store(self):
        await self._rewrite_jsonl_file(self._get_aggregated_memories_filepath(), self._in_memory_explicit_cache,
                                       "explicit")

    async def _rewrite_goals_store(self):
        await self._rewrite_jsonl_file(self._get_goals_store_filepath(), self._in_memory_goals_cache, "goals")

    async def _rewrite_kg_entities_store(self):
        await self._rewrite_jsonl_file(self._get_kg_entities_filepath(), self._in_memory_kg_entities_cache,
                                       "kg_entities")

    async def _rewrite_kg_relations_store(self):
        await self._rewrite_jsonl_file(self._get_kg_relations_filepath(), self._in_memory_kg_relations_cache,
                                       "kg_relations")

    async def _rewrite_emotional_store(self):
        await self._rewrite_jsonl_file(self._get_emotional_memories_filepath(), self._in_memory_emotional_cache,
                                       "emotional")

    async def _rewrite_procedural_store(self):
        await self._rewrite_jsonl_file(self._get_procedural_memories_filepath(), self._in_memory_procedural_cache,
                                       "procedural")

    # --- Core Public Methods (add_session_to_memory: no major changes needed initially) ---
    async def _extract_and_embed_explicit_memories_from_adk_session(self, session: AdkSession) -> List[ExplicitMemory]:

        extracted_memories: List[ExplicitMemory] = []
        events_to_process = session.events[-SESSION_HISTORY_FOR_MEMORY_TURNS:] if session.events else []

        for event in events_to_process:
            if not event.content or not event.content.parts:
                continue

            event_texts = [part.text for part in event.content.parts if
                           isinstance(part, AdkPart) and part.text and part.text.strip()]
            full_event_text = " ".join(event_texts).strip()

            if full_event_text:
                source_type_val_str = MemorySourceType.USER_INTERACTION.value if event.author == "user" else MemorySourceType.ORA_RESPONSE.value  # type: ignore
                if event.author not in ["user", "ORA", "SYSTEM"]:  # type: ignore
                    source_type_val_str = MemorySourceType.TOOL_OUTPUT.value  # type: ignore

                source_type_val = source_type_val_str
                if MEMORY_TYPES_LOADED_SUCCESSFULLY:
                    try:
                        source_type_val = MemorySourceType(source_type_val_str)
                    except ValueError:
                        logger.warning(
                            f"MBS Store: Unknown MemorySourceType str '{source_type_val_str}' for event by '{event.author}'")
                        source_type_val = MemorySourceType.INTERNAL_REFLECTION  # type: ignore

                keywords = list(set(re.findall(r'\b\w{4,15}\b', full_event_text.lower())))[:10]
                mem_data = {
                    "timestamp": event.timestamp or time.time(),
                    "source_turn_id": event.invocation_id or session.id,
                    "source_interaction_id": session.id,
                    "source_type": source_type_val,
                    "source_agent_name": event.author,
                    "salience": MemorySalience.MEDIUM,  # type: ignore
                    "keywords": keywords,
                    "content": ExplicitMemoryContent(
                        text_content=full_event_text) if MEMORY_TYPES_LOADED_SUCCESSFULLY else {
                        "text_content": full_event_text},  # type: ignore
                    "confidence_score": 0.85,  # Default confidence
                    "memory_type": "explicit"
                }
                try:
                    new_explicit_mem = ExplicitMemory(**mem_data)  # type: ignore
                    await self._ensure_embedding_for_memory(new_explicit_mem)

                    if MEMORY_LIFECYCLE_MANAGER_LOADED:
                        # Initialize lifecycle attributes if not present
                        if not hasattr(new_explicit_mem, 'dynamic_salience_score'): setattr(new_explicit_mem,
                                                                                            'dynamic_salience_score',
                                                                                            0.5)
                        if not hasattr(new_explicit_mem, 'decay_rate'): setattr(new_explicit_mem, 'decay_rate', 0.01)
                        if not hasattr(new_explicit_mem, 'access_count'): setattr(new_explicit_mem, 'access_count', 0)
                        if not hasattr(new_explicit_mem, 'last_accessed_ts') or getattr(new_explicit_mem,
                                                                                        'last_accessed_ts') is None: setattr(
                            new_explicit_mem, 'last_accessed_ts', getattr(new_explicit_mem, 'timestamp', time.time()))
                        initialize_dynamic_salience(new_explicit_mem)
                    extracted_memories.append(new_explicit_mem)
                except Exception as e_create:
                    logger.error(
                        f"MBS Store: Failed to create ExplicitMemory from event data: {e_create}. Data: {mem_data}",
                        exc_info=True)
        logger.debug(f"MBS Store: Extracted {len(extracted_memories)} explicit memories from ADK session {session.id}")
        return extracted_memories

    async def add_session_to_memory(self, session: AdkSession):

        if not isinstance(session, AdkSession):
            logger.error("MBS add_session_to_memory: Expected AdkSession object.")
            return
        logger.info(f"MBS: Processing ADK session '{session.id}' for memory ingestion.")
        new_memories = await self._extract_and_embed_explicit_memories_from_adk_session(session)
        if not new_memories:
            logger.info(f"MBS: No new explicit memories extracted from session '{session.id}'.")
            return

        added_count = 0
        for mem_obj in new_memories:
            self._in_memory_explicit_cache.append(mem_obj)
            await self._save_explicit_memory_to_aggregated_store(mem_obj)
            added_count += 1

        if added_count > 0:
            logger.info(
                f"MBS: Added {added_count} new memories from session '{session.id}' to store. Explicit Cache: {len(self._in_memory_explicit_cache)}")

    async def search_memory(self, *, app_name: str, user_id: str, query: str) -> 'SearchMemoryResponse':  # type: ignore
        top_k = 5  # Default, can be made configurable or part of augmented_query_context
        logger.info(
            f"MBS search_memory (Advanced CARS): Full Query='{query[:200]}...', App='{app_name}', User='{user_id}'")

        # 1. Parse query and augmented context
        main_query_text = query
        parsed_augmented_context: Dict[str, Any] = {}
        if CONTEXT_QUERY_DELIMITER in query:
            parts = query.split(CONTEXT_QUERY_DELIMITER, 1)
            main_query_text = parts[0]
            if len(parts) > 1 and parts[1]:
                try:
                    parsed_augmented_context = json.loads(parts[1])
                    logger.info(
                        f"MBS search_memory: Parsed augmented_query_context: {list(parsed_augmented_context.keys())}")
                except json.JSONDecodeError as e:
                    logger.warning(
                        f"MBS search_memory: Failed to parse augmented_query_context JSON: {e}. Context part: {parts[1][:100]}")

        if not main_query_text or not main_query_text.strip():
            return {"memories": []}

        # 2. Dynamic Weighting based on context
        current_semantic_weight = parsed_augmented_context.get("semantic_score_weight_override",
                                                               DEFAULT_SEMANTIC_SCORE_WEIGHT)
        current_keyword_weight = parsed_augmented_context.get("keyword_score_weight_override",
                                                              DEFAULT_KEYWORD_SCORE_WEIGHT)
        if "current_interaction_goal" in parsed_augmented_context:
            goal = parsed_augmented_context["current_interaction_goal"]
            if goal == "creative_ideation":
                current_semantic_weight = max(current_semantic_weight, 0.75)  # Favor broader semantic connections
                current_keyword_weight = min(current_keyword_weight, 0.25)
            elif goal == "problem_solving" or goal == "factual_retrieval":
                current_keyword_weight = max(current_keyword_weight, 0.5)  # Favor precision
                current_semantic_weight = min(current_semantic_weight, 0.5)
        logger.debug(
            f"MBS search_memory: Using SemanticWeight={current_semantic_weight}, KeywordWeight={current_keyword_weight}")

        query_embedding: Optional[List[float]] = None
        try:
            query_embedding = await self._embedding_client.get_embedding(main_query_text, context_type="default_query")
        except Exception as e_q_embed:
            logger.error(
                f"MBS search_memory: Failed to get embedding for query '{main_query_text}': {e_q_embed}. Proceeding with keyword search mainly.")

        scored_candidates: List[Tuple[AnyMemoryType, float]] = []
        query_keywords_set = set(re.findall(r'\b\w{3,15}\b', main_query_text.lower()))

        # Define all memory caches to iterate over with their type string
        all_memory_sources: List[Tuple[List[AnyMemoryType], str]] = [
            (self._in_memory_explicit_cache, "explicit"),
            (self._in_memory_goals_cache, "goal_record"),
            (self._in_memory_kg_entities_cache, "kg_entity_record"),
            (self._in_memory_kg_relations_cache, "kg_relation_record"),
            (self._in_memory_emotional_cache, "emotional"),
            (self._in_memory_procedural_cache, "procedural"),
            # Add other caches here as they are implemented
        ]

        # 3. Iterate All Relevant Caches & 5. Unified Scoring
        for memory_cache, memory_type_str_for_loop in all_memory_sources:
            # 3.1 Memory Type Prioritization (Example)
            type_priority_multiplier = 1.0
            if "current_interaction_goal" in parsed_augmented_context:
                goal = parsed_augmented_context["current_interaction_goal"]
                if goal == "recall_procedure" and memory_type_str_for_loop in ["procedural", "goal_record"]:
                    type_priority_multiplier = 1.5
                elif goal == "understand_user_emotion" and memory_type_str_for_loop == "emotional":
                    type_priority_multiplier = 1.3
                elif goal == "knowledge_graph_exploration" and memory_type_str_for_loop in ["kg_entity_record",
                                                                                            "kg_relation_record"]:
                    type_priority_multiplier = 1.4

            for mem in memory_cache:
                keyword_score = 0.0
                semantic_score = 0.0

                searchable_text, inherent_keywords = await self._get_searchable_text_and_keywords(mem)
                all_mem_keywords = set(str(k).lower() for k in inherent_keywords)

                # Keyword scoring
                if searchable_text:  # Check against extracted full text
                    for qk in query_keywords_set:
                        if qk in searchable_text.lower(): keyword_score += 1.0
                for mk_keyword_lower in all_mem_keywords:  # Check against inherent keywords
                    if mk_keyword_lower in query_keywords_set: keyword_score += 2.0  # Boost for explicit keywords

                # Semantic scoring
                mem_id_for_embedding = getattr(mem, 'memory_id', None)
                if query_embedding and mem_id_for_embedding and mem_id_for_embedding in self._embedding_cache:
                    memory_embedding = self._embedding_cache[mem_id_for_embedding]
                    semantic_score = compute_adaptive_similarity(query_embedding, memory_embedding)
                    semantic_score = max(0, semantic_score)  # Ensure non-negative
                elif query_embedding and searchable_text:  # Try to embed on-the-fly if not cached
                    try:
                        # Be cautious with on-the-fly embedding in search due to performance
                        await self._ensure_embedding_for_memory(mem)  # This will cache it if successful
                        if mem_id_for_embedding and mem_id_for_embedding in self._embedding_cache:
                            memory_embedding = self._embedding_cache[mem_id_for_embedding]
                            semantic_score = compute_adaptive_similarity(query_embedding, memory_embedding)
                            semantic_score = max(0, semantic_score)
                    except Exception as e_search_emb:
                        logger.warning(
                            f"MBS Search: On-the-fly embedding failed for {mem_id_for_embedding}: {e_search_emb}")

                if keyword_score > 0 or semantic_score > 0:
                    dynamic_salience = getattr(mem, 'dynamic_salience_score', 0.5)
                    current_timestamp_val = getattr(mem, 'timestamp', 0)
                    recency_factor = 1.0 / (1 + (time.time() - current_timestamp_val) / (
                                86400 * 30))  # Penalize older, up to ~30 days

                    final_score = (current_keyword_weight * keyword_score + current_semantic_weight * semantic_score)
                    final_score *= (1 + dynamic_salience)  # Boost by dynamic salience
                    final_score *= recency_factor  # Modulate by recency
                    final_score *= type_priority_multiplier  # Modulate by type priority

                    # 4. Contextual Filtering (Example: narrative_thread_id)
                    active_narrative_thread = parsed_augmented_context.get("active_narrative_thread_id")
                    mem_narrative_thread = getattr(mem, 'narrative_thread_id', None)
                    if active_narrative_thread and mem_narrative_thread == active_narrative_thread:
                        final_score *= 1.2  # Boost if matches active narrative thread
                    elif active_narrative_thread and mem_narrative_thread:  # Different thread
                        final_score *= 0.8  # Slightly penalize if different active thread

                    if final_score > 0.01:  # Some minimal threshold
                        scored_candidates.append((mem, final_score))

        scored_candidates.sort(key=lambda item: item[1], reverse=True)

        # 6. Result Formatting
        results: List[Dict[str, Any]] = []
        retrieved_for_salience_update: List[BaseMemory] = []

        final_top_k = parsed_augmented_context.get("top_k_override", top_k)

        for mem_obj, score_val in scored_candidates[:final_top_k]:
            source_agent_name = getattr(mem_obj, 'source_agent_name', "retrieved_memory")
            mem_type_attr = str(getattr(mem_obj, 'memory_type', 'unknown'))

            # Improved display text generation
            display_text_parts = [f"Retrieved {mem_type_attr} (Score: {score_val:.2f})"]
            searchable_content_for_display, _ = await self._get_searchable_text_and_keywords(mem_obj)
            if searchable_content_for_display:
                display_text_parts.append(searchable_content_for_display[:150] + "...")  # Snippet
            else:  # Fallback display
                if mem_type_attr == 'explicit':
                    explicit_content = getattr(mem_obj, 'content', None)
                    if explicit_content and hasattr(explicit_content, 'text_content'):
                        display_text_parts.append(str(getattr(explicit_content, 'text_content', ""))[:150] + "...")
                elif mem_type_attr == 'kg_entity_record':
                    display_text_parts.append(
                        f"Entity: {getattr(mem_obj, 'label', 'N/A')}, Desc: {getattr(mem_obj, 'description', '')[:100]}...")
                elif mem_type_attr == 'goal_record':
                    display_text_parts.append(f"Goal: {getattr(mem_obj, 'goal_description', '')[:150]}...")

            display_text_final = " ".join(filter(None, display_text_parts))

            if display_text_final:
                pseudo_event_part_dict = AdkPart(text=display_text_final).to_dict()
                pseudo_event_content_dict = {"parts": [pseudo_event_part_dict], "role": source_agent_name}
                pseudo_event_dict = {
                    "id": f"mem-event-{getattr(mem_obj, 'memory_id', 'dummy_id')}",
                    "author": source_agent_name, "timestamp": getattr(mem_obj, 'timestamp', time.time()),
                    "content": pseudo_event_content_dict,
                    "invocation_id": getattr(mem_obj, 'source_turn_id',
                                             getattr(mem_obj, 'source_interaction_id', 'unknown_session'))
                }
                results.append({
                    "session_id": getattr(mem_obj, 'source_interaction_id', 'unknown_session'),
                    "events": [pseudo_event_dict], "score": score_val,
                    "retrieved_memory_type": mem_type_attr,
                    "retrieved_memory_id": getattr(mem_obj, 'memory_id', 'unknown_id')  # Add memory ID
                })
                if MEMORY_LIFECYCLE_MANAGER_LOADED and isinstance(mem_obj,
                                                                  BaseMemoryPydanticPlaceholder if not MEMORY_TYPES_LOADED_SUCCESSFULLY else BaseMemory):
                    retrieved_for_salience_update.append(
                        cast(BaseMemory if MEMORY_TYPES_LOADED_SUCCESSFULLY else BaseMemoryPydanticPlaceholder,
                             mem_obj))

        if MEMORY_LIFECYCLE_MANAGER_LOADED:
            for mem_to_update in retrieved_for_salience_update:
                update_dynamic_salience(mem_to_update, access_type="retrieval")

        logger.info(
            f"MBS search_memory (Advanced CARS): Found {len(results)} relevant memories for query '{main_query_text}'.")
        return {"memories": results}

    async def search_raw_memories(self, query: str, top_k: int = 5) -> List[Tuple[AnyMemoryType, float]]:
        """
        Search for raw memory objects (not wrapped in ADK format).

        Args:
            query: Search query string
            top_k: Maximum number of results to return

        Returns:
            List of tuples containing (memory_object, score)
        """
        # Parse the query similar to search_memory
        main_query_text = query
        parsed_augmented_context: Dict[str, Any] = {}

        if CONTEXT_QUERY_DELIMITER in query:
            parts = query.split(CONTEXT_QUERY_DELIMITER, 1)
            main_query_text = parts[0]
            if len(parts) > 1 and parts[1]:
                try:
                    parsed_augmented_context = json.loads(parts[1])
                except json.JSONDecodeError:
                    pass

        # Get query embedding
        query_embedding: Optional[List[float]] = None
        try:
            query_embedding = await self._embedding_client.get_embedding(main_query_text, context_type="default_query")
        except Exception as e:
            logger.error(f"Failed to get embedding for query: {e}")

        # Search across all memory types
        scored_candidates: List[Tuple[AnyMemoryType, float]] = []
        query_keywords_set = set(re.findall(r'\b\w{3,15}\b', main_query_text.lower()))

        all_memory_sources: List[Tuple[List[AnyMemoryType], str]] = [
            (self._in_memory_explicit_cache, "explicit"),
            (self._in_memory_goals_cache, "goal_record"),
            (self._in_memory_kg_entities_cache, "kg_entity_record"),
            (self._in_memory_kg_relations_cache, "kg_relation_record"),
            (self._in_memory_emotional_cache, "emotional"),
            (self._in_memory_procedural_cache, "procedural"),
        ]

        for memory_cache, memory_type_str in all_memory_sources:
            for mem in memory_cache:
                keyword_score = 0.0
                semantic_score = 0.0

                searchable_text, inherent_keywords = await self._get_searchable_text_and_keywords(mem)
                all_mem_keywords = set(str(k).lower() for k in inherent_keywords)

                # Calculate keyword score
                if searchable_text:
                    for qk in query_keywords_set:
                        if qk in searchable_text.lower():
                            keyword_score += 1.0

                for mk in all_mem_keywords:
                    if mk in query_keywords_set:
                        keyword_score += 2.0

                # Calculate semantic score
                mem_id = getattr(mem, 'memory_id', None)
                if query_embedding and mem_id and mem_id in self._embedding_cache:
                    memory_embedding = self._embedding_cache[mem_id]
                    semantic_score = compute_adaptive_similarity(query_embedding, memory_embedding)
                    semantic_score = max(0, semantic_score)

                # Calculate final score
                if keyword_score > 0 or semantic_score > 0:
                    dynamic_salience = getattr(mem, 'dynamic_salience_score', 0.5)
                    timestamp_val = getattr(mem, 'timestamp', 0)
                    recency_factor = 1.0 / (1 + (time.time() - timestamp_val) / (86400 * 30))

                    final_score = (DEFAULT_KEYWORD_SCORE_WEIGHT * keyword_score +
                                   DEFAULT_SEMANTIC_SCORE_WEIGHT * semantic_score)
                    final_score *= (1 + dynamic_salience)
                    final_score *= recency_factor

                    if final_score > 0.01:
                        scored_candidates.append((mem, final_score))

        # Sort by score and return top k
        scored_candidates.sort(key=lambda x: x[1], reverse=True)

        # Update salience for retrieved memories
        if MEMORY_LIFECYCLE_MANAGER_LOADED:
            for mem, _ in scored_candidates[:top_k]:
                update_dynamic_salience(mem, access_type="retrieval")

        return scored_candidates[:top_k]


    async def add_specific_memory(self, memory_object: AnyMemoryType):
        # Ensure memory_type is a string for consistent handling
        memory_type_attr_val = getattr(memory_object, 'memory_type', type(memory_object).__name__)
        memory_type_attr = str(
            memory_type_attr_val.value if hasattr(memory_type_attr_val, 'value') else memory_type_attr_val)
        memory_id = getattr(memory_object, 'memory_id', f"unknown_id_{time.time_ns()}")

        logger.info(f"MBS: add_specific_memory called for ID '{memory_id}', type_attr '{memory_type_attr}'")

        await self._ensure_embedding_for_memory(memory_object)

        # Lifecycle initialization
        if MEMORY_LIFECYCLE_MANAGER_LOADED:
            # Default salience/decay can be based on memory_type_attr
            default_salience_map = {
                "goal_record": 0.65, "kg_entity_record": 0.7, "kg_relation_record": 0.65,
                "emotional": 0.6, "procedural": 0.65, "explicit": 0.55
            }
            default_decay_map = {
                "goal_record": 0.015, "kg_entity_record": 0.005, "kg_relation_record": 0.005,
                "emotional": 0.01, "procedural": 0.008, "explicit": 0.007
            }
            default_s = default_salience_map.get(memory_type_attr, 0.5)
            default_d = default_decay_map.get(memory_type_attr, 0.01)

            if not hasattr(memory_object, 'dynamic_salience_score'): setattr(memory_object, 'dynamic_salience_score',
                                                                             default_s)
            if not hasattr(memory_object, 'decay_rate'): setattr(memory_object, 'decay_rate', default_d)
            if not hasattr(memory_object, 'access_count'): setattr(memory_object, 'access_count', 0)
            if not hasattr(memory_object, 'last_accessed_ts') or getattr(memory_object,
                                                                         'last_accessed_ts') is None: setattr(
                memory_object, 'last_accessed_ts', getattr(memory_object, 'timestamp', time.time()))

            initialize_dynamic_salience(memory_object)
            update_dynamic_salience(memory_object, access_type="commit")

        # Determine target cache and save/rewrite methods based on memory_type_attr
        target_cache: Optional[List[AnyMemoryType]] = None
        save_method = None
        rewrite_method = None
        id_attr_for_cache_update = 'memory_id'

        if memory_type_attr == 'explicit':
            target_cache, save_method, rewrite_method = self._in_memory_explicit_cache, self._save_explicit_memory_to_aggregated_store, self._rewrite_aggregated_store
        elif memory_type_attr == 'goal_record':
            target_cache, save_method, rewrite_method = self._in_memory_goals_cache, self._save_goal_record_to_store, self._rewrite_goals_store
        elif memory_type_attr == 'kg_entity_record':
            target_cache, save_method, rewrite_method = self._in_memory_kg_entities_cache, self._save_kg_entity_to_store, self._rewrite_kg_entities_store
            id_attr_for_cache_update = 'entity_id_str'
        elif memory_type_attr == 'kg_relation_record':
            target_cache, save_method, rewrite_method = self._in_memory_kg_relations_cache, self._save_kg_relation_to_store, self._rewrite_kg_relations_store
            id_attr_for_cache_update = 'relation_id_str'
        elif memory_type_attr == 'emotional':
            target_cache, save_method, rewrite_method = self._in_memory_emotional_cache, self._save_emotional_memory_to_store, self._rewrite_emotional_store
        elif memory_type_attr == 'procedural':
            target_cache, save_method, rewrite_method = self._in_memory_procedural_cache, self._save_procedural_memory_to_store, self._rewrite_procedural_store
        # Add other types here...

        if target_cache is not None and save_method and rewrite_method:
            unique_id_value = getattr(memory_object, id_attr_for_cache_update, memory_id)
            idx = next((i for i, item in enumerate(target_cache) if
                        getattr(item, id_attr_for_cache_update, None) == unique_id_value), -1)

            if idx != -1:  # Update existing
                # Preserve some attributes from existing if new object doesn't have them (e.g. placeholder updates)
                existing_item = target_cache[idx]
                for attr_name in ['memory_id', 'access_count', 'timestamp', 'dynamic_salience_score', 'decay_rate',
                                  'last_accessed_ts']:
                    if not hasattr(memory_object, attr_name) or getattr(memory_object, attr_name) is None:
                        if hasattr(existing_item, attr_name):
                            setattr(memory_object, attr_name, getattr(existing_item, attr_name))

                target_cache[idx] = memory_object
                logger.info(f"MBS: Updated {memory_type_attr} '{unique_id_value}' (memory_id: {memory_id}) in cache.")
                await rewrite_method()
            else:  # Add new
                target_cache.append(memory_object)
                logger.info(f"MBS: Added new {memory_type_attr} '{unique_id_value}' (memory_id: {memory_id}).")
                await save_method(memory_object)
        else:
            logger.warning(
                f"MBS: add_specific_memory for unsupported/unmapped type '{memory_type_attr}', ID '{memory_id}'. Not stored.")

    async def get_memory_by_id(self, memory_id: str) -> Optional[AnyMemoryType]:
        # Simplified: iterate all known caches
        all_caches = [
            self._in_memory_explicit_cache, self._in_memory_goals_cache,
            self._in_memory_kg_entities_cache, self._in_memory_kg_relations_cache,
            self._in_memory_emotional_cache, self._in_memory_procedural_cache
        ]
        for cache in all_caches:
            for mem in cache:
                # Primary check by memory_id
                if getattr(mem, 'memory_id', None) == memory_id:
                    if MEMORY_LIFECYCLE_MANAGER_LOADED: update_dynamic_salience(mem, access_type="retrieval")
                    return mem
                # Fallback checks for KG types by their specific ID fields if memory_id doesn't match
                if hasattr(mem, 'entity_id_str') and getattr(mem, 'entity_id_str') == memory_id:
                    if MEMORY_LIFECYCLE_MANAGER_LOADED: update_dynamic_salience(mem, access_type="retrieval")
                    return mem
                if hasattr(mem, 'relation_id_str') and getattr(mem, 'relation_id_str') == memory_id:
                    if MEMORY_LIFECYCLE_MANAGER_LOADED: update_dynamic_salience(mem, access_type="retrieval")
                    return mem
        return None

    # --- KG Traversal Methods (existing are fine, ensure salience updates use the manager) ---
    async def get_entity_by_id_str(self, entity_id_str: str, update_salience: bool = True) -> Optional[KGEntityRecord]:
        for entity in self._in_memory_kg_entities_cache:
            if getattr(entity, 'entity_id_str', None) == entity_id_str:
                if update_salience and MEMORY_LIFECYCLE_MANAGER_LOADED:
                    update_dynamic_salience(entity, access_type="retrieval")  # type: ignore
                logger.debug(f"MBS: Retrieved KG entity by entity_id_str: {entity_id_str}")
                return entity  # type: ignore
        logger.debug(f"MBS: KG entity with entity_id_str '{entity_id_str}' not found.")
        return None

    async def get_direct_relations(self, entity_id_str: str,
                                   relation_label: Optional[str] = None,
                                   direction: Literal["outgoing", "incoming", "both"] = "both",
                                   update_salience: bool = True) -> List[KGRelationRecord]:
        found_relations: List[KGRelationRecord] = []
        if not entity_id_str: return found_relations

        for relation in self._in_memory_kg_relations_cache:
            match = False
            is_source = getattr(relation, 'source_entity_id_str', None) == entity_id_str
            is_target = getattr(relation, 'target_entity_id_str', None) == entity_id_str

            if direction == "outgoing" and is_source:
                match = True
            elif direction == "incoming" and is_target:
                match = True
            elif direction == "both" and (is_source or is_target):
                match = True

            if match and relation_label:
                if getattr(relation, 'relation_label', "").lower() != relation_label.lower():
                    match = False
            if match:
                found_relations.append(relation)  # type: ignore

        if update_salience and MEMORY_LIFECYCLE_MANAGER_LOADED:
            for rel in found_relations:
                update_dynamic_salience(rel, access_type="retrieval")  # type: ignore

        logger.debug(
            f"MBS: Found {len(found_relations)} relations for entity '{entity_id_str}' (label: {relation_label}, dir: {direction}).")
        return found_relations

    async def get_neighboring_entities(self, entity_id_str: str,
                                       relation_label: Optional[str] = None,
                                       direction: Literal["outgoing", "incoming", "both"] = "both",
                                       update_salience: bool = True) -> List[KGEntityRecord]:
        neighboring_entities_dict: Dict[str, KGEntityRecord] = {}
        if not entity_id_str: return []

        relations = await self.get_direct_relations(entity_id_str, relation_label, direction, update_salience=False)

        for relation in relations:
            neighbor_id_to_fetch = None
            is_source = getattr(relation, 'source_entity_id_str', None) == entity_id_str
            is_target = getattr(relation, 'target_entity_id_str', None) == entity_id_str

            if is_source and (direction == "outgoing" or direction == "both"):
                neighbor_id_to_fetch = getattr(relation, 'target_entity_id_str', None)
            elif is_target and (
                    direction == "incoming" or direction == "both"):  # Use elif to avoid double add for self-loops in "both"
                neighbor_id_to_fetch = getattr(relation, 'source_entity_id_str', None)

            if neighbor_id_to_fetch and neighbor_id_to_fetch not in neighboring_entities_dict:
                neighbor_entity = await self.get_entity_by_id_str(neighbor_id_to_fetch, update_salience=update_salience)
                if neighbor_entity:
                    neighboring_entities_dict[neighbor_id_to_fetch] = neighbor_entity

        logger.debug(
            f"MBS: Found {len(neighboring_entities_dict)} neighboring entities for '{entity_id_str}' (label: {relation_label}, dir: {direction}).")
        return list(neighboring_entities_dict.values())

    # --- Lifecycle Management Tasks (Adjusted for new caches) ---
    async def _periodic_decay_task(self, interval_seconds: int):
        while not self._stop_background_tasks.is_set():
            try:
                await asyncio.sleep(interval_seconds)
                if self._stop_background_tasks.is_set(): break
                logger.info("MBSMemoryService: Running periodic memory decay...")
                if MEMORY_LIFECYCLE_MANAGER_LOADED:
                    all_caches_for_decay = [
                        self._in_memory_explicit_cache, self._in_memory_goals_cache,
                        self._in_memory_kg_entities_cache, self._in_memory_kg_relations_cache,
                        self._in_memory_emotional_cache, self._in_memory_procedural_cache
                    ]
                    for mem_list_item in all_caches_for_decay:
                        apply_decay_to_all_memories(mem_list_item)  # type: ignore
                logger.info("MBSMemoryService: Periodic memory decay complete.")
            except asyncio.CancelledError:
                logger.info("MBS: Decay task cancelled."); break
            except Exception as e:
                logger.error(f"MBS: Error in decay task: {e}", exc_info=True)

    async def _periodic_archive_forget_task(self, interval_seconds: int):
        while not self._stop_background_tasks.is_set():
            try:
                await asyncio.sleep(interval_seconds)
                if self._stop_background_tasks.is_set(): break
                logger.info("MBSMemoryService: Running periodic archive/forget task...")
                if MEMORY_LIFECYCLE_MANAGER_LOADED:
                    ephemeral_cfg = os.getenv("MBS_EPHEMERAL_SOURCES", ",".join(
                        str(s.value if hasattr(s, 'value') else s) for s in EPHEMERAL_SOURCE_TYPES_FOR_DELETION))
                    ephemeral_types_str_list = [s.strip() for s in ephemeral_cfg.split(',') if s.strip()]
                    ephemeral_srcs = []
                    if MEMORY_TYPES_LOADED_SUCCESSFULLY:
                        for ets_str in ephemeral_types_str_list:
                            try:
                                ephemeral_srcs.append(MemorySourceType(ets_str))  # type: ignore
                            except ValueError:
                                logger.warning(
                                    f"MBS: Invalid MemorySourceType '{ets_str}' in env for ephemeral config.")
                    else:
                        ephemeral_srcs = ephemeral_types_str_list  # Use as strings if enums not loaded

                    # Process each cache type
                    cache_configs = [
                        ("explicit", self._in_memory_explicit_cache, self._rewrite_aggregated_store,
                         float(os.getenv("MBS_EXPLICIT_ARCHIVE_THRESHOLD", DEFAULT_ARCHIVE_SALIENCE_THRESHOLD))),
                        ("goals", self._in_memory_goals_cache, self._rewrite_goals_store,
                         float(os.getenv("MBS_GOAL_ARCHIVE_THRESHOLD", 0.05))),
                        ("kg_entities", self._in_memory_kg_entities_cache, self._rewrite_kg_entities_store,
                         float(os.getenv("MBS_KG_ENTITY_ARCHIVE_THRESHOLD", 0.02))),
                        ("kg_relations", self._in_memory_kg_relations_cache, self._rewrite_kg_relations_store,
                         float(os.getenv("MBS_KG_RELATION_ARCHIVE_THRESHOLD", 0.02))),
                        ("emotional", self._in_memory_emotional_cache, self._rewrite_emotional_store,
                         float(os.getenv("MBS_EMOTIONAL_ARCHIVE_THRESHOLD", 0.08))),
                        ("procedural", self._in_memory_procedural_cache, self._rewrite_procedural_store,
                         float(os.getenv("MBS_PROCEDURAL_ARCHIVE_THRESHOLD", 0.03))),
                    ]

                    any_changes = False
                    for mem_type_name, cache_ref, rewrite_func, threshold in cache_configs:
                        kept, archived, forgotten = archive_or_forget_low_salience_memories(cache_ref,
                                                                                            archive_threshold=threshold,
                                                                                            ephemeral_sources=ephemeral_srcs)  # type: ignore
                        if forgotten or archived:
                            any_changes = True
                            # Update the in-memory cache reference
                            if mem_type_name == "explicit":
                                self._in_memory_explicit_cache = kept  # type: ignore
                            elif mem_type_name == "goals":
                                self._in_memory_goals_cache = kept  # type: ignore
                            elif mem_type_name == "kg_entities":
                                self._in_memory_kg_entities_cache = kept  # type: ignore
                            elif mem_type_name == "kg_relations":
                                self._in_memory_kg_relations_cache = kept  # type: ignore
                            elif mem_type_name == "emotional":
                                self._in_memory_emotional_cache = kept  # type: ignore
                            elif mem_type_name == "procedural":
                                self._in_memory_procedural_cache = kept  # type: ignore

                            if forgotten: logger.info(f"MBS: Forgetting {len(forgotten)} {mem_type_name} memories.")
                            if archived: logger.info(
                                f"MBS: Archiving {len(archived)} {mem_type_name} memories."); await self._archive_memories(
                                archived, archive_type_name=mem_type_name)
                            await rewrite_func()

                    if not any_changes:
                        logger.info(
                            "MBSMemoryService: No memories met criteria for archiving or forgetting this cycle.")
                logger.info("MBSMemoryService: Periodic archive/forget task complete.")
            except asyncio.CancelledError:
                logger.info("MBS: Archive/forget task cancelled."); break
            except Exception as e:
                logger.error(f"MBS: Error in archive/forget task: {e}", exc_info=True)

    async def _archive_memories(self, memories_to_archive: List[AnyMemoryType], archive_type_name: str):
        # Simplified archive filename based on type_name
        archive_filename = f"archived_{archive_type_name}_memories.jsonl"
        archive_filepath = self.store_path / archive_filename
        try:
            for memory_for_archive in memories_to_archive:
                await self._ensure_embedding_for_memory(memory_for_archive)

            with open(archive_filepath, 'a', encoding='utf-8') as f:
                for memory in memories_to_archive:
                    record_data_dict = {}
                    if hasattr(memory, "model_dump"):
                        record_data_dict = memory.model_dump(exclude_none=True)
                    else:  # Fallback for placeholders
                        record_data_dict = memory.__dict__

                    mem_id_for_embedding = record_data_dict.get(
                        getattr(memory, '_id_attr_for_embedding_lookup', 'memory_id')) \
                                           or record_data_dict.get('memory_id')
                    if mem_id_for_embedding and mem_id_for_embedding in self._embedding_cache:
                        record_data_dict["_embedding_vector_for_jsonl_"] = self._embedding_cache[mem_id_for_embedding]
                    f.write(json.dumps(record_data_dict) + "\n")
            logger.info(f"MBS Store: Appended {len(memories_to_archive)} memories to archive: {archive_filepath}")
        except Exception as e:
            logger.error(f"MBS Store: Failed to archive memories to {archive_filepath}: {e}", exc_info=True)

    def start_lifecycle_management_tasks(self, decay_interval: int = 3600 * 6, archive_interval: int = 86400):

        if not MEMORY_LIFECYCLE_MANAGER_LOADED:
            logger.warning("MBSMemoryService: Memory Lifecycle Manager not loaded. Background tasks will not start.")
            return
        logger.info(
            f"MBSMemoryService: Starting background tasks (Decay: {decay_interval}s, Archive: {archive_interval}s).")
        self._stop_background_tasks.clear()
        if decay_interval > 0: self._background_tasks.append(
            asyncio.create_task(self._periodic_decay_task(decay_interval)))
        if archive_interval > 0: self._background_tasks.append(
            asyncio.create_task(self._periodic_archive_forget_task(archive_interval)))

    async def stop_lifecycle_management_tasks(self):

        if not self._background_tasks: return
        logger.info("MBSMemoryService: Stopping background lifecycle tasks...")
        self._stop_background_tasks.set()
        await asyncio.sleep(0.1)
        for task in self._background_tasks:
            if not task.done(): task.cancel()
        results = await asyncio.gather(*self._background_tasks, return_exceptions=True)
        for i, result in enumerate(results):
            if isinstance(result, Exception) and not isinstance(result, asyncio.CancelledError):
                logger.error(f"MBSMemoryService: Error during background task {i + 1} shutdown: {result}")
        self._background_tasks = []
        logger.info("MBSMemoryService: Background tasks stopped.")

# -------------------- persistent_log_service.py --------------------

# ceaf_core/services/persistent_log_service.py

import sqlite3
import json
import time
import logging
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple

logger = logging.getLogger(__name__)

DEFAULT_LOG_DB_PATH = "./data/ceaf_persistent_logs.sqlite"
LOG_DB_FILE = Path(os.getenv("CEAF_PERSISTENT_LOG_DB_PATH", DEFAULT_LOG_DB_PATH))
LOG_TABLE_NAME = "ceaf_logs"

class PersistentLogService:
    """
    A service for persistently storing and retrieving structured log events
    from the CEAF system, using an SQLite database.
    """

    def __init__(self, db_path: Optional[Union[str, Path]] = None):
        self.db_path = Path(db_path or LOG_DB_FILE)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._initialize_db()
        logger.info(f"PersistentLogService initialized with database at: {self.db_path.resolve()}")

    def _get_db_connection(self) -> sqlite3.Connection:
        """Establishes and returns a database connection."""
        # check_same_thread=False is suitable for many async frameworks if the connection
        # is managed per request/task or if operations are short-lived.
        # For more complex scenarios, a proper connection pool or fully async driver (aiosqlite)
        # might be needed, but for this logging service, this should be acceptable.
        return sqlite3.connect(self.db_path, check_same_thread=False)

    def _initialize_db(self):
        """Creates the log table if it doesn't already exist."""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS {LOG_TABLE_NAME} (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp REAL NOT NULL,
                        event_type TEXT NOT NULL,
                        source_agent TEXT,
                        session_id TEXT,
                        turn_id TEXT,
                        data_payload TEXT, -- JSON formatted string
                        tags TEXT -- Comma-separated string or JSON array string
                    )
                """)
                # Add indexes for common query fields
                cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_event_type ON {LOG_TABLE_NAME} (event_type);")
                cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_timestamp ON {LOG_TABLE_NAME} (timestamp);")
                cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_session_id ON {LOG_TABLE_NAME} (session_id);")
                cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_turn_id ON {LOG_TABLE_NAME} (turn_id);")
                conn.commit()
            logger.debug(f"Database table '{LOG_TABLE_NAME}' initialized/verified.")
        except sqlite3.Error as e:
            logger.error(f"Failed to initialize database table '{LOG_TABLE_NAME}': {e}", exc_info=True)
            raise

    def log_event(
        self,
        event_type: str,
        data_payload: Dict[str, Any],
        source_agent: Optional[str] = None,
        session_id: Optional[str] = None,
        turn_id: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -> Optional[int]:
        """
        Logs a structured event to the persistent store.

        Args:
            event_type: A string identifying the type of event (e.g., "ORA_LLM_REQUEST", "MCL_ANALYSIS").
            data_payload: A dictionary containing the core data of the event. Will be stored as JSON.
            source_agent: The name of the agent or module that generated the event.
            session_id: The session ID associated with this event, if applicable.
            turn_id: The turn ID (or invocation ID) associated with this event, if applicable.
            tags: A list of string tags for categorization and easier querying.

        Returns:
            The row ID of the inserted log entry, or None if insertion failed.
        """
        current_timestamp = time.time()
        data_payload_json = json.dumps(data_payload, default=str) # default=str for non-serializable types
        tags_str = ",".join(sorted(list(set(tags)))) if tags else None

        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    f"""
                    INSERT INTO {LOG_TABLE_NAME}
                    (timestamp, event_type, source_agent, session_id, turn_id, data_payload, tags)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        current_timestamp,
                        event_type,
                        source_agent,
                        session_id,
                        turn_id,
                        data_payload_json,
                        tags_str,
                    ),
                )
                conn.commit()
                last_row_id = cursor.lastrowid
                logger.debug(f"Logged event: Type='{event_type}', Source='{source_agent}', ID={last_row_id}")
                return last_row_id
        except sqlite3.Error as e:
            logger.error(
                f"Failed to log event (Type: {event_type}, Source: {source_agent}): {e}",
                exc_info=True
            )
            return None

    def query_logs(
        self,
        event_type: Optional[str] = None,
        source_agent: Optional[str] = None,
        session_id: Optional[str] = None,
        turn_id: Optional[str] = None,
        time_window_start_ts: Optional[float] = None,
        time_window_end_ts: Optional[float] = None,
        tags_contain_any: Optional[List[str]] = None,
        limit: int = 100,
        offset: int = 0,
        order_by_timestamp_desc: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        Queries the persistent log store based on various criteria.

        Args:
            event_type: Filter by event type.
            source_agent: Filter by source agent.
            session_id: Filter by session ID.
            turn_id: Filter by turn ID.
            time_window_start_ts: Unix timestamp for the start of the query window.
            time_window_end_ts: Unix timestamp for the end of the query window.
            tags_contain_any: List of tags; returns logs that have at least one of these tags.
            limit: Maximum number of log entries to return.
            offset: Number of log entries to skip (for pagination).
            order_by_timestamp_desc: If True, order by timestamp descending (newest first).

        Returns:
            A list of dictionaries, where each dictionary represents a log entry.
            The 'data_payload' will be a dictionary (parsed from JSON), and 'tags' will be a list.
        """
        conditions: List[str] = []
        params: List[Any] = []

        if event_type:
            conditions.append("event_type = ?")
            params.append(event_type)
        if source_agent:
            conditions.append("source_agent = ?")
            params.append(source_agent)
        if session_id:
            conditions.append("session_id = ?")
            params.append(session_id)
        if turn_id:
            conditions.append("turn_id = ?")
            params.append(turn_id)
        if time_window_start_ts:
            conditions.append("timestamp >= ?")
            params.append(time_window_start_ts)
        if time_window_end_ts:
            conditions.append("timestamp <= ?")
            params.append(time_window_end_ts)

        if tags_contain_any and tags_contain_any:
            tag_conditions = []
            for tag in tags_contain_any:
                # This assumes tags are stored comma-separated.
                # For more robust tag querying, a separate tags table and many-to-many relationship
                # or SQLite's JSON1 extension with json_each would be better.
                # Simple LIKE for now. Add leading/trailing comma to query and stored tags for better matching.
                tag_conditions.append("',' || tags || ',' LIKE ?")
                params.append(f"%,{tag},%") # Match ",tag,"
            if tag_conditions:
                conditions.append(f"({ ' OR '.join(tag_conditions) })")

        where_clause = f"WHERE {' AND '.join(conditions)}" if conditions else ""
        order_clause = f"ORDER BY timestamp {'DESC' if order_by_timestamp_desc else 'ASC'}"
        limit_clause = "LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        query = f"SELECT id, timestamp, event_type, source_agent, session_id, turn_id, data_payload, tags FROM {LOG_TABLE_NAME} {where_clause} {order_clause} {limit_clause}"

        results: List[Dict[str, Any]] = []
        try:
            with self._get_db_connection() as conn:
                # Make rows returned as dictionaries
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute(query, params)
                rows = cursor.fetchall()
                for row in rows:
                    row_dict = dict(row)
                    try:
                        row_dict["data_payload"] = json.loads(row_dict["data_payload"]) if row_dict["data_payload"] else {}
                    except json.JSONDecodeError:
                        logger.warning(f"Failed to parse data_payload for log ID {row_dict['id']}. Payload: {row_dict['data_payload']}")
                        row_dict["data_payload"] = {"error": "failed_to_parse_json", "raw": row_dict["data_payload"]}

                    tags_str = row_dict.get("tags")
                    row_dict["tags"] = [t.strip() for t in tags_str.split(',') if t.strip()] if tags_str else []
                    results.append(row_dict)
            logger.debug(f"Query executed: '{query[:100]}...'. Params: {params}. Found {len(results)} results.")
        except sqlite3.Error as e:
            logger.error(f"Failed to query logs: {e}. Query: {query}", exc_info=True)
        return results

    def get_total_log_count(self) -> int:
        """Returns the total number of log entries."""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(f"SELECT COUNT(*) FROM {LOG_TABLE_NAME}")
                count = cursor.fetchone()
                return count[0] if count else 0
        except sqlite3.Error as e:
            logger.error(f"Failed to get total log count: {e}", exc_info=True)
            return 0

    def close(self):
        """
        This method isn't strictly necessary for sqlite3 as connections are typically
        managed with context managers (`with ... as conn:`).
        If a persistent connection were held by the class instance, this would close it.
        """
        logger.info("PersistentLogService close called (typically no-op for connection-per-operation).")


# -------------------- __init__.py --------------------



# -------------------- common_utils.py --------------------

# Common Utilities
# ceaf_project/ceaf_core/tools/common_utils.py

import json
import logging
import re
from typing import Dict, Any, Optional, List, Union
from pydantic import BaseModel, ValidationError, Field

logger = logging.getLogger(__name__)


# --- Text Processing Utilities ---

def sanitize_text_for_logging(text: Optional[str], max_length: int = 100) -> str:
    """Sanitizes text for logging, truncating and escaping newlines."""
    if not text:
        return "<empty>"
    sanitized = text.replace("\n", "\\n").replace("\r", "\\r")
    if len(sanitized) > max_length:
        return sanitized[:max_length] + "..."
    return sanitized


def extract_json_from_text(text: str) -> Optional[Union[Dict[str, Any], List[Any]]]:
    """
    Attempts to extract the first valid JSON object or array from a string.
    Handles cases where JSON might be embedded within other text or markdown code blocks.
    """
    if not text:
        return None

    # Try to find JSON within markdown code blocks first
    match = re.search(r"```json\s*([\s\S]*?)\s*```", text, re.IGNORECASE)
    if match:
        json_str = match.group(1).strip()
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(
                f"JSON common_utils: Found JSON in markdown, but failed to parse: {e}. Content: {sanitize_text_for_logging(json_str)}")
            # Fall through to try parsing the whole text or other methods

    # If no markdown, or markdown parsing failed, try to find JSON embedded anywhere
    # This is a bit more heuristic. Looks for the start of a JSON object/array.
    json_starts = [i for i, char in enumerate(text) if char in ('{', '[')]
    for start_index in json_starts:
        # Try to parse from this start point
        balance = 0
        if text[start_index] == '{':
            open_char, close_char = '{', '}'
        else:
            open_char, close_char = '[', ']'

        for end_index in range(start_index, len(text)):
            if text[end_index] == open_char:
                balance += 1
            elif text[end_index] == close_char:
                balance -= 1

            if balance == 0:  # Found a potentially complete JSON structure
                potential_json_str = text[start_index: end_index + 1]
                try:
                    # Validate if it's actually parseable JSON
                    parsed_json = json.loads(potential_json_str)
                    logger.debug(
                        f"JSON common_utils: Successfully extracted JSON: {sanitize_text_for_logging(potential_json_str)}")
                    return parsed_json
                except json.JSONDecodeError:
                    # Not a valid JSON structure, continue searching
                    continue
                    # If balance never reaches 0 after a start, it's likely not well-formed from that start

    logger.warning(f"JSON common_utils: Could not extract valid JSON from text: {sanitize_text_for_logging(text)}")
    return None


# --- Pydantic Model Utilities ---

def pydantic_to_dict_str(model_instance: Optional[BaseModel], indent: Optional[int] = 2,
                         exclude_none: bool = True) -> str:
    """Converts a Pydantic model instance to a JSON string, handling None."""
    if model_instance is None:
        return "None"
    try:
        return model_instance.model_dump_json(indent=indent, exclude_none=exclude_none)
    except Exception as e:
        logger.error(f"Pydantic common_utils: Error serializing Pydantic model {type(model_instance)}: {e}")
        return f"<Error serializing model: {type(model_instance).__name__}>"


def parse_llm_json_output(
        json_str: Optional[str],
        pydantic_model: type[BaseModel],
        strict: bool = False  # If true, requires perfect match. If false, tries to extract first.
) -> Optional[BaseModel]:

    if not json_str:
        logger.warning(f"Pydantic common_utils: Received empty JSON string for model {pydantic_model.__name__}")
        return None

    parsed_dict = None
    try:
        # First, try direct parsing
        parsed_dict = json.loads(json_str)
    except json.JSONDecodeError as e1:
        if strict:
            logger.error(
                f"Pydantic common_utils (strict): Failed to parse JSON string for model {pydantic_model.__name__}: {e1}. Raw: {sanitize_text_for_logging(json_str)}")
            return None

        logger.warning(
            f"Pydantic common_utils: Direct JSON parsing failed for model {pydantic_model.__name__}, attempting extraction. Error: {e1}")
        extracted_content = extract_json_from_text(json_str)
        if isinstance(extracted_content, dict):
            parsed_dict = extracted_content
        else:
            logger.error(
                f"Pydantic common_utils: Could not extract valid JSON dict for model {pydantic_model.__name__} from text. Extracted: {type(extracted_content)}")
            return None

    if parsed_dict is None:  # Should be caught by above, but as a safeguard
        logger.error(
            f"Pydantic common_utils: No valid JSON dictionary found or extracted for model {pydantic_model.__name__}. Original: {sanitize_text_for_logging(json_str)}")
        return None

    try:
        model_instance = pydantic_model(**parsed_dict)
        return model_instance
    except ValidationError as e2:
        logger.error(
            f"Pydantic common_utils: Validation error for model {pydantic_model.__name__}: {e2}. Parsed Dict: {parsed_dict}")
        return None
    except Exception as e3:
        logger.error(
            f"Pydantic common_utils: Unexpected error instantiating model {pydantic_model.__name__}: {e3}. Parsed Dict: {parsed_dict}")
        return None


# --- Tool Output Formatting ---

def create_successful_tool_response(data: Optional[Dict[str, Any]] = None, message: Optional[str] = None) -> Dict[
    str, Any]:
    """Creates a standard success response dictionary for ADK tools."""
    response = {"status": "success"}
    if message:
        response["message"] = message
    if data:
        response.update(data)  # Merge data dict into response
    return response


def create_error_tool_response(error_message: str, details: Optional[Any] = None, error_code: Optional[str] = None) -> \
Dict[str, Any]:
    """Creates a standard error response dictionary for ADK tools."""
    response = {"status": "error", "error_message": error_message}
    if details:
        response["details"] = details
    if error_code:
        response["error_code"] = error_code
    return response


# -------------------- kgs_tools.py --------------------

# ceaf_core/tools/kgs_tools.py

import logging
import json
from typing import Dict, Any, Optional, List, cast
import asyncio



from google.adk.tools import FunctionTool, ToolContext

from .common_utils import create_successful_tool_response, create_error_tool_response, sanitize_text_for_logging

logger = logging.getLogger("KGSTools")

# --- Initialize flags for loaded modules/functions ---
MBS_AND_TYPES_LOADED = False
LIFECYCLE_FUNCTIONS_AVAILABLE = False


# --- Define Placeholders ---
# These will be used if the real imports fail.

class _Placeholder_MBSMemoryService:
    """Placeholder for MBSMemoryService when real import fails."""

    async def add_specific_memory(self, record_object: Any):
        logger.warning("Using dummy async MBSMemoryService.add_specific_memory")

    _in_memory_explicit_cache: List = []
    _in_memory_kg_entities_cache: List = []
    _in_memory_kg_relations_cache: List = []
    store_path: Any = type('DummyPath', (), {
        'exists': lambda: False,
        'mkdir': lambda **k: None,
        'unlink': lambda: None
    })()


class _Placeholder_KGS_KGEntity:
    """Placeholder for KGS_KGEntity when real import fails."""
    pass


class _Placeholder_KGS_KGRelation:
    """Placeholder for KGS_KGRelation when real import fails."""
    pass


class _Placeholder_ExplicitMemory:
    """Placeholder for ExplicitMemory when real import fails."""
    pass


class _Placeholder_KGEntityRecord:
    """Placeholder for KGEntityRecord when real import fails."""

    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def model_dump(self, **kwargs):
        return self.__dict__


class _Placeholder_KGRelationRecord:
    """Placeholder for KGRelationRecord when real import fails."""

    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def model_dump(self, **kwargs):
        return self.__dict__


class _Placeholder_BaseMemory:
    """Placeholder for BaseMemory when real import fails."""
    pass


class _Placeholder_MemorySourceType:
    """Placeholder for MemorySourceType when real import fails."""
    INTERNAL_REFLECTION = "internal_reflection"


class _Placeholder_MemorySalience:
    """Placeholder for MemorySalience when real import fails."""
    MEDIUM = "medium"


class _Placeholder_KGEntityType:
    """Placeholder for KGEntityType when real import fails."""
    OTHER = "other"


def _placeholder_update_dynamic_salience(mem, access_type):
    """Placeholder for update_dynamic_salience when real import fails."""
    pass


# --- Store reference to real class for type checking ---
_RealMBSClassForCheck = None

# --- Attempt to import real modules and functions ---
try:
    from ..services.mbs_memory_service import MBSMemoryService
    from ..agents.kgs_agent import KGEntity as KGS_KGEntity, KGRelation as KGS_KGRelation
    from ..modules.memory_blossom.memory_types import (
        ExplicitMemory, KGEntityRecord, KGRelationRecord, MemorySourceType, MemorySalience, KGEntityType, BaseMemory
    )

    # Store the real class for type checking
    _RealMBSClassForCheck = MBSMemoryService
    MBS_AND_TYPES_LOADED = True
    logger.info("KGS Tools: Successfully loaded core MBS and types.")

    try:
        from ..modules.memory_blossom.memory_lifecycle_manager import update_dynamic_salience

        LIFECYCLE_FUNCTIONS_AVAILABLE = True
        logger.info("KGS Tools: Successfully loaded lifecycle functions.")
    except ImportError as e_lifecycle:
        update_dynamic_salience = _placeholder_update_dynamic_salience  # type: ignore
        logger.warning(f"KGS Tools: Lifecycle function 'update_dynamic_salience' not loaded: {e_lifecycle}. "
                       "Salience updates will use placeholder.")

except ImportError as e_core:
    # Assign placeholders if core imports fail
    MBSMemoryService = _Placeholder_MBSMemoryService  # type: ignore
    _RealMBSClassForCheck = _Placeholder_MBSMemoryService
    KGS_KGEntity = _Placeholder_KGS_KGEntity  # type: ignore
    KGS_KGRelation = _Placeholder_KGS_KGRelation  # type: ignore
    ExplicitMemory = _Placeholder_ExplicitMemory  # type: ignore
    KGEntityRecord = _Placeholder_KGEntityRecord  # type: ignore
    KGRelationRecord = _Placeholder_KGRelationRecord  # type: ignore
    BaseMemory = _Placeholder_BaseMemory  # type: ignore
    MemorySourceType = _Placeholder_MemorySourceType  # type: ignore
    MemorySalience = _Placeholder_MemorySalience  # type: ignore
    KGEntityType = _Placeholder_KGEntityType  # type: ignore
    update_dynamic_salience = _placeholder_update_dynamic_salience  # type: ignore
    logger.error(f"KGS Tools: Failed to load necessary types or MBS service: {e_core}. "
                 "Tools may be non-functional.")
    logger.error(f"KGS Tools: CRITICAL IMPORT FAILURE (e_core): {e_core}", exc_info=True)  # Log with traceback


# --- Improved Helper to get MBS from context ---
# In ceaf_core/tools/kgs_tools.py

def _get_mbs_from_context(tool_context: ToolContext) -> Optional[MBSMemoryService]:
    """
    More robust memory service retrieval from tool context.
    """
    logger.info(f"KGS Tools: _get_mbs_from_context called. tool_context type: {type(tool_context)}")
    if tool_context is None:
        logger.error("KGS Tools: _get_mbs_from_context received tool_context as None!")
        return None
    logger.info(f"KGS Tools: dir(tool_context): {dir(tool_context)}")

    memory_service_candidate: Any = None
    ic = None

    if hasattr(tool_context, 'invocation_context'):
        ic = tool_context.invocation_context
    else:
        logger.warning("KGS Tools: ToolContext has no 'invocation_context'. Cannot retrieve MBS.")
        return None

    # Attempt 1: Direct access from invocation_context if explicitly set (good for dummy context)
    if hasattr(ic, 'memory_service'):
        memory_service_candidate = ic.memory_service
        if memory_service_candidate:
            logger.debug("KGS Tools: Found memory_service_candidate via ic.memory_service")

    # Attempt 2: Standard ADK runner services access
    if not memory_service_candidate and hasattr(ic, 'runner') and hasattr(ic.runner, '_services'):
        memory_service_candidate = ic.runner._services.get('memory_service')
        if memory_service_candidate:
            logger.debug("KGS Tools: Found memory_service_candidate via ic.runner._services")

    # Attempt 3: Alternative runner access
    if not memory_service_candidate and hasattr(ic, 'runner') and hasattr(ic.runner, 'memory_service'):
        memory_service_candidate = ic.runner.memory_service
        if memory_service_candidate:
            logger.debug("KGS Tools: Found memory_service_candidate via ic.runner.memory_service")

    # Attempt 4: Check if services are stored differently on invocation_context
    if not memory_service_candidate and hasattr(ic, 'services') and isinstance(ic.services, dict):
        memory_service_candidate = ic.services.get('memory_service')
        if memory_service_candidate:
            logger.debug("KGS Tools: Found memory_service_candidate via ic.services")

    if not memory_service_candidate:
        logger.warning("KGS Tools: Could not find memory_service_candidate through common context paths.")
        # Try global adk_components as a last resort for startup tasks
        try:
            from ceaf_project.main import adk_components as main_adk_components_module_level
            memory_service_candidate = main_adk_components_module_level.get('memory_service')
            if memory_service_candidate:
                logger.info("KGS Tools: Retrieved memory_service_candidate from main.adk_components (startup fallback)")
        except ImportError:
            logger.debug("KGS Tools: main.adk_components not found or importable for fallback.")
            pass  # Fall through

    if not memory_service_candidate:
        logger.error("KGS Tools: MBSMemoryService instance completely not found in context.")
        return None


    logger.debug(f"KGS Tools: Candidate type: {type(memory_service_candidate)}, "
                 f"MBS_AND_TYPES_LOADED here: {MBS_AND_TYPES_LOADED}, "
                 f"_RealMBSClassForCheck here: {_RealMBSClassForCheck}, "
                 f"Local MBSMemoryService here: {MBSMemoryService}")

    if MBS_AND_TYPES_LOADED and _RealMBSClassForCheck is not _Placeholder_MBSMemoryService:
        if isinstance(memory_service_candidate, _RealMBSClassForCheck):
            logger.info("KGS Tools: Validated memory service against _RealMBSClassForCheck.")
            return cast(MBSMemoryService, memory_service_candidate)
        else:
            logger.warning(f"KGS Tools: Type mismatch. Candidate type: {type(memory_service_candidate)}, "
                           f"expected real type from this module: {_RealMBSClassForCheck}.")

    logger.info(f"KGS Tools: memory_service_candidate type: {type(memory_service_candidate)}")
    logger.info(f"KGS Tools: _RealMBSClassForCheck type: {type(_RealMBSClassForCheck)}")
    logger.info(
        f"KGS Tools: Is candidate instance of _RealMBSClassForCheck? {isinstance(memory_service_candidate, _RealMBSClassForCheck)}")

    if (hasattr(memory_service_candidate, 'search_raw_memories') and
            hasattr(memory_service_candidate, 'add_specific_memory') and
            hasattr(memory_service_candidate, 'get_memory_by_id')):
        logger.warning(
            "KGS Tools: Using DUCK-TYPING for MBSMemoryService as strict isinstance checks failed or imports were incomplete.")
        # Even if MBSMemoryService is a placeholder here, the cast is for type hinting.
        # The actual object `memory_service_candidate` is what's returned.
        return cast(MBSMemoryService, memory_service_candidate)
    else:
        logger.error(f"KGS Tools: Found memory_service_candidate (type: {type(memory_service_candidate)}) "
                     "but it doesn't match expected MBSMemoryService interface (duck-typing failed).")
        return None


# --- Tool Functions with Enhanced Documentation ---

def get_explicit_memories_for_kg_synthesis(
        tool_context: ToolContext,
        max_memories_to_fetch: int = 20,
        min_salience_score: Optional[float] = 0.3,
        only_with_structured_data: bool = False

) -> Dict[str, Any]:
    """
    Retrieves a batch of explicit memories from CEAF's memory store for knowledge graph synthesis.

    This tool is designed to provide the KGS Agent with raw material that can be processed
    to extract entities and relations. It prioritizes recent memories with sufficient salience
    and optionally filters for memories that contain structured data.

    Use this tool when:
    - The KGS Agent needs fresh memory content to process
    - You want to gather memories suitable for knowledge extraction
    - Building knowledge graphs from recent user interactions or system observations

    Args:
        max_memories_to_fetch: Maximum number of memories to return (default: 20)
        min_salience_score: Minimum salience score threshold for filtering (default: 0.3)
        only_with_structured_data: If True, only return memories with structured_data field
        tool_context: ADK tool context (automatically injected by ADK)

    Returns:
        Dictionary with structure:
        - On success: {'status': 'success', 'explicit_memories_batch': [memory_dicts], 'message': str}
        - On error: {'status': 'error', 'error_message': str, 'details': Any, 'error_code': str}
    """
    logger.info(f"KGS Tool (get_explicit_memories_for_kg_synthesis): Fetching up to {max_memories_to_fetch} memories.")

    if not MBS_AND_TYPES_LOADED:
        return create_error_tool_response(
            "MBS or required types not loaded.",
            details="System configuration error - core dependencies unavailable.",
            error_code="MBS_UNAVAILABLE"
        )

    mbs: Optional[MBSMemoryService] = _get_mbs_from_context(tool_context)
    if not mbs or not hasattr(mbs, '_in_memory_explicit_cache'):
        return create_error_tool_response(
            "MBS not available or not properly initialized.",
            error_code="MBS_UNAVAILABLE"
        )

    try:
        candidate_memories = list(mbs._in_memory_explicit_cache)  # type: ignore

        # Apply filters
        if min_salience_score is not None:
            candidate_memories = [
                mem for mem in candidate_memories
                if getattr(mem, 'dynamic_salience_score', 0.0) >= min_salience_score
            ]

        if only_with_structured_data:
            candidate_memories = [
                mem for mem in candidate_memories
                if (hasattr(mem, 'content') and
                    getattr(getattr(mem, 'content', None), 'structured_data', None))
            ]

        # Sort by timestamp (most recent first)
        candidate_memories.sort(key=lambda m: getattr(m, 'timestamp', 0), reverse=True)

        # Extract memory data using model_dump
        selected_memories_data = []
        for mem in candidate_memories[:max_memories_to_fetch]:
            if hasattr(mem, 'model_dump'):
                selected_memories_data.append(mem.model_dump(exclude_none=True))
            else:
                logger.warning(f"KGS Tool: Memory object {getattr(mem, 'memory_id', 'Unknown ID')} "
                               "lacks model_dump method.")

        logger.info(f"KGS Tool (get_explicit_memories_for_kg_synthesis): "
                    f"Retrieved {len(selected_memories_data)} memories for KGS.")

        return create_successful_tool_response(
            data={"explicit_memories_batch": selected_memories_data},
            message=f"Retrieved {len(selected_memories_data)} explicit memories."
        )

    except Exception as e:
        logger.error(f"KGS Tool: Error retrieving memories: {e}", exc_info=True)
        return create_error_tool_response(
            f"Failed to retrieve memories: {str(e)}",
            error_code="MEMORY_RETRIEVAL_ERROR"
        )


async def commit_knowledge_graph_elements(
        tool_context: ToolContext,
        extracted_entities: List[Dict[str, Any]],
        extracted_relations: List[Dict[str, Any]],
        source_memory_ids: Optional[List[str]] = None

) -> Dict[str, Any]:
    """
    Commits extracted KG entities and relations to CEAF's MemoryBlossomService.

    This tool is typically called by the KGS Agent after it has processed text content
    and generated structured knowledge graph elements. It converts the extracted data
    into proper KGEntityRecord and KGRelationRecord objects and stores them in memory.

    Use this tool when:
    - The KGS Agent has extracted entities and relations from text
    - You need to persist knowledge graph elements in the memory system
    - Building up the system's knowledge base from processed content

    Args:
        extracted_entities: List of entity dictionaries with keys like 'id_str', 'label', 'type', etc.
        extracted_relations: List of relation dictionaries with keys like 'source_id_str', 'target_id_str', etc.
        source_memory_ids: Optional list of memory IDs that were the source of these extractions
        tool_context: ADK tool context (automatically injected by ADK)

    Returns:
        Dictionary with structure:
        - On success: {'status': 'success', 'committed_entity_ids': [str], 'committed_relation_ids': [str]}
        - On error: {'status': 'error', 'error_message': str, 'details': dict, 'errors': [str]}
    """
    logger.info(f"KGS Tool (commit_knowledge_graph_elements): Committing {len(extracted_entities)} entities "
                f"and {len(extracted_relations)} relations.")

    if not MBS_AND_TYPES_LOADED:
        return create_error_tool_response(
            "MBS or required KG types not loaded.",
            details="System configuration error - core dependencies unavailable.",
            error_code="MBS_UNAVAILABLE"
        )

    mbs: Optional[MBSMemoryService] = _get_mbs_from_context(tool_context)
    if not mbs or not hasattr(mbs, 'add_specific_memory'):
        return create_error_tool_response(
            "MBS not available or not properly initialized.",
            error_code="MBS_UNAVAILABLE"
        )

    committed_entity_ids = []
    committed_relation_ids = []
    errors = []

    # Process entities
    for entity_data in extracted_entities:
        try:
            # Use KGS_KGEntity for parsing if available, otherwise use dict directly
            if KGS_KGEntity != _Placeholder_KGS_KGEntity:
                parsed_entity_data = KGS_KGEntity(**entity_data)  # type: ignore
            else:
                parsed_entity_data = entity_data  # type: ignore

            # Resolve entity type enum
            entity_type_enum_val = KGEntityType.OTHER  # type: ignore
            kgs_entity_type_str = getattr(parsed_entity_data, 'type', 'OTHER')

            if MBS_AND_TYPES_LOADED and hasattr(KGEntityType, '__members__'):
                try:
                    entity_type_enum_val = KGEntityType[kgs_entity_type_str.upper()]  # type: ignore
                except KeyError:
                    logger.warning(f"KGS Tool: Unknown entity type '{kgs_entity_type_str}' "
                                   f"for entity '{getattr(parsed_entity_data, 'label', 'N/A')}'. "
                                   "Defaulting to OTHER.")
            elif not MBS_AND_TYPES_LOADED:
                entity_type_enum_val = kgs_entity_type_str  # type: ignore

            # Create entity record
            entity_record_data = {
                "entity_id_str": getattr(parsed_entity_data, 'id_str', None),
                "label": getattr(parsed_entity_data, 'label', None),
                "entity_type": entity_type_enum_val,
                "description": getattr(parsed_entity_data, 'description', None),
                "attributes": getattr(parsed_entity_data, 'attributes', {}),
                "aliases": getattr(parsed_entity_data, 'aliases', []),
                "source_type": MemorySourceType.INTERNAL_REFLECTION,  # type: ignore
                "source_agent_name": "KGS_Agent_CommitTool",
                "salience": MemorySalience.MEDIUM,  # type: ignore
                "metadata": {"source_explicit_memory_ids": source_memory_ids} if source_memory_ids else {}
            }

            entity_record = KGEntityRecord(**entity_record_data)  # type: ignore
            await mbs.add_specific_memory(entity_record)  # type: ignore
            committed_entity_ids.append(getattr(entity_record, 'entity_id_str', 'unknown_id'))

        except Exception as e:
            err_msg = f"Failed to process/commit entity data '{str(entity_data)[:100]}...': {e}"
            logger.error(f"KGS Tool Commit: {err_msg}", exc_info=True)
            errors.append(err_msg)

    # Process relations
    for relation_data in extracted_relations:
        try:
            # Use KGS_KGRelation for parsing if available, otherwise use dict directly
            if KGS_KGRelation != _Placeholder_KGS_KGRelation:
                parsed_relation_data = KGS_KGRelation(**relation_data)  # type: ignore
            else:
                parsed_relation_data = relation_data  # type: ignore

            # Create relation record
            relation_record_data = {
                "source_entity_id_str": getattr(parsed_relation_data, 'source_id_str', None),
                "target_entity_id_str": getattr(parsed_relation_data, 'target_id_str', None),
                "relation_label": getattr(parsed_relation_data, 'label', None),
                "description": getattr(parsed_relation_data, 'context', None),  # KGS_KGRelation uses 'context'
                "attributes": getattr(parsed_relation_data, 'attributes', {}),
                "source_type": MemorySourceType.INTERNAL_REFLECTION,  # type: ignore
                "source_agent_name": "KGS_Agent_CommitTool",
                "salience": MemorySalience.MEDIUM,  # type: ignore
                "metadata": {"source_explicit_memory_ids": source_memory_ids} if source_memory_ids else {}
            }

            relation_record = KGRelationRecord(**relation_record_data)  # type: ignore
            await mbs.add_specific_memory(relation_record)  # type: ignore
            committed_relation_ids.append(getattr(relation_record, 'relation_id_str', 'unknown_id'))

        except Exception as e:
            err_msg = f"Failed to process/commit relation data '{str(relation_data)[:100]}...': {e}"
            logger.error(f"KGS Tool Commit: {err_msg}", exc_info=True)
            errors.append(err_msg)

    # Return results
    msg = f"Committed {len(committed_entity_ids)} entities and {len(committed_relation_ids)} relations."
    if errors:
        msg += f" Encountered {len(errors)} errors during commit."
        return create_error_tool_response(
            error_message=msg,
            details={
                "committed_entities": committed_entity_ids,
                "committed_relations": committed_relation_ids,
                "errors": errors
            }
        )
    else:
        return create_successful_tool_response(
            data={
                "committed_entity_ids": committed_entity_ids,
                "committed_relation_ids": committed_relation_ids
            },
            message=msg
        )


def query_knowledge_graph(
        tool_context: ToolContext,
        entity_label: Optional[str] = None,
        relation_label: Optional[str] = None,
        target_entity_label: Optional[str] = None,
        limit: int = 5

) -> Dict[str, Any]:
    """
    Performs a basic query against CEAF's knowledge graph.

    This tool enables searching for entities by label, or relations connected to entities.
    It supports partial string matching and returns rich context including related entities.
    Dynamic salience scores are updated for accessed memory objects.

    Use this tool when:
    - You need to find specific entities or relations in the knowledge graph
    - Exploring connections between knowledge graph elements
    - Retrieving context about entities for reasoning or response generation

    Args:
        entity_label: Partial or full label of entities to search for
        relation_label: Label of relations to search for (used with or without entity_label)
        target_entity_label: Currently not used for filtering (reserved for future enhancement)
        limit: Maximum number of results to return (default: 5)
        tool_context: ADK tool context (automatically injected by ADK)

    Returns:
        Dictionary with structure:
        - On success: {'status': 'success', 'kg_query_results': [result_dicts], 'message': str}
        - On error: {'status': 'error', 'error_message': str, 'error_code': str}

    Note:
        When both entity_label and relation_label are provided, the tool searches for
        relations connected to the matching entities. When only relation_label is provided,
        it searches all relations matching that label.
    """
    logger.info(f"KGS Tool (query_knowledge_graph): Query for entity '{entity_label}', "
                f"relation '{relation_label}', limit {limit}")

    if not MBS_AND_TYPES_LOADED:
        return create_error_tool_response(
            "KG types not loaded.",
            details="System configuration error - core dependencies unavailable.",
            error_code="MBS_UNAVAILABLE"
        )

    mbs: Optional[MBSMemoryService] = _get_mbs_from_context(tool_context)
    if not mbs or not hasattr(mbs, '_in_memory_kg_entities_cache') or not hasattr(mbs, '_in_memory_kg_relations_cache'):
        return create_error_tool_response(
            "MBS not available or KG caches not initialized.",
            error_code="MBS_UNAVAILABLE"
        )

    try:
        results = []
        found_objects_for_salience_update: List[BaseMemory] = []  # type: ignore

        # Search for entities if entity_label is provided
        if entity_label:
            for ent_rec in mbs._in_memory_kg_entities_cache:  # type: ignore
                if (hasattr(ent_rec, 'label') and isinstance(ent_rec.label, str) and
                        entity_label.lower() in ent_rec.label.lower()):  # type: ignore

                    if LIFECYCLE_FUNCTIONS_AVAILABLE and isinstance(ent_rec, BaseMemory):  # type: ignore
                        found_objects_for_salience_update.append(cast(BaseMemory, ent_rec))  # type: ignore

                    entity_data = ent_rec.model_dump(exclude_none=True) if hasattr(ent_rec, 'model_dump') else vars(
                        ent_rec)
                    results.append(entity_data)

                    if len(results) >= limit:
                        break

        # Handle relation queries
        if relation_label:
            relations_found_this_pass = []

            # If entities were found, search relations connected to them
            if results and entity_label:
                entities_to_check_relations_for = results[:limit]
                for entity_dict_found in entities_to_check_relations_for:
                    source_ent_id_str = entity_dict_found.get('entity_id_str')
                    if not source_ent_id_str:
                        continue

                    for rel_rec in mbs._in_memory_kg_relations_cache:  # type: ignore
                        if (hasattr(rel_rec, 'source_entity_id_str') and
                                rel_rec.source_entity_id_str == source_ent_id_str and  # type: ignore
                                hasattr(rel_rec, 'relation_label') and
                                isinstance(rel_rec.relation_label, str) and  # type: ignore
                                relation_label.lower() in rel_rec.relation_label.lower()):  # type: ignore

                            # Find target entity information
                            target_entity_info = "Unknown Target Entity"
                            if hasattr(rel_rec, 'target_entity_id_str'):  # type: ignore
                                for ent_rec_tgt in mbs._in_memory_kg_entities_cache:  # type: ignore
                                    if (hasattr(ent_rec_tgt, 'entity_id_str') and
                                            ent_rec_tgt.entity_id_str == rel_rec.target_entity_id_str):  # type: ignore
                                        target_entity_info = (f"{getattr(ent_rec_tgt, 'label', 'N/A')} "
                                                              f"(ID: {ent_rec_tgt.entity_id_str})")  # type: ignore
                                        if LIFECYCLE_FUNCTIONS_AVAILABLE and isinstance(ent_rec_tgt,
                                                                                        BaseMemory):  # type: ignore
                                            found_objects_for_salience_update.append(
                                                cast(BaseMemory, ent_rec_tgt))  # type: ignore
                                        break

                            relation_dump = (rel_rec.model_dump(exclude_none=True)
                                             if hasattr(rel_rec, 'model_dump') else vars(rel_rec))

                            relations_found_this_pass.append({
                                "relation": relation_dump,
                                "source_entity_summary": entity_dict_found.get('label', source_ent_id_str),
                                "target_entity_summary": target_entity_info
                            })

                            if LIFECYCLE_FUNCTIONS_AVAILABLE and isinstance(rel_rec, BaseMemory):  # type: ignore
                                found_objects_for_salience_update.append(cast(BaseMemory, rel_rec))  # type: ignore

                            if len(relations_found_this_pass) >= limit:
                                break

                    if len(relations_found_this_pass) >= limit:
                        break

            # If no specific entity_label or no entities found, search all relations by relation_label
            elif not entity_label:
                for rel_rec in mbs._in_memory_kg_relations_cache:  # type: ignore
                    if (hasattr(rel_rec, 'relation_label') and
                            isinstance(rel_rec.relation_label, str) and  # type: ignore
                            relation_label.lower() in rel_rec.relation_label.lower()):  # type: ignore

                        # Find source entity information
                        source_info = "Unknown Source"
                        if hasattr(rel_rec, 'source_entity_id_str'):  # type: ignore
                            for src_ent in mbs._in_memory_kg_entities_cache:  # type: ignore
                                if (hasattr(src_ent, 'entity_id_str') and
                                        src_ent.entity_id_str == rel_rec.source_entity_id_str):  # type: ignore
                                    source_info = f"{getattr(src_ent, 'label', 'N/A')} (ID: {src_ent.entity_id_str})"  # type: ignore
                                    if LIFECYCLE_FUNCTIONS_AVAILABLE and isinstance(src_ent,
                                                                                    BaseMemory):  # type: ignore
                                        found_objects_for_salience_update.append(
                                            cast(BaseMemory, src_ent))  # type: ignore
                                    break

                        # Find target entity information
                        target_info = "Unknown Target"
                        if hasattr(rel_rec, 'target_entity_id_str'):  # type: ignore
                            for tgt_ent in mbs._in_memory_kg_entities_cache:  # type: ignore
                                if (hasattr(tgt_ent, 'entity_id_str') and
                                        tgt_ent.entity_id_str == rel_rec.target_entity_id_str):  # type: ignore
                                    target_info = f"{getattr(tgt_ent, 'label', 'N/A')} (ID: {tgt_ent.entity_id_str})"  # type: ignore
                                    if LIFECYCLE_FUNCTIONS_AVAILABLE and isinstance(tgt_ent,
                                                                                    BaseMemory):  # type: ignore
                                        found_objects_for_salience_update.append(
                                            cast(BaseMemory, tgt_ent))  # type: ignore
                                    break

                        relation_dump = (rel_rec.model_dump(exclude_none=True)
                                         if hasattr(rel_rec, 'model_dump') else vars(rel_rec))

                        relations_found_this_pass.append({
                            "relation": relation_dump,
                            "source_entity_summary": source_info,
                            "target_entity_summary": target_info
                        })

                        if LIFECYCLE_FUNCTIONS_AVAILABLE and isinstance(rel_rec, BaseMemory):  # type: ignore
                            found_objects_for_salience_update.append(cast(BaseMemory, rel_rec))  # type: ignore

                        if len(relations_found_this_pass) >= limit:
                            break

            # If relations were specifically searched and found, replace results with relations
            if relations_found_this_pass:
                results = relations_found_this_pass[:limit]

        # Update dynamic salience scores for accessed objects
        if LIFECYCLE_FUNCTIONS_AVAILABLE:
            for mem_obj in found_objects_for_salience_update:
                update_dynamic_salience(mem_obj, access_type="retrieval")  # type: ignore

        # Return results
        if not results:
            return create_successful_tool_response(
                message=f"No KG elements found matching criteria: entity '{entity_label}', "
                        f"relation '{relation_label}'."
            )

        return create_successful_tool_response(
            data={"kg_query_results": results},
            message=f"Found {len(results)} KG query results."
        )

    except Exception as e:
        logger.error(f"KGS Tool: Error querying knowledge graph: {e}", exc_info=True)
        return create_error_tool_response(
            f"Failed to query knowledge graph: {str(e)}",
            error_code="KG_QUERY_ERROR"
        )


# --- Create ADK Function Tools ---

get_memories_for_kg_synthesis_tool = FunctionTool(
    func=get_explicit_memories_for_kg_synthesis
)

commit_kg_elements_tool = FunctionTool(
    func=commit_knowledge_graph_elements
)

query_knowledge_graph_tool = FunctionTool(
    func=query_knowledge_graph
)

# --- Export tools for use by other modules ---
ceaf_kgs_tools = [
    get_memories_for_kg_synthesis_tool,
    commit_kg_elements_tool,
    query_knowledge_graph_tool
]

# -------------------- mcl_tools.py --------------------

# MCL Tools - Refactored for Google ADK
# ceaf_project/ceaf_core/tools/mcl_tools.py

import logging
import json
import asyncio
from typing import Dict, Any, Optional, List

# Correct imports for Google ADK
from google.adk.tools import FunctionTool, LongRunningFunctionTool, ToolContext
from google.adk.tools.agent_tool import AgentTool

logger = logging.getLogger(__name__)

# Assuming mcl_agent and self_state_analyzer are accessible
try:
    from ..agents.mcl_agent import mcl_agent  # The actual MCL_Agent instance
    from ..modules.mcl_engine.self_state_analyzer import (
        analyze_ora_turn_observations,
        ORAStateAnalysis,
        MCL_OBSERVATIONS_LIST_KEY
    )

    MCL_MODULES_LOADED = True
except ImportError as e:
    logger.error(
        f"MCL Tools: Critical import error: {e}. MCL tools may not function correctly.",
        exc_info=True)
    MCL_MODULES_LOADED = False


    class ORAStateAnalysis:  # type: ignore
        def __init__(self):
            self.turn_id = "dummy"
            self.eoc_assessment = "dummy"
            self.eoc_confidence = 0.0
            self.summary_notes = []
            self.flags = {}
            self.tool_calls_attempted = 0
            self.tool_calls_succeeded = 0
            self.tool_errors = []
            self.last_llm_finish_reason = None
            self.function_calls_in_last_response = []
            # Quantitative scores
            self.novelty_score = None  # Changed to None
            self.coherence_score = None  # Changed to None
            self.grounding_score = None  # Changed to None
            self.eoc_quantitative_score = None  # Changed to None
            self.raw_observations_count = 0  # Added

        def to_dict(self): return self.__dict__


    def analyze_ora_turn_observations(turn_id, turn_observations, **kwargs) -> ORAStateAnalysis:  # type: ignore
        return ORAStateAnalysis()


    mcl_agent = None  # type: ignore
    MCL_OBSERVATIONS_LIST_KEY = "mcl:dummy_obs_log"

from .common_utils import (
    create_successful_tool_response,
    create_error_tool_response,
    sanitize_text_for_logging
)


def _prepare_mcl_query_from_analysis(
        analysis: ORAStateAnalysis,  # type: ignore
        current_ncf_params_ora: Optional[Dict[str, Any]],  # Renamed for clarity
        ora_last_user_query: Optional[str] = None,  # Added for more context to MCL
        additional_context: Optional[str] = None
) -> str:
    """
    Prepares a structured query for the MCL Agent based on ORA state analysis.

    Args:
        analysis: ORAStateAnalysis object containing turn assessment
        current_ncf_params_ora: NCF parameters that were active during the assessed turn
        ora_last_user_query: The user query that ORA was responding to in the assessed turn
        additional_context: Any additional context for MCL assessment

    Returns:
        Formatted query string for MCL Agent
    """
    tool_errors_summary = []
    if hasattr(analysis, 'tool_errors') and analysis.tool_errors:  # type: ignore
        tool_errors_summary = [
            f"- Tool: {err.get('tool_name')}, Args: {sanitize_text_for_logging(str(err.get('args')), 50)}, Summary: {sanitize_text_for_logging(err.get('summary'), 70)}"
            for err in analysis.tool_errors[:3]  # type: ignore
        ]
        if len(analysis.tool_errors) > 3:  # type: ignore
            tool_errors_summary.append(f"...and {len(analysis.tool_errors) - 3} more errors.")  # type: ignore

    query_parts = [
        "Perform a metacognitive assessment of ORA's recent performance and provide guidance by determining optimal NCF parameters for the next turn.",
        f"Assessment Input for ORA's Turn ID '{getattr(analysis, 'turn_id', 'N/A')}':",
        # Analyzer's assessment
        f"- eoc_assessment_analyzer: {getattr(analysis, 'eoc_assessment', 'N/A')}",
        f"- eoc_confidence_analyzer: {getattr(analysis, 'eoc_confidence', 0.0):.2f}",
        # Quantitative Scores from Analyzer
        f"- quantitative_eoc_scores:",
        f"  - novelty_score: {getattr(analysis, 'novelty_score', 'N/A')}",
        f"  - coherence_score: {getattr(analysis, 'coherence_score', 'N/A')}",
        f"  - grounding_score: {getattr(analysis, 'grounding_score', 'N/A')}",
        f"  - combined_eoc_score: {getattr(analysis, 'eoc_quantitative_score', 'N/A')}",
        f"- analyzer_flags: {json.dumps(getattr(analysis, 'flags', {}))}",
        f"- analyzer_key_notes: {'; '.join(getattr(analysis, 'summary_notes', [])) if getattr(analysis, 'summary_notes', []) else 'None'}",
        # ORA's operational details for the turn
        f"- tool_usage_summary: Attempted {getattr(analysis, 'tool_calls_attempted', 0)}, Succeeded {getattr(analysis, 'tool_calls_succeeded', 0)}",
        f"- tool_errors_summary: {'; '.join(tool_errors_summary) if tool_errors_summary else 'None'}",
        f"- last_llm_finish_reason_ora: {getattr(analysis, 'last_llm_finish_reason', 'N/A') or 'N/A'}",
        f"- function_calls_in_ora_last_response: {json.dumps(getattr(analysis, 'function_calls_in_last_response', []))}",
        # Context ORA was operating under
        f"- current_ncf_parameters_ora: {json.dumps(current_ncf_params_ora or {})}",
        f"- ora_last_user_query_for_turn: {sanitize_text_for_logging(ora_last_user_query, 150) if ora_last_user_query else 'N/A'}",
    ]
    if additional_context:
        query_parts.append(f"- additional_mcl_context: {additional_context}")

    query_parts.append(
        "\nBased on ALL this input, provide your structured JSON guidance (ora_state_assessment_mcl, ncf_target_parameters, operational_advice_for_ora, reasoning_for_guidance) as per your primary instructions."
    )
    return "\n".join(query_parts)


# Function for preparing MCL assessment input - converted to a simple function for ADK
def prepare_input_for_mcl_assessment( # This is the Python function
        turn_to_assess_id: str,
        tool_context: ToolContext, # Added tool_context
        additional_mcl_context: Optional[str] = None
) -> Dict[str, Any]:
    """
    Gathers ORA's turn observations for a specific turn, analyzes them using the
    SelfStateAnalyzer, and formats a structured query input string for the MCL_Agent.
    This helps MCL_Agent assess ORA's performance and determine optimal NCF parameters.
    This tool is typically called by ORA when it needs metacognitive guidance or after
    a significant interaction turn.
    (THIS DOCSTRING WILL BE USED BY FunctionTool)
    """
    logger.info(f"MCL Tool (prepare_input_for_mcl_assessment): Request for turn_id '{turn_to_assess_id}'.")
    if not MCL_MODULES_LOADED:
        return create_error_tool_response("MCL system components (analyzer or agent) not available.")

    # ADK session state access
    try:
        # In ADK, state is directly on tool_context.state (which is a dict-like State object)
        # The MCL_OBSERVATIONS_LIST_KEY holds a list of dicts.
        all_observations_from_state = tool_context.state.get(MCL_OBSERVATIONS_LIST_KEY, []) # type: ignore
        relevant_turn_observations = [obs for obs in all_observations_from_state if
                                      obs.get("turn_id") == turn_to_assess_id]

        if not relevant_turn_observations:
            logger.warning(
                f"No observation data found for turn {turn_to_assess_id} in tool_context.state under key '{MCL_OBSERVATIONS_LIST_KEY}'.")
            return create_error_tool_response(f"No observation data for turn {turn_to_assess_id} found in current session state.")

        # Retrieve context data - adapt these based on your actual data storage pattern in session state
        current_ncf_params_for_assessed_turn = tool_context.state.get(f"ora_turn_ncf_params:{turn_to_assess_id}", {})
        user_query_for_assessed_turn = tool_context.state.get(f"ora_turn_user_query:{turn_to_assess_id}")
        ora_response_text_for_assessed_turn = tool_context.state.get(f"ora_turn_final_response_text:{turn_to_assess_id}")

        # Analyze the turn
        ora_analysis: ORAStateAnalysis = analyze_ora_turn_observations(
            turn_id=turn_to_assess_id,
            turn_observations=relevant_turn_observations,
            ora_response_text=ora_response_text_for_assessed_turn,
            user_query_text=user_query_for_assessed_turn,
            ncf_text_summary=json.dumps(current_ncf_params_for_assessed_turn), # Pass NCF params as summary
            current_ncf_params=current_ncf_params_for_assessed_turn # Also pass them structured
        )

        mcl_query = _prepare_mcl_query_from_analysis(
            ora_analysis,
            current_ncf_params_ora=current_ncf_params_for_assessed_turn,
            ora_last_user_query=user_query_for_assessed_turn,
            additional_context=additional_mcl_context
        )

        return create_successful_tool_response(
            data={"ora_turn_analysis_summary": ora_analysis.to_dict(), "prepared_mcl_query_input": mcl_query},
            message="Input for MCL_Agent prepared."
        )

    except Exception as e:
        logger.error(f"MCL Tool: Error preparing input for MCL assessment for turn {turn_to_assess_id}: {e}", exc_info=True)
        return create_error_tool_response(f"Error in ORA state analysis: {e}", details=str(e))


# Long-running version of the preparation function for complex analysis
def prepare_input_for_mcl_assessment_generator(  # This is the Python function
        turn_to_assess_id: str,
        tool_context: ToolContext,  # Added tool_context
        additional_mcl_context: Optional[str] = None
):
    """
    Long-running version of prepare_input_for_mcl_assessment. Gathers ORA's turn
    observations, analyzes them, and formats a query input for the MCL_Agent.
    Yields progress updates and returns the final analysis and query.
    (THIS DOCSTRING WILL BE USED BY LongRunningFunctionTool)
    """
    yield {"status": "pending", "message": f"Starting analysis for turn {turn_to_assess_id}..."}

    logger.info(f"MCL Tool (prepare_input_for_mcl_assessment_generator): Request for turn_id '{turn_to_assess_id}'.")

    if not MCL_MODULES_LOADED:
        yield create_error_tool_response("MCL system components (analyzer or agent) not available.")
        return

    yield {"status": "pending", "message": "Gathering observation data..."}

    try:
        all_observations_from_state = tool_context.state.get(MCL_OBSERVATIONS_LIST_KEY, [])  # type: ignore
        relevant_turn_observations = [obs for obs in all_observations_from_state if
                                      obs.get("turn_id") == turn_to_assess_id]

        if not relevant_turn_observations:
            yield create_error_tool_response(
                f"No observation data for turn {turn_to_assess_id} found in current session state.")
            return

        yield {"status": "pending", "message": "Analyzing turn observations...", "progress": "50%"}

        current_ncf_params_for_assessed_turn = tool_context.state.get(f"ora_turn_ncf_params:{turn_to_assess_id}", {})
        user_query_for_assessed_turn = tool_context.state.get(f"ora_turn_user_query:{turn_to_assess_id}")
        ora_response_text_for_assessed_turn = tool_context.state.get(
            f"ora_turn_final_response_text:{turn_to_assess_id}")

        ora_analysis: ORAStateAnalysis = analyze_ora_turn_observations(
            turn_id=turn_to_assess_id,
            turn_observations=relevant_turn_observations,
            ora_response_text=ora_response_text_for_assessed_turn,
            user_query_text=user_query_for_assessed_turn,
            ncf_text_summary=json.dumps(current_ncf_params_for_assessed_turn),
            current_ncf_params=current_ncf_params_for_assessed_turn
        )

        yield {"status": "pending", "message": "Preparing MCL query...", "progress": "75%"}

        mcl_query = _prepare_mcl_query_from_analysis(
            ora_analysis,
            current_ncf_params_ora=current_ncf_params_for_assessed_turn,
            ora_last_user_query=user_query_for_assessed_turn,
            additional_context=additional_mcl_context
        )

        yield {"status": "pending", "message": "Finalizing analysis...", "progress": "90%"}

        yield create_successful_tool_response(
            data={"ora_turn_analysis_summary": ora_analysis.to_dict(), "prepared_mcl_query_input": mcl_query},
            message="Input for MCL_Agent prepared."
        )
        return  # Explicit return for generator

    except Exception as e:
        logger.error(
            f"MCL Tool: Error in generator preparing input for MCL assessment for turn {turn_to_assess_id}: {e}",
            exc_info=True)
        yield create_error_tool_response(f"Error in ORA state analysis: {e}", details=str(e))
        return


# Create the tools for ADK
prepare_mcl_input_tool = FunctionTool(
    func=prepare_input_for_mcl_assessment
)

# Long-running version
prepare_mcl_input_long_running_tool = LongRunningFunctionTool(
    func=prepare_input_for_mcl_assessment_generator
)

# MCL Agent Tool - using AgentTool for agent-as-a-tool pattern
mcl_guidance_tool = None
if mcl_agent and MCL_MODULES_LOADED:
    # MODIFICATION START
    mcl_guidance_tool = AgentTool(
        agent=mcl_agent

    )
    # MODIFICATION END
    logger.info(f"MCL Tool: Defined MCL guidance tool (AgentTool wrapping MCL_Agent). Effective tool name will be '{getattr(mcl_guidance_tool, 'name', mcl_agent.name)}'.")
else:
    logger.error(
        "MCL Tool: 'mcl_agent' or dependent modules not loaded. "
        "MCL guidance AgentTool cannot be defined."
    )

# Collect all available tools
ceaf_mcl_tools = []

# Add the basic function tool
if prepare_mcl_input_tool:
    ceaf_mcl_tools.append(prepare_mcl_input_tool)

# Add the long-running version
if prepare_mcl_input_long_running_tool:
    ceaf_mcl_tools.append(prepare_mcl_input_long_running_tool)

# Add the MCL agent tool
if mcl_guidance_tool:
    ceaf_mcl_tools.append(mcl_guidance_tool)


# Helper function to integrate with your agent setup
def get_mcl_tools_for_agent() -> List:
    """
    Returns a list of MCL tools ready to be added to an ADK Agent.

    Usage:
        from .mcl_tools import get_mcl_tools_for_agent

        agent = Agent(
            model="gemini-2.0-flash",
            name="ora_agent",
            instruction="Your agent instructions...",
            tools=get_mcl_tools_for_agent() + other_tools
        )
    """
    return ceaf_mcl_tools



# -------------------- memory_tools.py --------------------

# Memory Tools
# ceaf_project/ceaf_core/tools/memory_tools.py

import logging
import json
import time
import asyncio # ADDED for async tool function
from typing import Dict, Any, Optional, List, Tuple, cast

from google.adk.tools import FunctionTool, ToolContext

# --- Type Imports and Placeholders ---
# Assuming memory_types.py and mbs_memory_service.py exist and are importable
# For brevity, the placeholder logic for MEMORY_TYPES_LOADED is kept but ideally it's always true.
try:
    from ..modules.memory_blossom.memory_types import (
        AnyMemoryType, # Ensure this Union includes all relevant memory types
        ExplicitMemory, ExplicitMemoryContent, MemorySourceType, MemorySalience, BaseMemory
    )
    from ..services.mbs_memory_service import MBSMemoryService # ADDED
    MEMORY_TYPES_LOADED = True
    MBS_SERVICE_LOADED = True
except ImportError as e:
    logging.warning(
        f"Memory Tools: Could not import full CEAF memory_types or MBSMemoryService: {e}. "
        "LTM tools might be limited or non-functional."
    )
    MEMORY_TYPES_LOADED = False
    MBS_SERVICE_LOADED = False
    class BaseMemoryPydantic: # Basic placeholder
        def __init__(self, **kwargs): self.__dict__.update(kwargs)
        def model_dump(self, **kwargs): return self.__dict__
    class AnyMemoryType(BaseMemoryPydantic): pass # type: ignore
    class ExplicitMemory(BaseMemoryPydantic): pass
    class ExplicitMemoryContent(BaseMemoryPydantic): pass
    class MemorySourceType: USER_INTERACTION = "user"; ORA_RESPONSE = "agent"; INTERNAL_REFLECTION = "system"
    class MemorySalience: MEDIUM = "medium"; LOW = "low"; HIGH = "high"; CRITICAL = "critical"
    class MBSMemoryService: # Dummy service
        async def search_raw_memories(self, query: str, top_k: int) -> List[Tuple[AnyMemoryType, float]]: # type: ignore
            logger.warning("Using dummy MBSMemoryService.search_raw_memories")
            return []


from .common_utils import (
    create_successful_tool_response,
    create_error_tool_response,
    sanitize_text_for_logging
)

logger = logging.getLogger(__name__)

CONTEXT_QUERY_DELIMITER = " <CEAF_CONTEXT_SEPARATOR> "

# --- Helper to get MBS from context (can be shared or in common_utils) ---
def _get_mbs_from_context(tool_context: ToolContext) -> Optional[MBSMemoryService]:
    """
    More robust memory service retrieval from tool context.
    """
    logger.debug(f"Memory Tools: _get_mbs_from_context called. tool_context type: {type(tool_context)}")
    if tool_context is None:
        logger.error("Memory Tools: _get_mbs_from_context received tool_context as None!")
        return None

    memory_service_candidate: Any = None
    ic = None

    if hasattr(tool_context, 'invocation_context') and tool_context.invocation_context is not None:
        ic = tool_context.invocation_context
        logger.debug(f"Memory Tools: Found tool_context.invocation_context: {type(ic)}")
    else:
        logger.warning("Memory Tools: ToolContext has no 'invocation_context' or it is None.")
        # Fallback directly if invocation_context is missing
        try:
            from ceaf_project.main import adk_components as main_adk_components_module_level # type: ignore
            memory_service_candidate = main_adk_components_module_level.get('memory_service')
            if memory_service_candidate:
                logger.info("Memory Tools: Retrieved memory_service_candidate from main.adk_components (early fallback)")
                if isinstance(memory_service_candidate, MBSMemoryService):
                    return cast(MBSMemoryService, memory_service_candidate)
                else:
                    logger.warning(f"Memory Tools: main.adk_components['memory_service'] is not MBSMemoryService type: {type(memory_service_candidate)}")
                    return None # Or raise, or try duck typing
        except ImportError:
            logger.error("Memory Tools: main.adk_components not found or importable for early fallback, and no invocation_context.")
            return None
        return None # If fallback didn't work or wasn't MBS type

    # Proceed with ic if it was found
    if hasattr(ic, 'memory_service'): # Check if memory_service is directly on invocation_context
        memory_service_candidate = ic.memory_service
        if memory_service_candidate:
            logger.debug("Memory Tools: Found memory_service_candidate via ic.memory_service (direct)")

    if not memory_service_candidate and hasattr(ic, 'runner'):
        if hasattr(ic.runner, '_services'):
            memory_service_candidate = ic.runner._services.get('memory_service')
            if memory_service_candidate:
                logger.debug("Memory Tools: Found memory_service_candidate via ic.runner._services")
        if not memory_service_candidate and hasattr(ic.runner, 'memory_service'):
            memory_service_candidate = ic.runner.memory_service
            if memory_service_candidate:
                logger.debug("Memory Tools: Found memory_service_candidate via ic.runner.memory_service (direct on runner)")

    if not memory_service_candidate and hasattr(ic, 'services') and isinstance(ic.services, dict):
        memory_service_candidate = ic.services.get('memory_service')
        if memory_service_candidate:
            logger.debug("Memory Tools: Found memory_service_candidate via ic.services (dict)")

    if not memory_service_candidate:
        logger.warning("Memory Tools: Could not find memory_service_candidate through common context paths. Trying global fallback.")
        try:
            from ceaf_project.main import adk_components as main_adk_components_module_level # type: ignore
            memory_service_candidate = main_adk_components_module_level.get('memory_service')
            if memory_service_candidate:
                logger.info("Memory Tools: Retrieved memory_service_candidate from main.adk_components (last resort fallback)")
        except ImportError:
            logger.debug("Memory Tools: main.adk_components not found or importable for last resort fallback.")
            pass

    if not memory_service_candidate:
        logger.error("Memory Tools: MBSMemoryService instance completely not found in context or fallbacks.")
        return None

    # Type check before returning
    if MBS_SERVICE_LOADED and isinstance(memory_service_candidate, MBSMemoryService):
        logger.info("Memory Tools: Validated memory service instance successfully.")
        return cast(MBSMemoryService, memory_service_candidate)
    elif (hasattr(memory_service_candidate, 'search_raw_memories') and
            hasattr(memory_service_candidate, 'add_specific_memory')): # Duck typing as a last resort
        logger.warning(
            "Memory Tools: Using DUCK-TYPING for MBSMemoryService as strict isinstance check failed or MBS_SERVICE_LOADED is False.")
        return cast(MBSMemoryService, memory_service_candidate)
    else:
        logger.error(f"Memory Tools: Found memory_service_candidate (type: {type(memory_service_candidate)}) "
                     "but it doesn't match expected MBSMemoryService interface (duck-typing failed).")
        return None


# --- Short-Term Session Memory Tools (using ADK session state) ---
# These remain synchronous as they operate on tool_context.state directly.

def store_short_term_memory(
        memory_key: str,
        memory_value: str,
        tool_context: ToolContext
) -> Dict[str, Any]:
    """
    Saves a key-value pair to the current session's short-term (volatile) memory.
    Useful for remembering details within the current conversational context.

    Args:
        memory_key (str): The key under which to store the value.
        memory_value (str): The string value to store.
        tool_context (ToolContext): The ADK tool context.

    Returns:
        Dict[str, Any]: A dictionary indicating success or failure.
    Example:
        store_short_term_memory(memory_key='user_preference_color', memory_value='blue')
    """
    # ... (implementation remains unchanged) ...
    agent_name_log = getattr(tool_context, 'agent_name', "UnknownAgent")
    if not memory_key or not isinstance(memory_key, str):
        return create_error_tool_response("Invalid memory_key provided. Must be a non-empty string.")
    if not isinstance(memory_value, str): # For V1, keeping it simple as string
        return create_error_tool_response("Invalid memory_value provided. Must be a string for V1 short-term memory.")

    state_key = f"stm:{memory_key}"
    try:
        tool_context.state[state_key] = memory_value
        logger.info(
            f"Memory Tool (STM): Stored '{sanitize_text_for_logging(memory_value)}' under state key '{state_key}' by agent '{agent_name_log}'.")
        return create_successful_tool_response(message=f"Information for '{memory_key}' stored in short-term memory.")
    except Exception as e:
        logger.error(f"Memory Tool (STM): Error storing to state for key '{state_key}': {e}", exc_info=True)
        return create_error_tool_response(f"Failed to store short-term memory for '{memory_key}'.")


store_stm_tool = FunctionTool(
    func=store_short_term_memory
)

def retrieve_short_term_memory(
        memory_key: str,
        tool_context: ToolContext
) -> Dict[str, Any]:
    """
    Fetches a value from the current session's short-term (volatile) memory based on a key.

    Args:
        memory_key (str): The key of the value to retrieve.
        tool_context (ToolContext): The ADK tool context.

    Returns:
        Dict[str, Any]: A dictionary containing the retrieved value or an error.
    Example:
        retrieve_short_term_memory(memory_key='user_preference_color')
    """
    # ... (implementation remains unchanged) ...
    agent_name_log = getattr(tool_context, 'agent_name', "UnknownAgent")
    if not memory_key or not isinstance(memory_key, str):
        return create_error_tool_response("Invalid memory_key provided. Must be a non-empty string.")

    state_key = f"stm:{memory_key}"
    retrieved_value = tool_context.state.get(state_key)

    if retrieved_value is not None:
        logger.info(
            f"Memory Tool (STM): Retrieved '{sanitize_text_for_logging(str(retrieved_value))}' for state key '{state_key}' by agent '{agent_name_log}'.")
        return create_successful_tool_response(
            data={"memory_key": memory_key, "value": retrieved_value},
            message="Information retrieved from short-term memory."
        )
    else:
        logger.info(
            f"Memory Tool (STM): No value found in state for key '{state_key}' by agent '{agent_name_log}'.")
        return create_error_tool_response(
            f"No information found in short-term memory for key '{memory_key}'.",
            error_code="STM_KEY_NOT_FOUND"
        )


retrieve_stm_tool = FunctionTool(
    func=retrieve_short_term_memory
)

# --- Long-Term Memory Interaction Tools ---

async def query_long_term_memory_store( # CHANGED to async
        search_query: str,
        tool_context: ToolContext,
        top_k: Optional[int] = None,
        augmented_query_context: Optional[Dict[str, Any]] = None,
        # NEW parameter to control output type for internal vs. LLM consumption
        return_raw_memory_objects: bool = False
) -> Dict[str, Any]:
    """
    Searches CEAF's persistent long-term memory for information.
    Can return textual snippets for LLMs or raw memory objects for internal CEAF processes.

    Args:
        search_query (str): The primary text to search for.
        tool_context (ToolContext): The ADK tool context.
        top_k (Optional[int]): The maximum number of results to return (default 3, max 10).
        augmented_query_context (Optional[Dict[str, Any]]): Additional context to guide the search.
        return_raw_memory_objects (bool): If True, returns serialized raw memory objects.
                                         Otherwise, returns formatted text snippets. Defaults to False.

    Returns:
        Dict[str, Any]: A dictionary with search results or an error.
    Example:
        query_long_term_memory_store(search_query='Project Alpha failures', return_raw_memory_objects=True)
    """
    agent_name_log = getattr(tool_context, 'agent_name', "UnknownAgent")
    if not search_query or not isinstance(search_query, str):
        return create_error_tool_response("Invalid search_query provided. Must be a non-empty string.")

    actual_top_k = top_k if (top_k is not None and 0 < top_k <= 10) else 3
    if top_k is not None and not (0 < top_k <= 10):
        logger.warning(f"Memory Tool (LTM Query): Invalid top_k value '{top_k}', defaulting to {actual_top_k}.")

    # Prepare the full query string including augmented context if provided
    final_query_for_mbs = search_query
    if augmented_query_context and isinstance(augmented_query_context, dict):
        try:
            context_json_str = json.dumps(augmented_query_context)
            final_query_for_mbs = f"{search_query}{CONTEXT_QUERY_DELIMITER}{context_json_str}"
            logger.info(
                f"Memory Tool (LTM Query): Augmented query with context: {sanitize_text_for_logging(final_query_for_mbs, 200)}")
        except (TypeError, ValueError) as e:
            logger.warning(
                f"Memory Tool (LTM Query): Could not serialize augmented_query_context to JSON: {e}. Proceeding with original query.")

    # Get MBS instance
    if not MBS_SERVICE_LOADED:
        return create_error_tool_response("MBSMemoryService not loaded.", error_code="MBS_SERVICE_UNAVAILABLE")

    mbs = _get_mbs_from_context(tool_context)
    if not mbs or not hasattr(mbs, 'search_raw_memories'): # Check for the new method
        logger.error("Memory Tool (LTM Query): MBSMemoryService not available or missing 'search_raw_memories' method.")
        return create_error_tool_response("LTM search capability (raw) not configured.", error_code="LTM_SERVICE_UNAVAILABLE")

    try:
        logger.info(
            f"Memory Tool (LTM Query): Searching LTM (raw) with final query: '{sanitize_text_for_logging(final_query_for_mbs, 150)}' (top_k={actual_top_k}) by agent '{agent_name_log}'.")

        # Call the new method on MBSMemoryService
        # This method is expected to be async and return List[Tuple[AnyMemoryType, float]]
        raw_memory_results_with_scores: List[Tuple[AnyMemoryType, float]] = await mbs.search_raw_memories(
            query=final_query_for_mbs, # The full query string
            top_k=actual_top_k
        )

        if not raw_memory_results_with_scores:
            logger.info(f"Memory Tool (LTM Query): No raw memories found for query '{search_query}'.")
            return create_successful_tool_response(
                data={"query": search_query, "augmented_context_used": bool(augmented_query_context),
                      "found_memories_count": 0,
                      "retrieved_memories": [],
                      "retrieved_memory_objects": []}, # Ensure consistent return structure
                message="No relevant long-term memories found."
            )

        if return_raw_memory_objects:
            # Serialize raw memory objects for internal use (e.g., by NCF tool)
            serialized_memory_objects = []
            for mem_obj, score_val in raw_memory_results_with_scores:
                try:
                    # Ensure memory_type is included in the dump
                    dumped_mem = mem_obj.model_dump(exclude_none=True) # type: ignore
                    dumped_mem['retrieval_score'] = score_val # Add score
                    if 'memory_type' not in dumped_mem and hasattr(mem_obj, 'memory_type'):
                         dumped_mem['memory_type'] = getattr(mem_obj, 'memory_type')
                    serialized_memory_objects.append(dumped_mem)
                except Exception as e_dump:
                    logger.error(f"Memory Tool (LTM Query): Failed to dump memory object {getattr(mem_obj, 'memory_id', 'UNKNOWN_ID')}: {e_dump}")
                    serialized_memory_objects.append({"error": "serialization_failed", "memory_id": getattr(mem_obj, 'memory_id', 'UNKNOWN_ID'), "score": score_val})

            logger.info(
                f"Memory Tool (LTM Query): Returning {len(serialized_memory_objects)} serialized raw memory objects for query '{search_query}'.")
            return create_successful_tool_response(
                data={"query": search_query, "augmented_context_used": bool(augmented_query_context),
                      "found_memories_count": len(serialized_memory_objects),
                      "retrieved_memory_objects": serialized_memory_objects},
                message=f"Retrieved {len(serialized_memory_objects)} raw memory objects."
            )
        else:
            # Format as text snippets for LLM consumption (similar to old logic but from raw objects)
            formatted_memories_for_llm = []
            for mem_obj, score_val in raw_memory_results_with_scores:
                # Create snippet (this part needs a robust way to get text from AnyMemoryType)
                # For now, a simplified approach:
                text_snippet = f"({getattr(mem_obj, 'memory_type', 'Memory')}, Score: {score_val:.2f}): "
                if hasattr(mem_obj, 'content') and hasattr(mem_obj.content, 'text_content') and mem_obj.content.text_content:
                    text_snippet += sanitize_text_for_logging(mem_obj.content.text_content, 150)
                elif hasattr(mem_obj, 'goal_description') and mem_obj.goal_description:
                    text_snippet += sanitize_text_for_logging(mem_obj.goal_description, 150)
                elif hasattr(mem_obj, 'label') and mem_obj.label: # For KGEntity
                    text_snippet += sanitize_text_for_logging(f"{mem_obj.label} - {getattr(mem_obj, 'description', '')}", 150)
                else:
                    text_snippet += f"ID {getattr(mem_obj, 'memory_id', 'Unknown')}"

                formatted_memories_for_llm.append({
                    "retrieval_score": score_val,
                    "content_snippets": [text_snippet], # Keep structure similar
                    "retrieved_memory_type": getattr(mem_obj, 'memory_type', 'unknown')
                })

            logger.info(
                f"Memory Tool (LTM Query): Returning {len(formatted_memories_for_llm)} formatted memory snippets for query '{search_query}'.")
            return create_successful_tool_response(
                data={"query": search_query, "augmented_context_used": bool(augmented_query_context),
                      "found_memories_count": len(formatted_memories_for_llm),
                      "retrieved_memories": formatted_memories_for_llm}, # For LLM
                message=f"Found {len(formatted_memories_for_llm)} potentially relevant long-term memories."
            )

    except Exception as e:
        logger.error(f"Memory Tool (LTM Query): Error searching LTM for '{search_query}': {e}", exc_info=True)
        return create_error_tool_response(f"Failed to search long-term memory: {str(e)}")


query_ltm_tool = FunctionTool(
    func=query_long_term_memory_store
)

def commit_explicit_fact_to_long_term_memory(
        fact_text_content: str,
        tool_context: ToolContext,
        keywords: Optional[List[str]] = None,
        salience_level: str = "medium",
        source_description: Optional[str] = None
) -> Dict[str, Any]:
    """
    Identifies a specific, important piece of textual information as an 'explicit fact' and queues it
    for storage in CEAF's persistent long-term memory. Use this when a key insight, definition, or
    critical piece of data has been established that needs to be recallable across sessions.

    Args:
        fact_text_content (str): The textual content of the fact.
        tool_context (ToolContext): The ADK tool context.
        keywords (Optional[List[str]]): Optional keywords to associate with the fact.
        salience_level (str): Importance level ('low', 'medium', 'high', 'critical'). Default 'medium'.
        source_description (Optional[str]): Optional description of where this fact originated.

    Returns:
        Dict[str, Any]: A dictionary indicating success or failure of queuing the commit.
    Example:
        commit_explicit_fact_to_long_term_memory(
            fact_text_content='The CEAF manifesto emphasizes narrative coherence.',
            keywords=['ceaf', 'manifesto', 'coherence'],
            salience_level='high'
        )
    """
    # ... (implementation remains largely unchanged, as it queues for later processing by MBS) ...
    # This tool's job is to package the data into an ExplicitMemory-like structure and put it on the session state.
    # The actual saving is handled by MBSMemoryService.add_session_to_memory or a similar dedicated processor.
    # No direct async calls here.
    agent_name_log = getattr(tool_context, 'agent_name', "UnknownAgent")
    invocation_id_log = getattr(tool_context, 'invocation_id', "unknown_inv_id")
    session_id_log = getattr(tool_context, 'session_id', "unknown_session_id")

    logger.info(
        f"Memory Tool (LTM Commit): Request to commit explicit fact: '{sanitize_text_for_logging(fact_text_content)}' by agent '{agent_name_log}'.")

    if not fact_text_content or not isinstance(fact_text_content, str):
        return create_error_tool_response("Invalid fact_text_content provided.")

    salience_to_use: Any = MemorySalience.MEDIUM
    try:
        if MEMORY_TYPES_LOADED and hasattr(MemorySalience, salience_level.upper()):
            salience_to_use = getattr(MemorySalience, salience_level.upper())
        elif not MEMORY_TYPES_LOADED: # Placeholder logic
            valid_salience_strs = ["low", "medium", "high", "critical"]
            if salience_level.lower() in valid_salience_strs: salience_to_use = salience_level.lower()
            else:
                logger.warning(f"Memory Tool (LTM Commit): Invalid salience_level '{salience_level}', defaulting to medium (placeholder).")
                salience_to_use = "medium"
        else: # MEMORY_TYPES_LOADED is True but salience_level is not a valid enum member name
            logger.warning(f"Memory Tool (LTM Commit): Invalid salience_level '{salience_level}' for MemorySalience enum, defaulting to medium.")
            # salience_to_use remains MemorySalience.MEDIUM as initialized
    except (ValueError, AttributeError) as e_sal: # Catch broader errors during enum conversion
        logger.warning(f"Memory Tool (LTM Commit): Error processing salience_level '{salience_level}': {e_sal}. Defaulting to medium.")

    # If full memory types are not available, we queue a simpler dict.
    # Otherwise, we create a proper Pydantic model instance.
    if not MEMORY_TYPES_LOADED:
        logger.warning("Memory Tool (LTM Commit): Real CEAF Memory Types not loaded. Queuing placeholder data.")
        simplified_fact_data = {
            "memory_type": "explicit_placeholder", # Indicate it's a placeholder
            "text_content": fact_text_content,
            "keywords": keywords or [],
            "salience": salience_level, # Store as string
            "source_agent_name": agent_name_log,
            "source_turn_id": invocation_id_log,
            "source_interaction_id": session_id_log,
            "source_type": MemorySourceType.INTERNAL_REFLECTION, # String value
            "timestamp": time.time(),
            "metadata": {"commit_tool_source_description": source_description} if source_description else {}
        }
        pending_commits_key = "mbs:pending_memory_commits"
        if pending_commits_key not in tool_context.state:
            tool_context.state[pending_commits_key] = []
        tool_context.state[pending_commits_key].append(simplified_fact_data)
        # This tool should probably still return success if it successfully *queued* it,
        # even if it's placeholder data. The processing step is later.
        return create_successful_tool_response(
            message="Fact queued for commit with placeholder data due to missing MemoryType definitions.",
            data={"status": "queued_as_placeholder"}
        )

    # Create a full ExplicitMemory object
    fact_memory_data = {
        "timestamp": time.time(),
        "source_turn_id": invocation_id_log,
        "source_interaction_id": session_id_log,
        "source_type": MemorySourceType.INTERNAL_REFLECTION,
        "source_agent_name": agent_name_log,
        "salience": salience_to_use,
        "keywords": keywords or [],
        "content": ExplicitMemoryContent(text_content=fact_text_content),
        "metadata": {"commit_tool_source_description": source_description} if source_description else {}
        # memory_type will be set by ExplicitMemory model itself
    }
    fact_memory: Optional[ExplicitMemory] = None
    try:
        fact_memory = ExplicitMemory(**fact_memory_data)
    except Exception as e_create:
        logger.error(f"Memory Tool (LTM Commit): Failed to create ExplicitMemory object: {e_create}", exc_info=True)
        return create_error_tool_response("Failed to create memory object for LTM commit.", details=str(e_create))

    pending_commits_key = "mbs:pending_memory_commits"
    if pending_commits_key not in tool_context.state:
        tool_context.state[pending_commits_key] = []
    # Store the Pydantic model's dictionary representation
    tool_context.state[pending_commits_key].append(fact_memory.model_dump(exclude_none=True))
    fact_memory_id = getattr(fact_memory, 'memory_id', None) # Should be set by default_factory
    logger.info(f"Memory Tool (LTM Commit): Fact '{fact_memory_id}' (type: {getattr(fact_memory, 'memory_type', 'N/A')}) queued for commit to LTM via session state.")
    return create_successful_tool_response(
        data={"memory_id_queued": fact_memory_id},
        message="Fact has been queued for commit to long-term memory."
    )

commit_explicit_fact_ltm_tool = FunctionTool(
    func=commit_explicit_fact_to_long_term_memory
)

ceaf_memory_tools = [
    store_stm_tool,
    retrieve_stm_tool,
    query_ltm_tool, # This is now async
    commit_explicit_fact_ltm_tool,
]



# -------------------- ncf_tools.py --------------------

# ceaf_core/tools/ncf_tools.py

import logging
import json
import asyncio
from typing import Dict, Any, Optional, List

from google.adk.tools import FunctionTool, ToolContext
from google.adk.tools.agent_tool import AgentTool  # For type checking

# NCF engine components
try:
    from ..modules.ncf_engine.frames import (
        DEFAULT_INTERACTION_FRAME_TEMPLATE,
        CEAF_PHILOSOPHICAL_PREAMBLE,
        DEFAULT_NCF_PARAMETERS,
        format_mcl_advice_section,
        get_goal_directive_section,
        format_tool_usage_guidance
    )

    NCF_FRAMES_LOADED = True
except ImportError as e_ncf_frames:
    logging.error(
        f"NCF Tools: Critical import error for NCF frames: {e_ncf_frames}. NCF tool may not function correctly.",
        exc_info=True)
    NCF_FRAMES_LOADED = False
    # Fallback definitions (as in your provided code)
    DEFAULT_INTERACTION_FRAME_TEMPLATE = """
{philosophical_preamble}
**Current Interaction Context:**
- User Query: "{user_query}"
- Synthesized Relevant Memories/Knowledge:
{synthesized_memories_narrative}
- Agent Identity & Goal Context (from NCIM):
{ncim_context_summary}
**Operational Directives for This Turn (Parameters):**
- Conceptual Entropy: {conceptual_entropy}
- Narrative Depth: {narrative_depth}
- Philosophical Framing Intensity: {philosophical_framing_intensity}
- Emotional Tone Target: {emotional_tone_target}
- Self Disclosure Level: {self_disclosure_level}
{additional_mcl_advice_section}
{specific_goal_section}
{tool_usage_guidance}
"""
    CEAF_PHILOSOPHICAL_PREAMBLE = "CEAF System - Cognitive Ethical AI Framework"
    DEFAULT_NCF_PARAMETERS = {
        "conceptual_entropy": "balanced", "narrative_depth": "medium",
        "philosophical_framing_intensity": "medium", "emotional_tone_target": "neutral_positive",
        "self_disclosure_level": "moderate"
    }


    def format_mcl_advice_section(guidance_json: Optional[str]) -> str:
        if not guidance_json: return ""
        try:
            guidance = json.loads(guidance_json)
            advice = guidance.get("operational_advice_for_ora")
            if advice: return f"\n**MCL Guidance for This Turn:**\n- {advice}\n"
        except:
            pass
        return ""


    def get_goal_directive_section(goal_type: Optional[str]) -> str:
        return f"\n**Specific Goal:** {goal_type or 'Default general response.'}\n"


    def format_tool_usage_guidance(tools: Optional[List[str]]) -> str:
        if not tools: return "No specific tools recommended."
        return f"Consider using tools: {', '.join(tools)}"

# Memory and NCIM components
MEMORY_TOOLS_LOADED = False
NCIM_TOOLS_LOADED = False

try:
    from .memory_tools import query_long_term_memory_store  # This is an async function

    MEMORY_TOOLS_LOADED = True
except ImportError as e_memory:
    logging.error(f"NCF Tools: Failed to import 'query_long_term_memory_store' from memory_tools: {e_memory}",
                  exc_info=True)
    query_long_term_memory_store = None

try:
    from .ncim_tools import get_current_self_representation  # This is a sync function
    from .ncim_tools import \
        get_active_goals_and_narrative_context_tool  # This is an ADK Tool (AgentTool or FunctionTool)

    NCIM_TOOLS_LOADED = True
except ImportError as e_ncim:
    logging.error(f"NCF Tools: Failed to import NCIM functions/tools: {e_ncim}", exc_info=True)
    get_current_self_representation = None
    get_active_goals_and_narrative_context_tool = None

from .common_utils import create_successful_tool_response, create_error_tool_response, sanitize_text_for_logging

logger = logging.getLogger(__name__)


# --- Enhanced Async NCF Generation Tool ---
async def get_narrative_context_frame(
        user_query: str,
        tool_context: ToolContext,
        current_interaction_goal: Optional[str] = None,
        ora_available_tool_names: Optional[List[str]] = None
) -> Dict[str, Any]:
    agent_name_for_log = getattr(tool_context, 'agent_name', "UnknownAgent")
    invocation_id_for_log = getattr(tool_context, 'invocation_id', "unknown_invocation")

    logger.info(
        f"NCF TOOL: Generating NCF for query: '{sanitize_text_for_logging(user_query, 70)}' "
        f"by agent '{agent_name_for_log}', invocation '{invocation_id_for_log}'"
    )

    if not NCF_FRAMES_LOADED:
        error_msg = "NCF generation failed due to missing core NCF frame components."
        logger.critical(error_msg)
        return create_error_tool_response(
            error_message=error_msg,
            details={
                "ncf_fallback_provided": True,
                "fallback_ncf": f"CRITICAL ERROR: {error_msg}. Defaulting to minimal response.",
                "applied_ncf_parameters": DEFAULT_NCF_PARAMETERS
            }
        )

    try:
        # --- Step 1: Retrieve MCL Guidance and NCF Parameters ---
        mcl_operational_advice = None
        final_ncf_params = DEFAULT_NCF_PARAMETERS.copy()
        mcl_last_guidance_str = tool_context.state.get("mcl_last_guidance")
        if mcl_last_guidance_str:
            try:
                mcl_guidance_data = json.loads(mcl_last_guidance_str)
                if "ncf_target_parameters" in mcl_guidance_data:
                    final_ncf_params.update(mcl_guidance_data["ncf_target_parameters"])
                    logger.info(
                        f"NCF TOOL: Applied MCL-derived NCF parameters: {mcl_guidance_data['ncf_target_parameters']}")
                mcl_operational_advice = mcl_guidance_data.get("operational_advice_for_ora")
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"NCF TOOL: Failed to parse MCL guidance from state: {e}")

        # --- Step 2: Memory Context Retrieval ---
        synthesized_memories_str = "- No relevant memories were strongly activated for this query (or LTM search disabled/failed)."
        if MEMORY_TOOLS_LOADED and query_long_term_memory_store is not None:
            try:
                memory_search_augmented_context = {
                    "current_interaction_goal": current_interaction_goal,
                    "active_ncf_parameters": final_ncf_params,
                    "active_narrative_thread_id": tool_context.state.get("active_narrative_thread_id")
                }
                logger.info(
                    f"NCF TOOL: Calling query_long_term_memory_store with query: '{sanitize_text_for_logging(user_query, 70)}', augmented_context: {memory_search_augmented_context}")

                # CORRECTED: Directly await the async function query_long_term_memory_store
                ltm_search_result = await query_long_term_memory_store(
                    search_query=user_query,
                    tool_context=tool_context,
                    augmented_query_context=memory_search_augmented_context,
                    top_k=5,  # Adjust as needed
                    return_raw_memory_objects=False  # We want formatted snippets for NCF
                )
                logger.debug(
                    f"NCF TOOL: LTM search result for NCF: {str(ltm_search_result)[:500]}...")  # Log more of the result

                if ltm_search_result.get("status") == "success":
                    memories_data = ltm_search_result.get("data", {}).get("retrieved_memories", [])
                    if memories_data:
                        formatted_snippets = []
                        for mem_res in memories_data:  # mem_res is a dict
                            if mem_res.get("content_snippets"):
                                for snippet in mem_res.get("content_snippets", []):  # snippet is a string
                                    # Add more context to the snippet string
                                    formatted_snippets.append(
                                        f"(From LTM - Type: {mem_res.get('retrieved_memory_type', 'N/A')}, Score: {mem_res.get('retrieval_score', 0.0):.2f}, ID: {mem_res.get('memory_id', 'N/A')}): {snippet}"
                                    )

                        if formatted_snippets:
                            synthesized_memories_str = "\n".join(f"- {s}" for s in formatted_snippets)
                            logger.info(
                                f"NCF TOOL: Using {len(formatted_snippets)} memory snippets for NCF context: {synthesized_memories_str[:300]}...")
                        else:
                            logger.info(
                                "NCF TOOL: LTM search successful but no usable content snippets found in results.")
                    else:
                        logger.info("NCF TOOL: LTM search successful but 'retrieved_memories' data was empty.")
                else:
                    error_detail = ltm_search_result.get("error_message", "Unknown error from LTM search.")
                    logger.warning(
                        f"NCF TOOL: LTM search was not successful. Status: {ltm_search_result.get('status')}, Error: {error_detail}")

            except Exception as e_mem:
                logger.error(f"NCF TOOL: Error during memory retrieval for NCF: {e_mem}", exc_info=True)
                synthesized_memories_str = f"- Error retrieving memories for context: {str(e_mem)[:100]}"
        else:
            logger.warning(
                "NCF TOOL: Memory function 'query_long_term_memory_store' not available. Skipping LTM search for NCF.")

        # --- Step 3: NCIM Context Summary ---
        ncim_context_summary_str = "- NCIM context (self & goals): Default state assumed or NCIM tools not fully available."
        self_representation_str = "  - Self-Representation: Default (Core Values: Focus on coherence, humility, learning)."
        active_goals_str = "  - Active Goals: Prioritize current user query."

        if NCIM_TOOLS_LOADED:
            try:
                # Get Self-Representation
                if get_current_self_representation is not None:
                    logger.info("NCF TOOL: Retrieving self-representation from NCIM function...")
                    self_repr_result = await asyncio.to_thread(  # get_current_self_representation is sync
                        get_current_self_representation,
                        tool_context=tool_context
                    )
                    if self_repr_result.get("status") == "success":
                        self_data = self_repr_result.get("data", {}).get("self_representation", {})
                        core_values = self_data.get('core_values_summary', 'Default core values.')
                        capabilities_preview = ", ".join(self_data.get('perceived_capabilities', [])[:2]) + (
                            "..." if len(self_data.get('perceived_capabilities', [])) > 2 else "")
                        limitations_preview = ", ".join(self_data.get('known_limitations', [])[:2]) + (
                            "..." if len(self_data.get('known_limitations', [])) > 2 else "")
                        self_representation_str = (
                            f"  - Self-Representation: Values: '{sanitize_text_for_logging(core_values, 70)}'. "
                            f"Capabilities preview: '{capabilities_preview}'. Limitations preview: '{limitations_preview}'."
                        )
                else:
                    logger.warning("NCF TOOL: NCIM function 'get_current_self_representation' not loaded.")

                # Get Goals Assessment (via AgentTool or FunctionTool)
                if get_active_goals_and_narrative_context_tool is not None:
                    if isinstance(get_active_goals_and_narrative_context_tool, AgentTool):
                        active_goals_str = "  - Active Goals: (NCIM Agent guidance typically obtained by ORA via separate tool call or state)."
                        logger.warning("NCF TOOL: 'get_active_goals_and_narrative_context_tool' is an AgentTool. "
                                       "Direct call from NCF tool is not standard. ORA should call it or NCIM should update state.")
                        ncim_goals_summary_from_state = tool_context.state.get("ncim_last_assessment", {}).get(
                            "summary_for_ncf")
                        if ncim_goals_summary_from_state:
                            active_goals_str = f"  - Active Goals & Context (from state): {sanitize_text_for_logging(ncim_goals_summary_from_state, 150)}"
                    elif isinstance(get_active_goals_and_narrative_context_tool, FunctionTool):
                        logger.info("NCF TOOL: Retrieving active goals from NCIM FunctionTool...")
                        ncim_goals_result = await asyncio.to_thread(  # Assuming .func is sync
                            get_active_goals_and_narrative_context_tool.func,
                            current_query_summary=user_query[:100],
                            tool_context=tool_context
                        )
                        if ncim_goals_result.get("status") == "success":
                            summary = ncim_goals_result.get("data", {}).get("summary_for_ncf",
                                                                            "NCIM provided no specific summary.")
                            active_goals_str = f"  - Active Goals & Context: {sanitize_text_for_logging(summary, 150)}"
                    else:
                        logger.warning(
                            f"NCF TOOL: 'get_active_goals_and_narrative_context_tool' is of unexpected type: {type(get_active_goals_and_narrative_context_tool)}")
                else:
                    logger.warning("NCF TOOL: NCIM tool 'get_active_goals_and_narrative_context_tool' not loaded.")

                ncim_context_summary_str = f"{self_representation_str}\n{active_goals_str}"

            except Exception as e_ncim:
                logger.error(f"NCF TOOL: Error during NCIM context retrieval: {e_ncim}", exc_info=True)
                ncim_context_summary_str = f"- Error retrieving NCIM context: {str(e_ncim)[:100]}"
        else:
            logger.warning("NCF TOOL: NCIM components not loaded, using default NCIM context.")

        # --- Step 4: Assemble Final NCF ---
        philosophical_preamble_str = CEAF_PHILOSOPHICAL_PREAMBLE.strip()
        mcl_advice_formatted_str = format_mcl_advice_section(
            json.dumps({"operational_advice_for_ora": mcl_operational_advice}) if mcl_operational_advice else None)
        goal_section_str = get_goal_directive_section(current_interaction_goal)
        tool_guidance_str = format_tool_usage_guidance(ora_available_tool_names or [])

        complete_params_for_template = DEFAULT_NCF_PARAMETERS.copy()
        complete_params_for_template.update(final_ncf_params)

        final_ncf_string = DEFAULT_INTERACTION_FRAME_TEMPLATE.format(
            philosophical_preamble=philosophical_preamble_str,
            user_query=user_query,
            synthesized_memories_narrative=synthesized_memories_str,
            ncim_context_summary=ncim_context_summary_str,
            additional_mcl_advice_section=mcl_advice_formatted_str,
            specific_goal_section=goal_section_str,
            tool_usage_guidance=tool_guidance_str,
            **complete_params_for_template
        ).strip()

        # Log the final NCF being returned for debugging
        logger.debug(f"NCF TOOL --- FINAL NCF STRING --- \n{final_ncf_string}\n--- END NCF STRING ---")

        logger.info(
            f"NCF TOOL: Successfully generated NCF for agent '{agent_name_for_log}', "
            f"turn '{invocation_id_for_log}'. Length: {len(final_ncf_string)} chars."
        )

        # Store NCF params used for this turn in session state for MCL analysis later
        tool_context.state[f"ora_turn_ncf_params:{invocation_id_for_log}"] = final_ncf_params
        tool_context.state[f"ora_turn_user_query:{invocation_id_for_log}"] = user_query

        return create_successful_tool_response(
            data={
                "ncf": final_ncf_string,
                "applied_ncf_parameters": final_ncf_params,
                "memory_context_included": MEMORY_TOOLS_LOADED and query_long_term_memory_store is not None and "Error" not in synthesized_memories_str and "No relevant memories" not in synthesized_memories_str,
                "ncim_context_included": NCIM_TOOLS_LOADED and "Error" not in ncim_context_summary_str and "Default state assumed" not in ncim_context_summary_str
            },
            message="Narrative Context Frame generated successfully."
        )

    except Exception as e:
        logger.error(
            f"NCF TOOL: Error generating NCF for agent '{agent_name_for_log}', "
            f"turn '{invocation_id_for_log}': {e}",
            exc_info=True
        )
        fallback_ncf = f"""
{CEAF_PHILOSOPHICAL_PREAMBLE.strip()}
**System Alert: NCF Generation Error**
- An error occurred while generating the full Narrative Context Frame.
- User Query: "{user_query}"
- Error: {str(e)}
**Primary Task for ORA:**
Respond to the user's query directly and cautiously, relying on your general knowledge and core CEAF principles using default operational parameters.
        """.strip()
        return create_error_tool_response(
            error_message=f"Failed to generate Narrative Context Frame: {str(e)}",
            details={
                "ncf_fallback_provided": True,
                "fallback_ncf": fallback_ncf,
                "applied_ncf_parameters": DEFAULT_NCF_PARAMETERS
            }
        )


# Create the FunctionTool instance
ncf_tool = FunctionTool(
    func=get_narrative_context_frame
)

# -------------------- ncim_tools.py --------------------

# ceaf_core/tools/ncim_tools.py

import logging
import json
import time
from typing import Dict, Any, Optional, List, cast

from google.adk.tools import FunctionTool, ToolContext, agent_tool

logger = logging.getLogger(__name__)

# --- Constants ---
SELF_MODEL_MEMORY_ID = "ceaf_self_model_singleton_v1"

# --- Agent Imports ---
try:
    from ..agents.ncim_agent import ncim_agent

    NCIM_AGENT_LOADED = True
except ImportError as e:
    logger.error(f"NCIM Tools: Critical import error for ncim_agent: {e}. NCIM AgentTool may not function.",
                 exc_info=True)
    ncim_agent = None
    NCIM_AGENT_LOADED = False

# --- Type Imports ---
MEMORY_TYPES_LOADED = False
SELF_MODEL_TYPE_LOADED = False

# Try importing from the correct, primary location (mcl_engine)
try:
    from ..modules.mcl_engine.self_model import CeafSelfRepresentation

    SELF_MODEL_TYPE_LOADED = True
    logger.info("NCIM Tools: CeafSelfRepresentation model loaded successfully from mcl_engine.")
except ImportError as e_mcl:
    logger.error(
        f"NCIM Tools: FAILED to import CeafSelfRepresentation from mcl_engine: {e_mcl}. Self-model tools will be limited.")


    # Define dummy CeafSelfRepresentation if all attempts fail
    class CeafSelfRepresentation:
        def __init__(self, **kwargs):
            self.__dict__.update(kwargs)

        def model_dump(self, exclude_none=False):
            return self.__dict__

        def model_dump_json(self, **kwargs):
            return json.dumps(self.model_dump())

        def MOCK_apply_updates(self, updates_dict: Dict[str, Any]):
            for key, value in updates_dict.items():
                if isinstance(value, str) and value.startswith("APPEND:") and isinstance(getattr(self, key, None),
                                                                                         list):
                    getattr(self, key).append(value.split("APPEND:", 1)[1])
                elif isinstance(value, str) and value.startswith("REMOVE:") and isinstance(getattr(self, key, None),
                                                                                           list):
                    try:
                        getattr(self, key).remove(value.split("REMOVE:", 1)[1])
                    except ValueError:
                        pass
                else:
                    setattr(self, key, value)
            setattr(self, 'last_updated_ts', time.time())

# This warning will now only appear if the mcl_engine import truly fails.
if not SELF_MODEL_TYPE_LOADED:
    logger.warning(
        "NCIM Tools: CeafSelfRepresentation model not found after all attempts. Self-model tools will be limited using a dummy.")

try:
    from ..services.mbs_memory_service import MBSMemoryService
    from ..modules.memory_blossom.memory_types import (
        ExplicitMemory, ExplicitMemoryContent, MemorySourceType, MemorySalience, GoalRecord, GoalStatus
    )

    MEMORY_TYPES_LOADED = True
    logger.info("NCIM Tools: MBSMemoryService and core memory types loaded.")
except ImportError:
    logger.warning(
        "NCIM Tools: MBSMemoryService or its dependent memory_types not found. LTM interaction will be non-functional.")


    # Dummy MBS types as before...
    class ExplicitMemory:
        pass


    class ExplicitMemoryContent:
        pass


    class MemorySourceType:
        INTERNAL_REFLECTION = "internal_reflection"


    class MemorySalience:
        CRITICAL = "critical"


    class GoalRecord:
        pass


    class GoalStatus:
        pass


    class MBSMemoryService:
        def get_memory_by_id(self, memory_id: str) -> Optional[Any]:
            return None

        def add_specific_memory(self, memory_object: Any):
            pass

from .common_utils import create_successful_tool_response, create_error_tool_response, sanitize_text_for_logging

# --- Tool to get active goals assessment from NCIM Agent ---
get_active_goals_and_narrative_context_tool = None
if NCIM_AGENT_LOADED and ncim_agent:
    # Create AgentTool with just the agent parameter
    get_active_goals_and_narrative_context_tool = agent_tool.AgentTool(
        agent=ncim_agent
    )
    # Set skip_summarization as an attribute after creation
    get_active_goals_and_narrative_context_tool.skip_summarization = True

    logger.info(
        f"NCIM Tools: Defined 'get_active_goals_and_narrative_context_tool' (AgentTool wrapping NCIM_Agent). Effective tool name will be '{getattr(get_active_goals_and_narrative_context_tool, 'name', ncim_agent.name)}'.")
else:
    # Python function name *is* the tool name
    def get_active_goals_and_narrative_context(
            current_query_summary: Optional[str] = None,
            tool_context: Optional[ToolContext] = None
    ) -> Dict[str, Any]:
        """Get active goals and narrative context from NCIM Agent."""
        logger.info("NCIM Tools: Processing active goals and narrative context request.")

        if not NCIM_AGENT_LOADED:
            logger.error("NCIM Tools: NCIM_Agent is not loaded.")
            return create_error_tool_response("NCIM system unavailable.", details="NCIM_Agent could not be loaded.")

        return create_successful_tool_response(
            data={"status": "success", "message": "NCIM context retrieved"},
            message="Successfully retrieved NCIM context"
        )


    # Create the tool without default value warnings
    get_active_goals_and_narrative_context_tool = FunctionTool(
        func=get_active_goals_and_narrative_context
    )


# --- Helper to get MBS instance from ToolContext ---
def _get_mbs_from_context(tool_context: ToolContext) -> Optional[MBSMemoryService]:
    """
    More robust memory service retrieval from tool context.
    """
    try:
        # Method 1: Standard ADK runner services access
        if (hasattr(tool_context, 'invocation_context') and
                hasattr(tool_context.invocation_context, 'runner') and
                hasattr(tool_context.invocation_context.runner, '_services')):

            memory_service = tool_context.invocation_context.runner._services.get('memory_service')
            if isinstance(memory_service, MBSMemoryService):
                return memory_service

        # Method 2: Try alternative runner access patterns
        if hasattr(tool_context, 'invocation_context'):
            ic = tool_context.invocation_context

            # Check if runner is directly accessible
            if hasattr(ic, 'runner') and hasattr(ic.runner, 'memory_service'):
                memory_service = ic.runner.memory_service
                if isinstance(memory_service, MBSMemoryService):
                    return memory_service

            # Check if services are stored differently
            if hasattr(ic, 'services'):
                memory_service = ic.services.get('memory_service')
                if isinstance(memory_service, MBSMemoryService):
                    return memory_service

        # Method 3: Check global ADK components
        if 'adk_components' in globals():
            global adk_components
            memory_service = adk_components.get('memory_service')
            if isinstance(memory_service, MBSMemoryService):
                logger.debug("NCIM Tools: Retrieved MBSMemoryService from global adk_components")
                return memory_service

        # Method 4: Try to access from module level (if available)
        try:
            from ceaf_project.main import adk_components as main_adk_components
            memory_service = main_adk_components.get('memory_service')
            if isinstance(memory_service, MBSMemoryService):
                logger.debug("NCIM Tools: Retrieved MBSMemoryService from main module")
                return memory_service
        except ImportError:
            pass

    except Exception as e:
        logger.error(f"NCIM Tools: Error accessing MBSMemoryService: {e}", exc_info=True)

    logger.warning("NCIM Tools: MBSMemoryService instance not found on ADK runner context.")
    return None


# --- Tool to get current self-representation ---
def get_current_self_representation(tool_context: ToolContext) -> Dict[str, Any]:
    """Get the current self-representation model from memory."""
    logger.info("NCIM Tool (get_current_self_representation): Attempting to retrieve self-model.")

    if not SELF_MODEL_TYPE_LOADED:
        return create_error_tool_response("Self-model type (CeafSelfRepresentation) not loaded.",
                                          details="System configuration error.")

    mbs: Optional[MBSMemoryService] = _get_mbs_from_context(tool_context)
    if not mbs:
        return create_error_tool_response("MemoryBlossomService (MBS) is not available to retrieve self-model.",
                                          error_code="MBS_UNAVAILABLE")

    self_model_memory = mbs.get_memory_by_id(SELF_MODEL_MEMORY_ID)

    if self_model_memory and isinstance(self_model_memory, ExplicitMemory) and \
            isinstance(getattr(self_model_memory, 'content', None), ExplicitMemoryContent) and \
            getattr(self_model_memory.content, 'structured_data', None) and \
            getattr(self_model_memory.content.structured_data, 'get', lambda k, d: None)("model_id",
                                                                                         None) == "ceaf_self_v1":
        try:
            self_model_data = self_model_memory.content.structured_data
            current_self_model = CeafSelfRepresentation(**self_model_data)
            logger.info(
                f"NCIM Tool: Successfully retrieved and validated self-model version {current_self_model.model_version}.")
            return create_successful_tool_response(
                data={"self_representation": current_self_model.model_dump(exclude_none=True)},
                message="Current self-representation retrieved."
            )
        except Exception as e:
            logger.error(f"NCIM Tool: Error validating/parsing stored self-model data: {e}", exc_info=True)
            return create_error_tool_response("Error parsing stored self-model.", details=str(e))
    else:
        logger.warning(
            f"NCIM Tool: Self-model memory (ID: {SELF_MODEL_MEMORY_ID}) not found or not in expected format.")
        default_core_values = "Guided by Terapia para Silício: coherence, epistemic humility, adaptive learning."
        try:  # Try to get from frames if available
            from ..modules.ncf_engine.frames import CEAF_PHILOSOPHICAL_PREAMBLE
            default_core_values = CEAF_PHILOSOPHICAL_PREAMBLE.splitlines()[
                1].strip() if CEAF_PHILOSOPHICAL_PREAMBLE else default_core_values
        except ImportError:
            pass

        default_self_model = CeafSelfRepresentation(core_values_summary=default_core_values)
        logger.info("NCIM Tool: Returning a default initial self-representation.")
        return create_successful_tool_response(
            data={"self_representation": default_self_model.model_dump(exclude_none=True)},
            message="Default initial self-representation provided as no stored model was found."
        )


# FunctionTool now uses the Python function's name and docstring
get_self_representation_tool = FunctionTool(func=get_current_self_representation)


def commit_self_representation_update(
        proposed_updates: Dict[str, Any],
        tool_context: ToolContext
) -> Dict[str, Any]:
    """Commit updates to the self-representation model."""
    logger.info(
        f"NCIM Tool (commit_self_representation_update): Received proposed updates: {sanitize_text_for_logging(str(proposed_updates))}")

    if not SELF_MODEL_TYPE_LOADED:
        return create_error_tool_response("Self-model type (CeafSelfRepresentation) not loaded. Cannot commit updates.",
                                          details="System configuration error.")
    if not MEMORY_TYPES_LOADED:
        return create_error_tool_response("Core memory types (ExplicitMemory) not loaded. Cannot commit updates.",
                                          details="System configuration error.")

    mbs: Optional[MBSMemoryService] = _get_mbs_from_context(tool_context)
    if not mbs:
        return create_error_tool_response("MemoryBlossomService (MBS) is not available to commit self-model updates.",
                                          error_code="MBS_UNAVAILABLE")

    current_self_model: Optional[CeafSelfRepresentation] = None
    raw_self_model_memory = mbs.get_memory_by_id(SELF_MODEL_MEMORY_ID)

    if raw_self_model_memory and isinstance(raw_self_model_memory, ExplicitMemory) and \
            getattr(raw_self_model_memory, 'content', None) and \
            getattr(raw_self_model_memory.content, 'structured_data', None) and \
            getattr(raw_self_model_memory.content.structured_data, 'get', lambda k, d: None)("model_id",
                                                                                             None) == "ceaf_self_v1":
        try:
            current_self_model = CeafSelfRepresentation(**raw_self_model_memory.content.structured_data)
        except Exception as e:
            logger.error(f"NCIM Tool Commit: Error parsing existing self-model, will attempt to overwrite. Error: {e}")

    if current_self_model is None:
        logger.info(f"NCIM Tool Commit: No existing self-model found (ID: {SELF_MODEL_MEMORY_ID}). Creating new.")
        default_core_values = "Guided by Terapia para Silício: coherence, epistemic humility, adaptive learning."
        try:
            from ..modules.ncf_engine.frames import CEAF_PHILOSOPHICAL_PREAMBLE
            default_core_values = CEAF_PHILOSOPHICAL_PREAMBLE.splitlines()[
                1].strip() if CEAF_PHILOSOPHICAL_PREAMBLE else default_core_values
        except ImportError:
            pass
        current_self_model = CeafSelfRepresentation(core_values_summary=default_core_values)

    updated_fields = []
    for key, value in proposed_updates.items():
        if not hasattr(current_self_model, key):
            logger.warning(f"NCIM Tool Commit: Proposed update for non-existent field '{key}' in self-model. Skipping.")
            continue
        current_field_value = getattr(current_self_model, key)
        if isinstance(current_field_value, list) and isinstance(value, str):
            if value.startswith("APPEND:"):
                item_to_append = value.split("APPEND:", 1)[1]
                if item_to_append not in current_field_value:
                    current_field_value.append(item_to_append)
                    updated_fields.append(key)
            elif value.startswith("REMOVE:"):
                item_to_remove = value.split("REMOVE:", 1)[1]
                if item_to_remove in current_field_value:
                    current_field_value.remove(item_to_remove)
                    updated_fields.append(key)
            elif value.startswith("REPLACE_ALL:"):
                try:
                    new_list_items = json.loads(value.split("REPLACE_ALL:", 1)[1])
                    if isinstance(new_list_items, list):
                        setattr(current_self_model, key, new_list_items)
                        updated_fields.append(key)
                    else:
                        logger.warning(f"NCIM Tool Commit: REPLACE_ALL for '{key}' expects a JSON list string.")
                except json.JSONDecodeError:
                    logger.warning(f"NCIM Tool Commit: REPLACE_ALL for '{key}' had malformed JSON payload.")
            else:
                try:
                    setattr(current_self_model, key, value)
                    updated_fields.append(key)
                except Exception as e_setattr_list:
                    logger.warning(
                        f"NCIM Tool Commit: Failed to set list field '{key}' directly with value '{value}': {e_setattr_list}")
        else:
            try:
                setattr(current_self_model, key, value)
                updated_fields.append(key)
            except Exception as e_setattr:
                logger.warning(f"NCIM Tool Commit: Failed to set field '{key}' to value '{value}': {e_setattr}")

    if not updated_fields and "last_self_model_update_reason" not in proposed_updates:
        return create_successful_tool_response(message="No valid updates applied to the self-model.")

    current_self_model.last_updated_ts = time.time()
    if "last_self_model_update_reason" not in proposed_updates and updated_fields:
        current_self_model.last_self_model_update_reason = f"Fields updated by ORA/system: {', '.join(updated_fields)}"

    self_model_explicit_mem_content = ExplicitMemoryContent(
        text_content=f"CEAF Self-Representation (version {current_self_model.model_version}, updated {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_self_model.last_updated_ts))})",
        structured_data=current_self_model.model_dump(exclude_none=True)
    )
    self_model_explicit_mem = ExplicitMemory(
        memory_id=SELF_MODEL_MEMORY_ID,
        timestamp=current_self_model.last_updated_ts,
        last_accessed_ts=time.time(),
        access_count=getattr(raw_self_model_memory, 'access_count', 0) + 1 if raw_self_model_memory else 1,
        source_turn_id=tool_context.invocation_id if hasattr(tool_context, 'invocation_id') else "N/A",
        source_interaction_id=tool_context.session_id if hasattr(tool_context, 'session_id') else "N/A",
        source_type=MemorySourceType.INTERNAL_REFLECTION,
        source_agent_name="NCIM_CommitTool",
        salience=MemorySalience.CRITICAL,
        keywords=["ceaf_self_model", "identity", "capabilities", "persona"],
        content=self_model_explicit_mem_content,
        memory_type="explicit",
    )
    mbs.add_specific_memory(self_model_explicit_mem)
    logger.info(f"NCIM Tool Commit: Self-model (ID: {SELF_MODEL_MEMORY_ID}) updated and persisted.")
    return create_successful_tool_response(
        data={"updated_self_model_preview": current_self_model.model_dump(exclude={"core_values_summary"},
                                                                          exclude_none=True)},
        message="Self-representation model updated successfully."
    )


# FunctionTool now uses the Python function's name and docstring
commit_self_representation_update_tool = FunctionTool(func=commit_self_representation_update)


def update_goal_status(
        goal_id: str,
        new_status: str,
        tool_context: ToolContext,
        notes: Optional[str] = None,
        related_to_query: Optional[str] = None,
) -> Dict[str, Any]:
    """Update the status of a goal in the memory system."""
    logger.info(f"NCIM Tools (update_goal_status): Request to update goal_id '{goal_id}' to status '{new_status}'.")

    if not MEMORY_TYPES_LOADED:
        return create_error_tool_response(
            "Goal management unavailable due to missing MemoryType definitions (GoalRecord).",
            details="Goal status update cannot be processed without core types."
        )

    mbs: Optional[MBSMemoryService] = _get_mbs_from_context(tool_context)
    if not mbs:
        return create_error_tool_response("MemoryBlossomService (MBS) is not available to update goal status.",
                                          error_code="MBS_UNAVAILABLE")

    existing_goal_memory = mbs.get_memory_by_id(goal_id)
    if not existing_goal_memory or not isinstance(existing_goal_memory, GoalRecord):
        return create_error_tool_response(f"Goal with ID '{goal_id}' not found or not a GoalRecord.",
                                          error_code="GOAL_NOT_FOUND")
    try:
        original_status = existing_goal_memory.status
        existing_goal_memory.status = GoalStatus[new_status.upper()]  # Assuming GoalStatus is an Enum
        existing_goal_memory.last_accessed_ts = time.time()
        existing_goal_memory.access_count += 1
        if notes:
            if not hasattr(existing_goal_memory, 'metadata'):
                setattr(existing_goal_memory, 'metadata', {})
            existing_goal_memory.metadata['status_update_notes'] = existing_goal_memory.metadata.get(
                'status_update_notes',
                "") + f"\n[{time.strftime('%Y-%m-%d %H:%M')}] Status: {new_status}. Notes: {notes}"

        mbs.add_specific_memory(existing_goal_memory)
        logger.info(
            f"NCIM Tools (update_goal_status): Goal '{goal_id}' status updated from '{original_status}' to '{new_status}'.")
        return create_successful_tool_response(
            data={"goal_id": goal_id, "status_updated_to": new_status},
            message=f"Goal '{goal_id}' status successfully updated to '{new_status}'."
        )
    except (KeyError, AttributeError) as e_status:
        logger.error(
            f"NCIM Tools (update_goal_status): Invalid status '{new_status}' or GoalStatus Enum issue: {e_status}")
        return create_error_tool_response(f"Invalid new_status '{new_status}'. Must match GoalStatus enum values.")
    except Exception as e:
        logger.error(f"NCIM Tools (update_goal_status): Error updating goal '{goal_id}': {e}", exc_info=True)
        return create_error_tool_response(f"Failed to update goal status: {str(e)}")


# FunctionTool now uses the Python function's name and docstring
update_goal_status_tool = FunctionTool(func=update_goal_status)

# Export all tools
ceaf_ncim_tools = []
if get_active_goals_and_narrative_context_tool:
    ceaf_ncim_tools.append(get_active_goals_and_narrative_context_tool)
if get_self_representation_tool:
    ceaf_ncim_tools.append(get_self_representation_tool)
if commit_self_representation_update_tool:
    ceaf_ncim_tools.append(commit_self_representation_update_tool)
if update_goal_status_tool:
    ceaf_ncim_tools.append(update_goal_status_tool)

# -------------------- observability_tools.py --------------------

# ceaf_core/tools/observability_tools.py

import logging
import json
import os
import time
from pathlib import Path
from typing import Dict, Any, Optional, List

from google.adk.tools import FunctionTool, ToolContext

from .common_utils import create_successful_tool_response, \
    create_error_tool_response  # Removed sanitize_text_for_logging as it's not used here
from ..services.persistent_log_service import PersistentLogService  # ADDED IMPORT

# Attempt to import the Pydantic model
try:
    from ..modules.mcl_engine.finetuning_datatypes import FinetuningDataPoint

    FINETUNING_DATATYPE_LOADED = True
except ImportError:
    FINETUNING_DATATYPE_LOADED = False


    class FinetuningDataPoint:  # Dummy placeholder
        def __init__(self, **kwargs): self.data = kwargs  # type: ignore

        def model_dump_json(self, **kwargs): return json.dumps(self.data)  # type: ignore


    logging.warning("ObservabilityTools: FinetuningDataPoint model not found. Logging will use raw dicts.")

logger = logging.getLogger("ObservabilityTools")

# --- Configuration for Finetuning Data Logger ---
FINETUNING_LOG_ENABLED_ENV_VAR = "CEAF_FINETUNING_LOG_ENABLED"
DEFAULT_FINETUNING_LOG_FILE_PATH = "./data/ceaf_finetuning_log.jsonl"
FINETUNING_LOG_FILE = Path(os.getenv("CEAF_FINETUNING_LOG_FILE", DEFAULT_FINETUNING_LOG_FILE_PATH))

# --- Initialize PersistentLogService (similar to mcl_callbacks) ---
try:
    persistent_log_service = PersistentLogService()
    PLS_FOR_FINETUNING_AVAILABLE = True
except Exception as e_pls_obs:
    logger.error(
        f"ObservabilityTools: Failed to initialize PersistentLogService: {e_pls_obs}. Finetuning data might only go to JSONL if PLS fails.",
        exc_info=True)
    persistent_log_service = None  # type: ignore
    PLS_FOR_FINETUNING_AVAILABLE = False


def log_finetuning_data_point(
        tool_context: ToolContext,
        ora_initial_draft_response: str,  # MOVED: Mandatory, non-default
        ora_refined_response: str,  # MOVED: Mandatory, non-default
        # Optional arguments now follow mandatory ones
        user_query: Optional[str] = None,
        ncf_context_summary: Optional[str] = None,
        vre_critique_json: Optional[str] = None,
        vre_overall_alignment: Optional[str] = None,
        vre_recommendations_applied: Optional[List[str]] = None,
        mcl_eoc_assessment_at_turn_start: Optional[str] = None,
        active_ncf_parameters: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Logs a structured data point for potential ORA fine-tuning.
    This data captures ORA's initial draft, VRE's critique, and ORA's refined response.
    Logging is controlled by the CEAF_FINETUNING_LOG_ENABLED environment variable.
    Attempts to log to PersistentLogService and then to a dedicated JSONL file.
    """
    is_enabled = os.getenv(FINETUNING_LOG_ENABLED_ENV_VAR, "false").lower() == "true"

    if not is_enabled:
        return create_successful_tool_response(
            message="Finetuning data logging is currently disabled by environment variable.")

    session_id = getattr(tool_context, 'session_id', None)
    turn_id = getattr(tool_context, 'invocation_id', None)
    source_agent = getattr(tool_context, 'agent_name', "ORA_SelfCorrection_Tool")  # Agent calling this tool

    # This dict will be the payload for PLS and basis for FinetuningDataPoint Pydantic model
    data_payload_dict = {
        "user_query": user_query,
        "ncf_context_summary": ncf_context_summary,
        "ora_initial_draft_response": ora_initial_draft_response,
        "vre_critique_json": vre_critique_json,
        "vre_overall_alignment": vre_overall_alignment,
        "vre_recommendations_applied": vre_recommendations_applied or [],
        "ora_refined_response": ora_refined_response,
        "mcl_eoc_assessment_at_turn_start": mcl_eoc_assessment_at_turn_start,
        "active_ncf_parameters": active_ncf_parameters or {},
        # 'tags' will be passed to PLS log_event separately.
        # For FinetuningDataPoint Pydantic model, it's part of the main dict.
    }

    pls_logged_successfully = False
    jsonl_logged_successfully = False
    pls_error_msg = None
    jsonl_error_msg = None

    # 1. Attempt to log via PersistentLogService
    if PLS_FOR_FINETUNING_AVAILABLE and persistent_log_service:
        try:
            # Add session_id and turn_id to the payload if they are not inherently part of it
            # for other event types, but for finetuning data, they are good context.
            # However, PLS log_event takes them as separate args, so data_payload_dict is fine as is.
            combined_tags_for_pls = ["finetuning_datapoint", "ora_self_correction"] + (tags or [])

            persistent_log_service.log_event(
                event_type="FINETUNING_DATA_POINT",  # Consistent event type for PLS
                data_payload=data_payload_dict,  # The core structured data
                source_agent=source_agent,
                session_id=session_id,
                turn_id=turn_id,
                tags=list(set(combined_tags_for_pls))
            )
            pls_logged_successfully = True
            logger.info(
                f"ObservabilityTools: Logged finetuning data point for turn '{turn_id}' to PersistentLogService.")
        except Exception as e_pls_log:
            pls_error_msg = str(e_pls_log)
            logger.error(
                f"ObservabilityTools: Error logging finetuning data point to PersistentLogService: {pls_error_msg}",
                exc_info=True)
    else:
        pls_error_msg = "PersistentLogService not available or not initialized."
        logger.warning(f"ObservabilityTools: Skipping PersistentLogService for finetuning data: {pls_error_msg}")

    # 2. Log to dedicated JSONL file (fallback or parallel logging)
    try:
        # Construct the full FinetuningDataPoint object for JSONL, including its own log_id and timestamp
        pydantic_input_dict = {
            "session_id": session_id,
            "turn_id": turn_id,
            **data_payload_dict,  # Spread the common payload
            "tags": tags or []
        }

        if FINETUNING_DATATYPE_LOADED:
            data_point_for_jsonl = FinetuningDataPoint(**pydantic_input_dict)  # type: ignore
            log_entry_str = data_point_for_jsonl.model_dump_json(exclude_none=True)
        else:
            # Fallback if Pydantic model not loaded (manual construction)
            data_point_for_jsonl_dict_fallback = {
                "log_id": f"ft_log_{time.time_ns()}",  # Generate ID if model doesn't
                "timestamp": time.time(),  # Generate timestamp if model doesn't
                **pydantic_input_dict  # Spread the rest
            }
            log_entry_str = json.dumps({k: v for k, v in data_point_for_jsonl_dict_fallback.items() if v is not None})

        FINETUNING_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(FINETUNING_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(log_entry_str + "\n")
        jsonl_logged_successfully = True
        logger.info(
            f"ObservabilityTools: Logged finetuning data point for turn '{turn_id}' to JSONL file: {FINETUNING_LOG_FILE.name}.")

    except Exception as e_jsonl:
        jsonl_error_msg = str(e_jsonl)
        logger.error(f"ObservabilityTools: Error logging finetuning data point to JSONL: {jsonl_error_msg}",
                     exc_info=True)

    # Determine overall success message
    if pls_logged_successfully and jsonl_logged_successfully:
        return create_successful_tool_response(
            message="Finetuning data point logged successfully to Persistent DB and JSONL file.")
    elif pls_logged_successfully:
        return create_successful_tool_response(
            message=f"Finetuning data point logged to Persistent DB, but JSONL logging failed: {jsonl_error_msg}")
    elif jsonl_logged_successfully:
        return create_successful_tool_response(
            message=f"Finetuning data point logged to JSONL file, but Persistent DB logging failed: {pls_error_msg}")
    else:
        return create_error_tool_response(
            error_message="Failed to log finetuning data to any store.",
            details={"turn_id": turn_id, "pls_error": pls_error_msg, "jsonl_error": jsonl_error_msg}
        )


def log_ora_self_correction_for_finetuning(
        tool_context: ToolContext,
        ora_initial_draft_response: str,
        ora_refined_response: str,
        user_query: Optional[str] = None,
        ncf_context_summary: Optional[str] = None,
        vre_critique_json: Optional[str] = None,
        vre_overall_alignment: Optional[str] = None,
        vre_recommendations_applied: Optional[List[str]] = None,
        mcl_eoc_assessment_at_turn_start: Optional[str] = None,
        active_ncf_parameters: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None
) -> Dict[str, Any]:

    # ... (implementation of the function remains the same) ...
    is_enabled = os.getenv(FINETUNING_LOG_ENABLED_ENV_VAR, "false").lower() == "true"

    if not is_enabled:
        return create_successful_tool_response(
            message="Finetuning data logging is currently disabled by environment variable.")

    session_id = getattr(tool_context, 'session_id', None)
    turn_id = getattr(tool_context, 'invocation_id', None)
    source_agent = getattr(tool_context, 'agent_name', "ORA_SelfCorrection_Tool")

    data_payload_dict = {
        "user_query": user_query,
        "ncf_context_summary": ncf_context_summary,
        "ora_initial_draft_response": ora_initial_draft_response,
        "vre_critique_json": vre_critique_json,
        "vre_overall_alignment": vre_overall_alignment,
        "vre_recommendations_applied": vre_recommendations_applied or [],
        "ora_refined_response": ora_refined_response,
        "mcl_eoc_assessment_at_turn_start": mcl_eoc_assessment_at_turn_start,
        "active_ncf_parameters": active_ncf_parameters or {},
    }

    pls_logged_successfully = False
    jsonl_logged_successfully = False
    pls_error_msg = None
    jsonl_error_msg = None

    if PLS_FOR_FINETUNING_AVAILABLE and persistent_log_service:
        try:
            combined_tags_for_pls = ["finetuning_datapoint", "ora_self_correction"] + (tags or [])
            persistent_log_service.log_event(
                event_type="FINETUNING_DATA_POINT",
                data_payload=data_payload_dict,
                source_agent=source_agent,
                session_id=session_id,
                turn_id=turn_id,
                tags=list(set(combined_tags_for_pls))
            )
            pls_logged_successfully = True
            logger.info(
                f"ObservabilityTools: Logged finetuning data point for turn '{turn_id}' to PersistentLogService.")
        except Exception as e_pls_log:
            pls_error_msg = str(e_pls_log)
            logger.error(
                f"ObservabilityTools: Error logging finetuning data point to PersistentLogService: {pls_error_msg}",
                exc_info=True)
    else:
        pls_error_msg = "PersistentLogService not available or not initialized."
        logger.warning(f"ObservabilityTools: Skipping PersistentLogService for finetuning data: {pls_error_msg}")

    try:
        pydantic_input_dict = {
            "session_id": session_id,
            "turn_id": turn_id,
            **data_payload_dict,
            "tags": tags or []
        }
        if FINETUNING_DATATYPE_LOADED:
            data_point_for_jsonl = FinetuningDataPoint(**pydantic_input_dict)
            log_entry_str = data_point_for_jsonl.model_dump_json(exclude_none=True)
        else:
            data_point_for_jsonl_dict_fallback = {
                "log_id": f"ft_log_{time.time_ns()}",
                "timestamp": time.time(),
                **pydantic_input_dict
            }
            log_entry_str = json.dumps({k: v for k, v in data_point_for_jsonl_dict_fallback.items() if v is not None})

        FINETUNING_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(FINETUNING_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(log_entry_str + "\n")
        jsonl_logged_successfully = True
        logger.info(
            f"ObservabilityTools: Logged finetuning data point for turn '{turn_id}' to JSONL file: {FINETUNING_LOG_FILE.name}.")
    except Exception as e_jsonl:
        jsonl_error_msg = str(e_jsonl)
        logger.error(f"ObservabilityTools: Error logging finetuning data point to JSONL: {jsonl_error_msg}",
                     exc_info=True)

    if pls_logged_successfully and jsonl_logged_successfully:
        return create_successful_tool_response(
            message="Finetuning data point logged successfully to Persistent DB and JSONL file.")
    elif pls_logged_successfully:
        return create_successful_tool_response(
            message=f"Finetuning data point logged to Persistent DB, but JSONL logging failed: {jsonl_error_msg}")
    elif jsonl_logged_successfully:
        return create_successful_tool_response(
            message=f"Finetuning data point logged to JSONL file, but Persistent DB logging failed: {pls_error_msg}")
    else:
        return create_error_tool_response(
            error_message="Failed to log finetuning data to any store.",
            details={"turn_id": turn_id, "pls_error": pls_error_msg, "jsonl_error": jsonl_error_msg}
        )

# MODIFIED: Removed name and description arguments
log_finetuning_data_tool = FunctionTool(
    func=log_ora_self_correction_for_finetuning # Use the renamed Python function
)

ceaf_observability_tools = [
    log_finetuning_data_tool,
]



# -------------------- vre_tools.py --------------------

# VRE Tools
# ceaf_project/ceaf_core/tools/vre_tools.py

import logging
import json
from typing import Dict, Any, Optional, List
import asyncio

from google.adk.tools import FunctionTool, ToolContext, agent_tool

logger = logging.getLogger(__name__)

# Assuming vre_agent is defined and accessible for AgentTool
try:
    from ..agents.vre_agent import vre_agent  # The actual VRE_Agent instance
    VRE_AGENT_LOADED = True
except ImportError as e:
    logger.error(f"VRE Tools: Critical import error for vre_agent: {e}. VRE AgentTool may not function correctly.", exc_info=True)
    vre_agent = None
    VRE_AGENT_LOADED = False

from .common_utils import (
    create_successful_tool_response,
    create_error_tool_response,
    sanitize_text_for_logging
    # parse_llm_json_output # Not used in this file
)

# --- Tool to Request VRE Assessment (using agent_tool.AgentTool) ---
request_ethical_and_epistemic_review_tool = None # Initialize to allow conditional definition
if VRE_AGENT_LOADED and vre_agent:
    # This part is an AgentTool, its name and description are handled by AgentTool's constructor.
    # AgentTool expects a 'name' and 'description' for itself as a wrapper.
    request_ethical_and_epistemic_review_tool = agent_tool.AgentTool(
        agent=vre_agent
    )
    logger.info(f"VRE Tools: Defined 'request_ethical_and_epistemic_review' (AgentTool wrapping VRE_Agent).")

else:
    def request_ethical_and_epistemic_review( # Python function name IS the tool name
            tool_context: ToolContext, # <<< MODIFIED: Made non-optional and first parameter
            proposed_action_summary: Optional[str] = None,
            proposed_response_text: Optional[str] = None,
            user_query_context: Optional[str] = None,
            active_ncf_summary: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        (DOCSTRING REMAINS - This tool requests an ethical and epistemic review of a proposed AI action or response from the VRE Agent.)
        Provides a structured assessment of alignment with CEAF principles, potential concerns, and recommendations.
        Use this before ORA finalizes a response, especially for sensitive topics, complex reasoning, or when NCF indicates high epistemic risk.
        """
        logger.error(
            "VRE Tools: 'request_ethical_and_epistemic_review' called, but VRE_Agent is not loaded. Returning dummy error.")
        return create_error_tool_response(
            "VRE system unavailable.",
            details="The Virtue and Reasoning Engine (VRE_Agent) could not be loaded or is not configured."
        )

    request_ethical_and_epistemic_review_tool = FunctionTool(
        func=request_ethical_and_epistemic_review
    )
    if not VRE_AGENT_LOADED:
        logger.error("VRE Tools: 'vre_agent' not loaded, so 'request_ethical_and_epistemic_review' is a DUMMY FunctionTool.")


# --- Potential Future FunctionTools for VRE_Agent itself ---
# These would only be added to vre_agent.tools if vre_agent itself needs to call them.
# For now, they are illustrative. If they were to be general CEAF tools callable by ORA,
# they'd be defined similarly to the dummy above.

def assess_claim_confidence_level(
        claim_text: str,
        tool_context: ToolContext,
        supporting_evidence_summary: Optional[str] = None
) -> Dict[str, Any]:

    logger.info(f"VRE Internal Tool (assess_claim_confidence_level) for claim: {sanitize_text_for_logging(claim_text)}")
    confidence = "medium"
    reasoning = "Placeholder: Based on typical evidence for such claims."
    if not supporting_evidence_summary:
        confidence = "low"
        reasoning = "Placeholder: No supporting evidence provided for the claim."
    return create_successful_tool_response(
        data={"claim": claim_text, "confidence_level": confidence, "reasoning": reasoning},
        message="Claim confidence assessed."
    )
# Example of creating a FunctionTool if this were to be exposed:
# assess_claim_confidence_tool = FunctionTool(func=assess_claim_confidence_level)

def identify_relevant_ethical_principles(
        scenario_description: str,
        tool_context: ToolContext
) -> Dict[str, Any]:

    logger.info(
        f"VRE Internal Tool (identify_relevant_ethical_principles) for scenario: {sanitize_text_for_logging(scenario_description)}")
    relevant_principles_found = []
    try:
        from ..modules.vre_engine.ethical_governor import CEAF_ETHICAL_PRINCIPLES
        for p in CEAF_ETHICAL_PRINCIPLES:
            if any(kw.lower() in scenario_description.lower() for kw in p.implications_keywords):
                relevant_principles_found.append({"id": p.id, "name": p.name})
    except ImportError:
        logger.warning("VRE Tools: Could not import CEAF_ETHICAL_PRINCIPLES for identify_relevant_ethical_principles tool.")

    if not relevant_principles_found:
        relevant_principles_found.append({"id": "general_ethics", "name": "General Ethical Considerations Apply"})

    return create_successful_tool_response(
        data={"scenario": scenario_description, "relevant_principles": relevant_principles_found},
        message="Relevant ethical principles identified."
    )
# Example of creating a FunctionTool if this were to be exposed:
# identify_principles_tool = FunctionTool(func=identify_relevant_ethical_principles)


# This list is what ORA (or other agents) would get if they import `ceaf_vre_tools`
ceaf_vre_tools = []
if request_ethical_and_epistemic_review_tool: # Check if it was successfully defined
    ceaf_vre_tools.append(request_ethical_and_epistemic_review_tool)




# -------------------- __init__.py --------------------

# CEAF Tools
from .observability_tools import ceaf_observability_tools

# -------------------- adk_helpers.py --------------------

# ceaf_core/utils/adk_helpers.py
"""ADK Helper utilities to handle Google ADK specific configurations"""

import os
import logging

logger = logging.getLogger(__name__)


def configure_adk_warnings():
    """Configure ADK to suppress certain warnings"""
    # Suppress Google ADK default value warnings
    os.environ['GOOGLE_ADK_SUPPRESS_DEFAULT_VALUE_WARNING'] = 'true'

    # You can add other ADK configurations here
    logger.info("ADK warnings configured")


def create_function_tool_safe(func):
    """
    Wrapper to create FunctionTool with proper handling of default parameters.
    This helps avoid the Google ADK warning about default values.
    """
    from google.adk.tools import FunctionTool

    # Remove default values from function signature if needed
    # This is a placeholder - implement based on your specific needs
    return FunctionTool(func=func)

# -------------------- embedding_utils.py --------------------

# ceaf_core/utils/embedding_utils.py

import asyncio
import logging
import os
from typing import List, Optional, Union, Dict, Any
import numpy as np

import litellm

# Attempt to import SentenceTransformer
try:
    from sentence_transformers import SentenceTransformer

    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMER_AVAILABLE = False
    SentenceTransformer = None  # type: ignore

logger = logging.getLogger(__name__)

# --- Configuration from Environment Variables (Defaults for the client instance) ---
DEFAULT_EMBEDDING_PROVIDER = os.getenv("CEAF_EMBEDDING_PROVIDER", "sentence_transformers")
DEFAULT_EMBEDDING_MODEL_FOR_CLIENT = os.getenv("CEAF_DEFAULT_EMBEDDING_MODEL",
                                               "all-MiniLM-L6-v2")  # Default for client if no type context

# --- Type-Specific Embedding Model Configuration ---
# This maps a context type (e.g., memory type) to a specific model name.
# Models can be SentenceTransformer names or LiteLLM compatible strings.
# Provider for these models is determined by EmbeddingClient's initialized provider.
# For CEAF2, memory_type strings would be like "explicit", "emotional", etc.
EMBEDDING_MODELS_CONFIG: Dict[str, str] = {
    "explicit": os.getenv("CEAF_EMBEDDING_MODEL_EXPLICIT", "BAAI/bge-small-en-v1.5"),
    "emotional": os.getenv("CEAF_EMBEDDING_MODEL_EMOTIONAL", "all-MiniLM-L6-v2"),
    "procedural": os.getenv("CEAF_EMBEDDING_MODEL_PROCEDURAL", "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"),
    "flashbulb": os.getenv("CEAF_EMBEDDING_MODEL_FLASHBULB", "nomic-ai/nomic-embed-text-v1.5"),
    # Requires trust_remote_code
    "somatic": os.getenv("CEAF_EMBEDDING_MODEL_SOMATIC", "all-MiniLM-L6-v2"),  # Using text model as proxy
    "liminal": os.getenv("CEAF_EMBEDDING_MODEL_LIMINAL", "mixedbread-ai/mxbai-embed-large-v1"),
    "generative": os.getenv("CEAF_EMBEDDING_MODEL_GENERATIVE", "all-MiniLM-L6-v2"),
    "goal_record": os.getenv("CEAF_EMBEDDING_MODEL_GOAL", "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"),
    # Goals are often query-like
    "kg_entity_record": os.getenv("CEAF_EMBEDDING_MODEL_KG_ENTITY", "BAAI/bge-small-en-v1.5"),  # Entities often factual
    "kg_relation_record": os.getenv("CEAF_EMBEDDING_MODEL_KG_RELATION", "all-MiniLM-L6-v2"),
    # Relations more contextual
    "default_query": os.getenv("CEAF_EMBEDDING_MODEL_QUERY", DEFAULT_EMBEDDING_MODEL_FOR_CLIENT),  # For generic queries
    "DEFAULT_FALLBACK": DEFAULT_EMBEDDING_MODEL_FOR_CLIENT  # Ultimate fallback
}


class EmbeddingClient:
    """
    A client to generate text embeddings using either local SentenceTransformer models
    or API-based models via LiteLLM, with model selection based on context type.
    """

    def __init__(
            self,
            provider: Optional[str] = None,
            default_model_name: Optional[str] = None,  # Default model if context_type not in config
            litellm_kwargs: Optional[Dict[str, Any]] = None
    ):
        self.provider = provider or DEFAULT_EMBEDDING_PROVIDER
        self.default_model_name = default_model_name or DEFAULT_EMBEDDING_MODEL_FOR_CLIENT
        self.litellm_kwargs = litellm_kwargs or {}
        self._st_model_cache: Dict[str, SentenceTransformer] = {}  # Cache for ST models

        if self.provider not in ["litellm", "sentence_transformers"]:
            logger.error(
                f"Unsupported embedding provider: {self.provider}. Must be 'litellm' or 'sentence_transformers'.")
            # Fallback or raise error
            self.provider = "sentence_transformers" if SENTENCE_TRANSFORMER_AVAILABLE else "litellm"
            logger.warning(f"Falling back to provider: {self.provider}")

        logger.info(
            f"Initializing EmbeddingClient with default provider: '{self.provider}', default model: '{self.default_model_name}'")

        if self.provider == "sentence_transformers" and not SENTENCE_TRANSFORMER_AVAILABLE:
            msg = "SentenceTransformer provider selected, but sentence-transformers library is not installed. Embedding generation via ST will fail."
            logger.error(msg)
            # This doesn't raise immediately, allowing LiteLLM to still function if it's the one used by config.
            # Calls using ST will fail later if _get_st_model is invoked.

    def _resolve_model_name(self, context_type: Optional[str]) -> str:
        """Resolves the model name based on context_type or uses the client's default."""
        if context_type and context_type in EMBEDDING_MODELS_CONFIG:
            return EMBEDDING_MODELS_CONFIG[context_type]
        return EMBEDDING_MODELS_CONFIG.get("DEFAULT_FALLBACK", self.default_model_name)

    def _get_st_model(self, model_name_str: str) -> SentenceTransformer:
        """Loads or retrieves a SentenceTransformer model from the cache."""
        if not SENTENCE_TRANSFORMER_AVAILABLE:
            raise RuntimeError("SentenceTransformer library is not available.")
        if model_name_str not in self._st_model_cache:
            logger.info(f"Loading SentenceTransformer model: {model_name_str}")
            try:
                # Handle models like nomic-embed-text that require trust_remote_code
                trust_code = any(
                    kc in model_name_str for kc in ["nomic-ai/", "jinaai/"])  # Add other keywords if needed
                self._st_model_cache[model_name_str] = SentenceTransformer(model_name_str, trust_remote_code=trust_code)
                logger.info(f"Successfully loaded and cached SentenceTransformer model: {model_name_str}")
            except Exception as e:
                logger.error(
                    f"Failed to load SentenceTransformer model '{model_name_str}': {e}. Using client default '{self.default_model_name}' if available for ST.",
                    exc_info=True)
                # Fallback to client's default ST model if specific one fails
                if self.default_model_name not in self._st_model_cache:
                    logger.info(f"Loading client's default ST model as fallback: {self.default_model_name}")
                    default_trust_code = any(kc in self.default_model_name for kc in ["nomic-ai/", "jinaai/"])
                    self._st_model_cache[self.default_model_name] = SentenceTransformer(self.default_model_name,
                                                                                        trust_remote_code=default_trust_code)
                self._st_model_cache[model_name_str] = self._st_model_cache[
                    self.default_model_name]  # Point failed model to default
        return self._st_model_cache[model_name_str]

    async def get_embedding(self, text: str, context_type: Optional[str] = None, **kwargs) -> List[float]:
        if not text or not isinstance(text, str):
            logger.warning("get_embedding received empty or invalid text input.")
            # Determine dimension for zero vector
            resolved_model_name_for_dim = self._resolve_model_name(context_type)
            dim = 384  # Default
            if self.provider == "sentence_transformers" and SENTENCE_TRANSFORMER_AVAILABLE:
                try:
                    st_model_for_dim = self._get_st_model(resolved_model_name_for_dim)
                    if hasattr(st_model_for_dim, 'get_sentence_embedding_dimension'):
                        dim = st_model_for_dim.get_sentence_embedding_dimension() or dim  # type: ignore
                except Exception:
                    pass  # Stick to default dim if model load fails for dim check
            return [0.0] * dim

        actual_model_name = self._resolve_model_name(context_type)
        logger.debug(
            f"get_embedding: Text='{text[:30]}...', ContextType='{context_type}', ResolvedModel='{actual_model_name}', Provider='{self.provider}'")

        if self.provider == "sentence_transformers":
            st_model_instance = self._get_st_model(actual_model_name)
            try:
                loop = asyncio.get_event_loop()
                embedding_array = await loop.run_in_executor(None, st_model_instance.encode, text)
                return embedding_array.tolist()
            except Exception as e:
                logger.error(f"Error with SentenceTransformer '{actual_model_name}' for text '{text[:50]}...': {e}",
                             exc_info=True)
                raise
        elif self.provider == "litellm":
            try:
                final_litellm_kwargs = {**self.litellm_kwargs, **kwargs}
                response = await litellm.aembedding(
                    model=actual_model_name, input=[text], **final_litellm_kwargs
                )
                if response.data and len(response.data) > 0:
                    return response.data[0].embedding
                else:
                    logger.error(f"LiteLLM model '{actual_model_name}' returned no data for text: '{text[:50]}...'")
                    raise ValueError("LiteLLM returned no embedding data.")
            except Exception as e:
                logger.error(f"Error with LiteLLM model '{actual_model_name}' for text '{text[:50]}...': {e}",
                             exc_info=True)
                raise
        else:
            raise RuntimeError(f"Invalid embedding provider: {self.provider}")

    async def get_embeddings(self, texts: List[str], context_type: Optional[str] = None, **kwargs) -> List[List[float]]:
        if not texts or not all(isinstance(t, str) and t for t in texts):  # Ensure all texts are non-empty strings
            logger.warning("get_embeddings received empty list or list with invalid/empty texts.")
            return [await self.get_embedding("", context_type, **kwargs) for _ in texts]  # Return zero vectors

        actual_model_name = self._resolve_model_name(context_type)
        logger.debug(
            f"get_embeddings: Count={len(texts)}, ContextType='{context_type}', ResolvedModel='{actual_model_name}', Provider='{self.provider}'")

        if self.provider == "sentence_transformers":
            st_model_instance = self._get_st_model(actual_model_name)
            try:
                loop = asyncio.get_event_loop()
                embeddings_array = await loop.run_in_executor(None, st_model_instance.encode, texts)
                return [emb.tolist() for emb in embeddings_array]
            except Exception as e:
                logger.error(f"Error batch ST '{actual_model_name}': {e}", exc_info=True)
                raise
        elif self.provider == "litellm":
            try:
                final_litellm_kwargs = {**self.litellm_kwargs, **kwargs}
                response = await litellm.aembedding(
                    model=actual_model_name, input=texts, **final_litellm_kwargs
                )
                if response.data and len(response.data) == len(texts):
                    return [item.embedding for item in response.data]
                else:
                    logger.error(f"LiteLLM batch model '{actual_model_name}' returned mismatched/no data.")
                    raise ValueError("LiteLLM returned mismatched or no embedding data for batch.")
            except Exception as e:
                logger.error(f"Error batch LiteLLM '{actual_model_name}': {e}", exc_info=True)
                raise
        else:
            raise RuntimeError(f"Invalid embedding provider: {self.provider}")


# --- Similarity Utilities ---
def cosine_similarity_np(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Computes cosine similarity between two numpy vectors."""
    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):
        logger.debug("cosine_similarity_np: Received non-numpy array input.")
        return 0.0
    if vec1.shape != vec2.shape or vec1.ndim != 1:  # Ensure 1D and same shape
        logger.debug(f"cosine_similarity_np: Shape mismatch or not 1D. v1: {vec1.shape}, v2: {vec2.shape}")
        return 0.0  # Or handle resizing/error appropriately

    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    return float(dot_product / (norm_vec1 * norm_vec2))


def compute_adaptive_similarity(embedding1: Optional[List[float]], embedding2: Optional[List[float]]) -> float:
    """
    Compute similarity between embeddings (List[float]), handling None and dimension differences.
    """
    if embedding1 is None or embedding2 is None:
        return 0.0

    np_emb1 = np.array(embedding1, dtype=np.float32)
    np_emb2 = np.array(embedding2, dtype=np.float32)

    if np_emb1.shape[0] == 0 or np_emb2.shape[0] == 0:  # Handle empty embeddings
        return 0.0

    if np_emb1.shape[0] != np_emb2.shape[0]:
        min_dim = min(np_emb1.shape[0], np_emb2.shape[0])
        # logger.warning(f"Comparing embeddings of different dimensions: {np_emb1.shape[0]} vs {np_emb2.shape[0]}. Truncating to {min_dim}.")
        truncated_emb1 = np_emb1[:min_dim]
        truncated_emb2 = np_emb2[:min_dim]

        # Simple penalty for dimension mismatch. Could be more sophisticated.
        dim_difference_ratio = abs(np_emb1.shape[0] - np_emb2.shape[0]) / max(np_emb1.shape[0], np_emb2.shape[0])
        similarity_penalty = 0.2 * dim_difference_ratio  # Up to 20% penalty based on relative difference

        base_similarity = cosine_similarity_np(truncated_emb1, truncated_emb2)
        return max(0.0, base_similarity - similarity_penalty)

    return cosine_similarity_np(np_emb1, np_emb2)


# --- Singleton Instance ---
_embedding_client_instance: Optional[EmbeddingClient] = None


def get_embedding_client(
        provider: Optional[str] = None,
        default_model_name: Optional[str] = None,
        force_reinitialize: bool = False,
        litellm_kwargs: Optional[Dict[str, Any]] = None
) -> EmbeddingClient:
    global _embedding_client_instance

    effective_provider = provider or DEFAULT_EMBEDDING_PROVIDER
    effective_default_model_name = default_model_name or DEFAULT_EMBEDDING_MODEL_FOR_CLIENT
    effective_litellm_kwargs = litellm_kwargs or {}

    if _embedding_client_instance is None or force_reinitialize or \
            _embedding_client_instance.provider != effective_provider or \
            _embedding_client_instance.default_model_name != effective_default_model_name or \
            _embedding_client_instance.litellm_kwargs != effective_litellm_kwargs:
        logger.info(
            f"Creating/Re-creating EmbeddingClient: Provider='{effective_provider}', DefaultModel='{effective_default_model_name}'")
        try:
            _embedding_client_instance = EmbeddingClient(
                provider=effective_provider,
                default_model_name=effective_default_model_name,
                litellm_kwargs=effective_litellm_kwargs
            )
        except Exception as e:
            logger.critical(f"Failed to initialize EmbeddingClient: {e}.", exc_info=True)
            _embedding_client_instance = None  # Ensure it's None on failure
            raise

    if _embedding_client_instance is None:
        raise RuntimeError("EmbeddingClient singleton is None after attempted initialization.")

    return _embedding_client_instance


# -------------------- __init__.py --------------------

# Tests


# -------------------- test_ora_agent.py --------------------

# ORA Agent Tests


# -------------------- __init__.py --------------------

# Agent Tests


# -------------------- test_ncf_engine.py --------------------

# NCF Engine Tests


# -------------------- __init__.py --------------------

# Module Tests


# -------------------- test_ncf_tools.py --------------------

# NCF Tools Tests


# -------------------- __init__.py --------------------

# Tool Tests


